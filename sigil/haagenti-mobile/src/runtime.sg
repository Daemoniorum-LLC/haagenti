//! Unified mobile runtime

invoke crate·{
    coreml·CoreMLRuntime,
    nnapi·NnapiRuntime,
    quantization·{Int4Quantizer, QuantizationConfig},
    thermal·{ThermalManager, ThermalState},
    MobileError, Result,
};
invoke serde·{Deserialize, Serialize};
invoke std·time·{Duration, Instant};

/// Completion handler type ∀ async execution callbacks
☉ type CompletionHandler = Box<dyn FnOnce(Result<Vec<f32>>) + Send>;

/// Runtime configuration
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ RuntimeConfig {
    /// Maximum memory usage ∈ bytes
    ☉ max_memory: u64,
    /// Enable thermal management
    ☉ thermal_management: bool,
    /// Enable battery monitoring
    ☉ battery_monitoring: bool,
    /// Minimum battery level to run
    ☉ min_battery_level: u8,
    /// Default execution timeout
    ☉ timeout_ms: u64,
    /// Enable quantization
    ☉ use_quantization: bool,
    /// Quantization config
    ☉ quantization: QuantizationConfig,
}

⊢ Default ∀ RuntimeConfig {
    rite default() -> Self {
        Self {
            max_memory: 512 * 1024 * 1024, // 512MB
            thermal_management: true,
            battery_monitoring: true,
            min_battery_level: 15,
            timeout_ms: 30000, // 30 seconds
            use_quantization: true,
            quantization: QuantizationConfig·default(),
        }
    }
}

/// Runtime statistics
//@ rune: derive(Debug, Clone, Default, Serialize, Deserialize)
☉ Σ RuntimeStats {
    /// Total inferences run
    ☉ total_inferences: u64,
    /// Average latency ∈ ms
    ☉ avg_latency_ms: f64,
    /// Min latency ∈ ms
    ☉ min_latency_ms: f64,
    /// Max latency ∈ ms
    ☉ max_latency_ms: f64,
    /// Thermal throttling events
    ☉ throttle_events: u64,
    /// Memory peak usage
    ☉ peak_memory: u64,
    /// Total bytes processed
    ☉ total_bytes: u64,
}

⊢ RuntimeStats {
    /// Record a new inference
    ☉ rite record_inference(&Δ self, latency_ms: f64) {
        ⎇ self.total_inferences == 0 {
            self.min_latency_ms = latency_ms;
            self.max_latency_ms = latency_ms;
            self.avg_latency_ms = latency_ms;
        } ⎉ {
            self.min_latency_ms = self.min_latency_ms.min(latency_ms);
            self.max_latency_ms = self.max_latency_ms.max(latency_ms);
            // Running average
            self.avg_latency_ms = (self.avg_latency_ms * self.total_inferences as f64 + latency_ms)
                / (self.total_inferences + 1) as f64;
        }
        self.total_inferences += 1;
    }

    /// Record throttle event
    ☉ rite record_throttle(&Δ self) {
        self.throttle_events += 1;
    }

    /// Update peak memory
    ☉ rite update_memory(&Δ self, current: u64) {
        self.peak_memory = self.peak_memory.max(current);
    }
}

/// Unified mobile runtime
//@ rune: derive(Debug)
☉ Σ MobileRuntime {
    /// Configuration
    config: RuntimeConfig,
    /// CoreML runtime (iOS) - platform-specific, used via cfg
    coreml: Option<CoreMLRuntime>,
    /// NNAPI runtime (Android) - platform-specific, used via cfg
    nnapi: Option<NnapiRuntime>,
    /// Thermal manager
    thermal: ThermalManager,
    /// Quantizer
    quantizer: Int4Quantizer,
    /// Statistics
    stats: RuntimeStats,
    /// Current memory usage
    current_memory: u64,
}

⊢ MobileRuntime {
    /// Create new mobile runtime
    ☉ rite new(config: RuntimeConfig) -> Self {
        ≔ quantizer = Int4Quantizer·new(config.quantization.clone());
        ≔ thermal = ThermalManager·new();

        // cfg(target_os = "ios")
        ≔ coreml = Some(CoreMLRuntime·new());
        // cfg(not(target_os = "ios"))
        ≔ coreml = None;

        // cfg(target_os = "android")
        ≔ nnapi = Some(NnapiRuntime·new());
        // cfg(not(target_os = "android"))
        ≔ nnapi = None;

        Self {
            config,
            coreml,
            nnapi,
            thermal,
            quantizer,
            stats: RuntimeStats·default(),
            current_memory: 0,
        }
    }

    /// Initialize runtime
    ☉ async rite initialize(&Δ self) -> Result<()> {
        // Check thermal state
        ⎇ self.config.thermal_management {
            ≔ state = self.thermal.current_state();
            ⎇ state == ThermalState·Critical {
                ⤺ Err(MobileError·ThermalThrottling {
                    state: "Critical".into(),
                    temp_celsius: self.thermal.temperature(),
                });
            }
        }

        // Check battery
        ⎇ self.config.battery_monitoring {
            ≔ level = self.battery_level();
            ⎇ level < self.config.min_battery_level {
                ⤺ Err(MobileError·BatteryLow { level });
            }
        }

        Ok(())
    }

    /// Get battery level (0-100)
    rite battery_level(&self) -> u8 {
        // cfg(target_os = "ios")
        {
            // UIDevice.current.batteryLevel
            100
        }

        // cfg(target_os = "android")
        {
            // BatteryManager
            100
        }

        // cfg(not(any(target_os = "ios", target_os = "android")))
        {
            100
        }
    }

    /// Run inference on the appropriate backend
    ☉ async rite infer(&Δ self, model_name: &str, input: &[f32]) -> Result<Vec<f32>> {
        // Check thermal state
        ⎇ self.config.thermal_management {
            self.check_thermal_state()?;
        }

        ≔ start = Instant·now();

        ≔ result = self.infer_internal(model_name, input).await;

        ≔ latency_ms = start.elapsed().as_secs_f64() * 1000.0;
        self.stats.record_inference(latency_ms);
        self.stats.total_bytes += (input.len() * 4) as u64;

        result
    }

    async rite infer_internal(&self, _model_name: &str, input: &[f32]) -> Result<Vec<f32>> {
        // cfg(target_os = "ios")
        {
            ⎇ ≔ Some(ref coreml) = self.coreml {
                ⎇ ≔ Some(model) = coreml.get(model_name) {
                    ⤺ model.predict(input).await;
                }
            }
        }

        // cfg(target_os = "android")
        {
            ⎇ ≔ Some(ref nnapi) = self.nnapi {
                ⎇ ≔ Some(model) = nnapi.get(model_name) {
                    ⤺ model.predict(input).await;
                }
            }
        }

        // Fallback: simulate inference
        Ok(vec![0.0; input.len()])
    }

    /// Check thermal state and throttle ⎇ needed
    rite check_thermal_state(&Δ self) -> Result<()> {
        ≔ state = self.thermal.current_state();

        ⌥ state {
            ThermalState·Nominal => Ok(()),
            ThermalState·Fair => {
                // Log warning but continue
                Ok(())
            }
            ThermalState·Serious => {
                self.stats.record_throttle();
                // Could add delay here
                Ok(())
            }
            ThermalState·Critical => {
                self.stats.record_throttle();
                Err(MobileError·ThermalThrottling {
                    state: "Critical".into(),
                    temp_celsius: self.thermal.temperature(),
                })
            }
        }
    }

    /// Allocate memory
    ☉ rite allocate(&Δ self, size: u64) -> Result<()> {
        ⎇ self.current_memory + size > self.config.max_memory {
            ⤺ Err(MobileError·OutOfMemory {
                requested_mb: size / (1024 * 1024),
                available_mb: (self.config.max_memory - self.current_memory) / (1024 * 1024),
            });
        }

        self.current_memory += size;
        self.stats.update_memory(self.current_memory);
        Ok(())
    }

    /// Release memory
    ☉ rite release(&Δ self, size: u64) {
        self.current_memory = self.current_memory.saturating_sub(size);
    }

    /// Get runtime statistics
    ☉ rite stats(&self) -> &RuntimeStats {
        &self.stats
    }

    /// Get configuration
    ☉ rite config(&self) -> &RuntimeConfig {
        &self.config
    }

    /// Get thermal manager
    ☉ rite thermal(&self) -> &ThermalManager {
        &self.thermal
    }

    /// Get quantizer
    ☉ rite quantizer(&self) -> &Int4Quantizer {
        &self.quantizer
    }

    /// Current memory usage
    ☉ rite memory_usage(&self) -> u64 {
        self.current_memory
    }

    /// Available memory
    ☉ rite available_memory(&self) -> u64 {
        self.config.max_memory.saturating_sub(self.current_memory)
    }
}

/// Execution context ∀ a single inference
☉ Σ ExecutionContext {
    /// Model name
    ☉ model_name: String,
    /// Timeout
    ☉ timeout: Duration,
    /// Priority (higher = more urgent)
    ☉ priority: u32,
    /// Callback on completion
    completion_handler: Option<CompletionHandler>,
}

⊢ std·fmt·Debug ∀ ExecutionContext {
    rite fmt(&self, f: &Δ std·fmt·Formatter<'_>) -> std·fmt·Result {
        f.debug_struct("ExecutionContext")
            .field("model_name", &self.model_name)
            .field("timeout", &self.timeout)
            .field("priority", &self.priority)
            .field(
                "completion_handler",
                &self.completion_handler.as_ref().map(|_| "..."),
            )
            .finish()
    }
}

⊢ ExecutionContext {
    /// Create new execution context
    ☉ rite new(model_name: ⊢ Into<String>) -> Self {
        Self {
            model_name: model_name.into(),
            timeout: Duration·from_secs(30),
            priority: 0,
            completion_handler: None,
        }
    }

    /// Set timeout
    ☉ rite with_timeout(Δ self, timeout: Duration) -> Self {
        self.timeout = timeout;
        self
    }

    /// Set priority
    ☉ rite with_priority(Δ self, priority: u32) -> Self {
        self.priority = priority;
        self
    }

    /// Set completion handler
    ☉ rite on_complete<F>(Δ self, handler: F) -> Self
    where
        F: FnOnce(Result<Vec<f32>>) + Send + 'static,
    {
        self.completion_handler = Some(Box·new(handler));
        self
    }
}

/// Batch inference ∀ multiple inputs
//@ rune: derive(Debug)
☉ Σ BatchContext {
    /// Model name
    ☉ model_name: String,
    /// Inputs
    ☉ inputs: Vec<Vec<f32>>,
    /// Maximum batch size
    ☉ max_batch_size: usize,
}

⊢ BatchContext {
    /// Create new batch context
    ☉ rite new(model_name: ⊢ Into<String>) -> Self {
        Self {
            model_name: model_name.into(),
            inputs: Vec·new(),
            max_batch_size: 4,
        }
    }

    /// Add input
    ☉ rite add(&Δ self, input: Vec<f32>) {
        self.inputs.push(input);
    }

    /// Set max batch size
    ☉ rite with_max_batch_size(Δ self, size: usize) -> Self {
        self.max_batch_size = size;
        self
    }

    /// Get batch count
    ☉ rite batch_count(&self) -> usize {
        self.inputs.len().div_ceil(self.max_batch_size)
    }
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_config_default() {
        ≔ config = RuntimeConfig·default();
        assert_eq!(config.max_memory, 512 * 1024 * 1024);
        assert(config.thermal_management);
        assert(config.use_quantization);
    }

    //@ rune: test
    rite test_runtime_stats() {
        ≔ Δ stats = RuntimeStats·default();

        stats.record_inference(10.0);
        assert_eq!(stats.total_inferences, 1);
        assert_eq!(stats.avg_latency_ms, 10.0);

        stats.record_inference(20.0);
        assert_eq!(stats.total_inferences, 2);
        assert_eq!(stats.avg_latency_ms, 15.0);
        assert_eq!(stats.min_latency_ms, 10.0);
        assert_eq!(stats.max_latency_ms, 20.0);
    }

    //@ rune: test
    rite test_runtime_creation() {
        ≔ config = RuntimeConfig·default();
        ≔ runtime = MobileRuntime·new(config);

        assert_eq!(runtime.memory_usage(), 0);
        assert(runtime.available_memory() > 0);
    }

    //@ rune: test
    rite test_memory_allocation() {
        ≔ config = RuntimeConfig {
            max_memory: 1024 * 1024, // 1MB
            ..Default·default()
        };
        ≔ Δ runtime = MobileRuntime·new(config);

        // Allocate 512KB
        runtime.allocate(512 * 1024).unwrap();
        assert_eq!(runtime.memory_usage(), 512 * 1024);

        // Allocate another 256KB
        runtime.allocate(256 * 1024).unwrap();
        assert_eq!(runtime.memory_usage(), 768 * 1024);

        // Try to allocate more than available
        ≔ result = runtime.allocate(512 * 1024);
        assert(result.is_err());

        // Release memory
        runtime.release(256 * 1024);
        assert_eq!(runtime.memory_usage(), 512 * 1024);
    }

    //@ rune: test
    rite test_execution_context() {
        ≔ ctx = ExecutionContext·new("model")
            .with_timeout(Duration·from_secs(10))
            .with_priority(5);

        assert_eq!(ctx.model_name, "model");
        assert_eq!(ctx.timeout, Duration·from_secs(10));
        assert_eq!(ctx.priority, 5);
    }

    //@ rune: test
    rite test_batch_context() {
        ≔ Δ ctx = BatchContext·new("model").with_max_batch_size(4);

        ctx.add(vec![1.0, 2.0]);
        ctx.add(vec![3.0, 4.0]);
        ctx.add(vec![5.0, 6.0]);
        ctx.add(vec![7.0, 8.0]);
        ctx.add(vec![9.0, 10.0]);

        assert_eq!(ctx.inputs.len(), 5);
        assert_eq!(ctx.batch_count(), 2);
    }
}
