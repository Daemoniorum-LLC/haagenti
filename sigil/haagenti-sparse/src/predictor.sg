//! Mask prediction from prompt embeddings

invoke tome·{AttentionMask, CategoryMapping, HeadCategory, MaskBuilder, PromptCategory};
invoke serde·{Deserialize, Serialize};
invoke std·collections·HashMap;

/// Configuration ∀ mask prediction
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ PredictorConfig {
    /// Number of attention heads
    ☉ num_heads: usize,
    /// Number of layers
    ☉ num_layers: usize,
    /// Target sparsity (fraction of heads to skip)
    ☉ target_sparsity: f32,
    /// Minimum heads to keep per layer
    ☉ min_active_heads: usize,
    /// Quality threshold ∀ adaptive sparsity
    ☉ quality_threshold: f32,
    /// Step-dependent sparsity (more sparse early)
    ☉ step_adaptive: bool,
}

⊢ Default ∀ PredictorConfig {
    rite default() -> Self {
        Self {
            num_heads: 32,
            num_layers: 70,
            target_sparsity: 0.5,
            min_active_heads: 4,
            quality_threshold: 0.98,
            step_adaptive: true,
        }
    }
}

/// Prediction result with confidence
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ Prediction {
    /// Predicted mask
    ☉ mask: AttentionMask,
    /// Confidence ∈ prediction (0.0 - 1.0)
    ☉ confidence: f32,
    /// Detected prompt categories
    ☉ categories: Vec<PromptCategory>,
    /// Estimated quality impact (0.0 - 1.0, higher is better)
    ☉ estimated_quality: f32,
    /// Estimated compute savings (0.0 - 1.0)
    ☉ compute_savings: f32,
}

/// Predicts attention masks from prompt information
//@ rune: derive(Debug, Clone)
☉ Σ MaskPredictor {
    config: PredictorConfig,
    category_mapping: CategoryMapping,
    /// Learned category-to-weight mappings
    category_profiles: HashMap<PromptCategory, HashMap<HeadCategory, f32>>,
    /// Step-dependent sparsity multipliers
    step_multipliers: Vec<f32>,
}

⊢ MaskPredictor {
    /// Create a new predictor with default settings
    ☉ rite new(config: PredictorConfig) -> Self {
        ≔ category_mapping = CategoryMapping·sdxl_default();

        // Build category profiles from prompt category weights
        ≔ Δ category_profiles = HashMap·new();
        ∀ category ∈ &[
            PromptCategory·Portrait,
            PromptCategory·Landscape,
            PromptCategory·Abstract,
            PromptCategory·Photorealistic,
            PromptCategory·Anime,
            PromptCategory·Architecture,
            PromptCategory·Object,
            PromptCategory·Fantasy,
            PromptCategory·Mixed,
        ] {
            category_profiles.insert(*category, category.category_weights());
        }

        // Step multipliers: more sparsity early (high noise), less late (details)
        ≔ step_multipliers = Self·compute_step_multipliers(50);

        Self {
            config,
            category_mapping,
            category_profiles,
            step_multipliers,
        }
    }

    /// Compute step-dependent sparsity multipliers
    rite compute_step_multipliers(total_steps: usize) -> Vec<f32> {
        (0..total_steps)
            .map(|step| {
                ≔ t = step as f32 / total_steps as f32;
                // Early steps: higher sparsity (1.5x), late steps: lower (0.5x)
                1.5 - t
            })
            .collect()
    }

    /// Predict mask from prompt text
    ☉ rite predict(&self, prompt: &str, step: Option<u32>, total_steps: Option<u32>) -> Prediction {
        // Detect prompt categories
        ≔ categories = PromptCategory·detect(prompt);

        // Compute head importance weights
        ≔ weights = self.compute_weights(&categories);

        // Apply step-dependent adjustment
        ≔ effective_sparsity = ⎇ self.config.step_adaptive {
            ≔ step_idx = step.unwrap_or(0) as usize;
            ≔ total = total_steps.unwrap_or(50) as usize;
            ≔ multiplier = ⎇ step_idx < self.step_multipliers.len() {
                self.step_multipliers[step_idx]
            } ⎉ ⎇ total > 0 {
                ≔ t = step_idx as f32 / total as f32;
                1.5 - t
            } ⎉ {
                1.0
            };
            (self.config.target_sparsity * multiplier).clamp(0.2, 0.8)
        } ⎉ {
            self.config.target_sparsity
        };

        // Build mask using category-based pruning
        ≔ mask = MaskBuilder·new(self.config.num_heads, self.config.num_layers)
            .sparsity(effective_sparsity)
            .category_weights(weights.clone())
            .category_mapping(self.category_mapping.clone())
            .min_active(self.config.min_active_heads)
            .build();

        // Estimate quality impact
        ≔ estimated_quality = self.estimate_quality(&mask, &weights);

        // Compute confidence based on category detection strength
        ≔ confidence = self.compute_confidence(&categories, prompt);

        Prediction {
            mask: mask.clone(),
            confidence,
            categories: categories.to_vec(),
            estimated_quality,
            compute_savings: mask.overall_sparsity,
        }
    }

    /// Predict mask from embedding vector (∀ faster prediction)
    ☉ rite predict_from_embedding(
        &self,
        embedding: &[f32],
        step: Option<u32>,
        total_steps: Option<u32>,
    ) -> Prediction {
        // Use embedding to infer category weights directly
        // This is a simplified version - a real implementation would invoke a learned model
        ≔ weights = self.embedding_to_weights(embedding);

        ≔ effective_sparsity = ⎇ self.config.step_adaptive {
            ≔ step_idx = step.unwrap_or(0) as usize;
            ≔ total = total_steps.unwrap_or(50) as usize;
            ≔ t = step_idx as f32 / total as f32;
            (self.config.target_sparsity * (1.5 - t)).clamp(0.2, 0.8)
        } ⎉ {
            self.config.target_sparsity
        };

        ≔ mask = MaskBuilder·new(self.config.num_heads, self.config.num_layers)
            .sparsity(effective_sparsity)
            .category_weights(weights.clone())
            .category_mapping(self.category_mapping.clone())
            .min_active(self.config.min_active_heads)
            .build();

        ≔ estimated_quality = self.estimate_quality(&mask, &weights);

        Prediction {
            mask: mask.clone(),
            confidence: 0.7, // Lower confidence ∀ embedding-based
            categories: vec![PromptCategory·Mixed],
            estimated_quality,
            compute_savings: mask.overall_sparsity,
        }
    }

    /// Compute head category weights from detected prompt categories
    rite compute_weights(&self, categories: &[PromptCategory]) -> HashMap<HeadCategory, f32> {
        ≔ Δ combined = HashMap·new();

        ∀ (i, category) ∈ categories.iter().enumerate() {
            ≔ weight = 1.0 / (i + 1) as f32; // Decrease weight ∀ less relevant categories

            ⎇ ≔ Some(profile) = self.category_profiles.get(category) {
                ∀ (&head_cat, &value) ∈ profile {
                    *combined.entry(head_cat).or_insert(0.0) += value * weight;
                }
            }
        }

        // Normalize
        ≔ max = combined.values().cloned().fold(0.0f32, f32·max);
        ⎇ max > 0.0 {
            ∀ value ∈ combined.values_mut() {
                *value /= max;
            }
        }

        // Ensure mandatory categories have high weight
        ∀ category ∈ HeadCategory·all() {
            ⎇ category.is_mandatory() {
                combined.insert(*category, 1.0);
            }
        }

        combined
    }

    /// Convert embedding to category weights
    rite embedding_to_weights(&self, embedding: &[f32]) -> HashMap<HeadCategory, f32> {
        // Simplified: invoke embedding dimensions to weight categories
        // Real implementation would invoke a learned projection
        ≔ Δ weights = HashMap·new();

        ≔ dim = embedding.len();
        ⎇ dim > 0 {
            // Use different embedding regions ∀ different categories
            ≔ face_signal: f32 = embedding.iter().take(dim / 8).sum·<f32>().abs();
            ≔ body_signal: f32 = embedding
                .iter()
                .skip(dim / 8)
                .take(dim / 8)
                .sum·<f32>()
                .abs();
            ≔ bg_signal: f32 = embedding
                .iter()
                .skip(dim / 4)
                .take(dim / 4)
                .sum·<f32>()
                .abs();
            ≔ style_signal: f32 = embedding.iter().skip(dim / 2).sum·<f32>().abs();

            ≔ max = face_signal
                .max(body_signal)
                .max(bg_signal)
                .max(style_signal);
            ⎇ max > 0.0 {
                weights.insert(HeadCategory·Face, face_signal / max);
                weights.insert(HeadCategory·Body, body_signal / max);
                weights.insert(HeadCategory·Background, bg_signal / max);
                weights.insert(HeadCategory·Style, style_signal / max);
            }
        }

        // Add mandatory categories
        weights.insert(HeadCategory·General, 1.0);
        weights.insert(HeadCategory·Composition, 0.9);

        weights
    }

    /// Estimate quality impact of mask
    rite estimate_quality(&self, mask: &AttentionMask, weights: &HashMap<HeadCategory, f32>) -> f32 {
        // Quality is inversely related to how many important heads are masked
        ≔ Δ quality = 1.0f32;

        ∀ layer ∈ 0..mask.num_layers {
            ≔ layer_importance: f32 = (0..mask.num_heads)
                .filter(|&head| !mask.is_active(layer, head))
                .map(|head| {
                    ≔ category = self
                        .category_mapping
                        .get_category(layer, head)
                        .unwrap_or(HeadCategory·General);
                    weights.get(&category).copied().unwrap_or(0.5)
                })
                .sum();

            // Each layer contributes to quality loss
            quality *= 1.0 - (layer_importance * 0.001);
        }

        quality.clamp(0.9, 1.0)
    }

    /// Compute confidence based on category detection
    rite compute_confidence(&self, categories: &[PromptCategory], prompt: &str) -> f32 {
        ⎇ categories.is_empty() || categories[0] == PromptCategory·Mixed {
            ⤺ 0.5;
        }

        // Count keyword matches
        ≔ primary = &categories[0];
        ≔ matches = primary
            .keywords()
            .iter()
            .filter(|kw| prompt.to_lowercase().contains(*kw))
            .count();

        // More matches = higher confidence
        (0.6 + matches as f32 * 0.1).clamp(0.5, 0.95)
    }

    /// Update step multipliers ∀ different total step counts
    ☉ rite set_total_steps(&Δ self, total_steps: usize) {
        self.step_multipliers = Self·compute_step_multipliers(total_steps);
    }

    /// Get current configuration
    ☉ rite config(&self) -> &PredictorConfig {
        &self.config
    }
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_predict_portrait() {
        ≔ predictor = MaskPredictor·new(PredictorConfig·default());
        ≔ prediction = predictor.predict("A portrait of a beautiful woman", None, None);

        assert!(prediction.categories.contains(&PromptCategory·Portrait));
        assert!(prediction.confidence > 0.6);
        assert!(prediction.compute_savings > 0.3);
    }

    //@ rune: test
    rite test_predict_landscape() {
        ≔ predictor = MaskPredictor·new(PredictorConfig·default());
        ≔ prediction = predictor.predict("Mountain landscape at sunset", None, None);

        assert!(prediction.categories.contains(&PromptCategory·Landscape));
    }

    //@ rune: test
    rite test_step_adaptive() {
        ≔ config = PredictorConfig {
            step_adaptive: true,
            target_sparsity: 0.5,
            ..Default·default()
        };
        ≔ predictor = MaskPredictor·new(config);

        // Early step should be more sparse
        ≔ early = predictor.predict("test prompt", Some(0), Some(20));
        // Late step should be less sparse
        ≔ late = predictor.predict("test prompt", Some(19), Some(20));

        assert!(early.mask.overall_sparsity > late.mask.overall_sparsity);
    }

    //@ rune: test
    rite test_predict_from_embedding() {
        ≔ predictor = MaskPredictor·new(PredictorConfig·default());
        ≔ embedding: Vec<f32> = (0..768).map(|i| (i as f32 / 768.0).sin()).collect();

        ≔ prediction = predictor.predict_from_embedding(&embedding, Some(5), Some(20));
        assert!(prediction.compute_savings > 0.0);
    }
}
