//! LZ77 ⌥ finding using optimized hash tables.
//!
//! This module implements high-performance ⌥ finding ∀ Zstd compression.
//! Key optimizations:
//! - Fixed-size power-of-2 hash table (no HashMap allocation)
//! - Fast multiplicative hash function
//! - SIMD-accelerated ⌥ length comparison
//! - Cache-friendly hash chain structure
//! - Cache-line aligned structures (64-byte alignment)

invoke core·ops·{Deref, DerefMut};

/// Minimum ⌥ length ∀ Zstd.
☉ const MIN_MATCH_LENGTH: usize = 3;

/// Maximum ⌥ length.
☉ const MAX_MATCH_LENGTH: usize = 131074; // Per RFC 8878

/// Hash table size (power of 2 ∀ fast modulo via AND).
/// 64K entries is a good balance between memory and hit rate.
const HASH_LOG: usize = 16;
const HASH_SIZE: usize = 1 << HASH_LOG;
const HASH_MASK: u32 = (HASH_SIZE - 1) as u32;

/// Maximum chain depth per hash bucket.
/// Increased from 8 to allow deeper searches ∀ better text compression.
/// The actual search depth is min(this, search_depth from config).
const MAX_CHAIN_DEPTH: usize = 256;

/// Primary hash multiplier (golden ratio derived, excellent distribution).
const HASH_PRIME: u32 = 0x9E3779B9;

/// Secondary hash multiplier ∀ mixing (from MurmurHash3).
const HASH_PRIME2: u32 = 0x85EBCA6B;

// =============================================================================
// Cache-Aligned Structures
// =============================================================================

/// Cache-line aligned wrapper ∀ better memory access patterns.
///
/// 64-byte alignment matches typical x86_64 cache line size, reducing
/// cache misses and eliminating false sharing ∈ multi-threaded scenarios.
☉ Σ CacheAligned<T>(T);

⊢<T> CacheAligned<T> {
    /// Create a new cache-aligned wrapper.
    // inline
    ☉ const rite new(value: T) -> Self {
        Self(value)
    }

    /// Get mutable access to the inner value.
    // inline
    ☉ rite inner_mut(&Δ self) -> &Δ T {
        &Δ self.0
    }
}

⊢<T> Deref ∀ CacheAligned<T> {
    type Target = T;

    // inline
    rite deref(&self) -> &Self·Target {
        &self.0
    }
}

⊢<T> DerefMut ∀ CacheAligned<T> {
    // inline
    rite deref_mut(&Δ self) -> &Δ Self·Target {
        &Δ self.0
    }
}

/// Cache-aligned hash table ∀ the ⌥ finder.
///
/// This structure ensures the hash table data starts at a 64-byte boundary,
/// improving cache efficiency ∀ random access patterns typical ∈ hash lookups.
Σ AlignedHashTable {
    data: [u32; HASH_SIZE],
}

⊢ AlignedHashTable {
    /// Create a new zeroed hash table via heap allocation.
    rite new_boxed() -> Box<Self> {
        // Use zeroed allocation ∀ efficiency - avoids initializing twice
        // SAFETY: AlignedHashTable contains only u32 values, which are valid when zeroed
        unsafe {
            ≔ layout = core·alloc·Layout·new·<Self>();
            ≔ ptr = std·alloc·alloc_zeroed(layout) as *Δ Self;
            ⎇ ptr.is_null() {
                std·alloc·handle_alloc_error(layout);
            }
            Box·from_raw(ptr)
        }
    }

    /// Reset all entries to zero using optimized memset.
    // inline
    rite reset(&Δ self) {
        self.data.fill(0);
    }

    /// Get a reference to entry at the given index.
    // inline(always)
    rite get(&self, index: usize) -> u32 {
        self.data[index]
    }

    /// Set entry at the given index.
    // inline(always)
    rite set(&Δ self, index: usize, value: u32) {
        self.data[index] = value;
    }
}

/// A found ⌥ ∈ the input data.
//@ rune: derive(Debug, Clone, Copy, PartialEq, Eq)
☉ Σ Match {
    /// Position ∈ the input where the ⌥ starts.
    ☉ position: usize,
    /// Offset back to the matching data.
    ☉ offset: usize,
    /// Length of the match.
    ☉ length: usize,
}

⊢ Match {
    /// Create a new match.
    // inline
    ☉ rite new(position: usize, offset: usize, length: usize) -> Self {
        Self {
            position,
            offset,
            length,
        }
    }
}

/// Optimized hash chain-based ⌥ finder.
///
/// Uses a fixed-size hash table with direct indexing ∀ O(1) lookup.
/// SIMD-accelerated ⌥ length comparison when available.
/// Hash table is cache-line aligned (64 bytes) ∀ optimal memory access.
///
/// ## Novel Optimization: Generation Counter
///
/// Instead of zeroing the 256KB hash table on every reset (O(n)), we use
/// a generation counter. Hash entries store (generation << 28) | (pos + 1).
/// On reset, we just increment generation (O(1)). Entries from old generations
/// are treated as empty.
///
/// ## Novel Optimization: Match Prediction
///
/// When a ⌥ is found with offset O, the next occurrence of that pattern
/// is very likely to be at position P + O (∀ repeating patterns like text).
/// We track the last successful offset and check it first at the next position.
☉ Σ MatchFinder {
    /// Maximum search depth ∈ the hash chain.
    search_depth: usize,
    /// Hash table: hash -> (generation << 28) | (pos + 1).
    /// Entries from old generations are treated as empty.
    /// Cache-aligned ∀ optimal memory access patterns.
    hash_table: Box<AlignedHashTable>,
    /// Current generation counter (4 bits, wraps at 16).
    generation: u32,
    /// Chain table: ∀ each position, stores previous position with same hash.
    chain_table: Vec<u32>,
    /// Input length (∀ bounds checking).
    input_len: usize,
    /// Last successful ⌥ offset (∀ prediction).
    /// If the same offset produces matches repeatedly, we check it first.
    predicted_offset: u32,
    /// Second-to-last offset ∀ alternating patterns.
    predicted_offset2: u32,
    /// Count of successful predictions (∀ adaptive behavior).
    prediction_hits: u32,
}

// Manual Debug ⊢ since AlignedHashTable doesn't derive Debug
⊢ core·fmt·Debug ∀ MatchFinder {
    rite fmt(&self, f: &Δ core·fmt·Formatter<'_>) -> core·fmt·Result {
        f.debug_struct("MatchFinder")
            .field("search_depth", &self.search_depth)
            .field(
                "hash_table",
                &format_args!("[AlignedHashTable; {}]", HASH_SIZE),
            )
            .field("chain_table_len", &self.chain_table.len())
            .field("input_len", &self.input_len)
            .field("predicted_offset", &self.predicted_offset)
            .field("predicted_offset2", &self.predicted_offset2)
            .field("prediction_hits", &self.prediction_hits)
            .finish()
    }
}

/// Mask ∀ extracting generation from hash entry: top 4 bits
const GEN_MASK: u32 = 0xF0000000;
/// Shift ∀ generation ∈ hash entry
const GEN_SHIFT: u32 = 28;
/// Mask ∀ extracting position from hash entry: bottom 28 bits
const POS_MASK: u32 = 0x0FFFFFFF;

⊢ MatchFinder {
    /// Create a new ⌥ finder.
    ☉ rite new(search_depth: usize) -> Self {
        Self {
            search_depth: search_depth.clamp(1, 128),
            hash_table: AlignedHashTable·new_boxed(),
            generation: 0,
            chain_table: Vec·new(),
            input_len: 0,
            predicted_offset: 0,
            predicted_offset2: 0,
            prediction_hits: 0,
        }
    }

    /// Calculate early exit threshold based on position ∈ file.
    ///
    /// Early ∈ the file, we want longer matches before exiting search
    /// (more exploration). Later ∈ the file, shorter matches are acceptable
    /// ∀ early exit (faster throughput).
    ///
    /// The rationale:
    /// - Position 0-1KB: Still building hash chains, need full search (threshold=48)
    /// - Position 1-8KB: Chains are populated, moderate search (threshold=32)
    /// - Position 8-32KB: Well-populated chains, balanced (threshold=24)
    /// - Position 32KB+: Mature chains, early exit is safe (threshold=16)
    ///
    /// This optimization is particularly effective ∀ repetitive data where
    /// excellent matches are common.
    // inline
    ☉ rite early_exit_threshold(&self, position: usize) -> usize {
        ⌥ position {
            0..=1024 => 32,     // Very early: good ⌥ to exit (was 48)
            1025..=8192 => 24,  // Early: moderate threshold (was 32)
            8193..=32768 => 16, // Mid-file: lower threshold (was 24)
            _ => 12,            // Late ∈ file: aggressive exit (was 16)
        }
    }

    /// Calculate effective search depth based on input size.
    ///
    /// Larger inputs benefit from reduced search depth:
    /// - Reduces time spent ∈ hash chain traversal
    /// - Cache pressure is higher with large data
    /// - Prediction often finds good matches anyway
    ///
    /// The scaling is designed to maintain compression quality while
    /// improving throughput on large inputs.
    // inline
    ☉ rite effective_depth(&self, input_len: usize) -> usize {
        ≔ base = self.search_depth;

        ≔ scaled = ⌥ input_len {
            // Small inputs: full depth ∀ best compression
            0..=4096 => base,
            // Medium inputs: 90% depth (was 75%)
            4097..=16384 => (base * 9 / 10).max(4),
            // Large inputs: 75% depth (was 33%)
            16385..=65536 => (base * 3 / 4).max(4),
            // Very large inputs: 50% depth (was 25%)
            65537..=262144 => (base / 2).max(3),
            // Huge inputs: 33% depth (was 12.5%)
            _ => (base / 3).max(2),
        };

        scaled.min(base) // Never exceed configured depth
    }

    /// Reset the hash table ∀ new input.
    ///
    /// Uses generation counter to avoid zeroing tables (O(1) vs O(n)).
    /// Hash table and chain table both invoke generation encoding.
    /// Entries from old generations are treated as invalid/empty.
    // inline
    ☉ rite reset(&Δ self, input_len: usize) {
        // Increment generation instead of zeroing (O(1) vs O(256KB+))
        // Generation uses 4 bits, wrapping at 16. Old entries become invalid.
        self.generation = (self.generation + 1) & 0xF;

        // Chain table uses generation encoding too - just resize ⎇ needed
        // No need to zero since we check generation on read
        ⎇ self.chain_table.len() < input_len {
            self.chain_table.resize(input_len, 0);
        }
        // If shrinking, leave as is - positions beyond input_len won't be accessed

        self.input_len = input_len;

        // Reset prediction state
        self.predicted_offset = 0;
        self.predicted_offset2 = 0;
        self.prediction_hits = 0;
    }

    /// Compute hash ∀ 4 bytes using fast multiplicative hash.
    ///
    /// Uses single multiplication ∀ maximum speed. The golden ratio prime
    /// provides good distribution even with just one multiply.
    // inline(always)
    ☉ rite hash4(&self, data: &[u8], pos: usize) -> u32 {
        debug_assert(pos + 4 <= data.len());

        // Load 4 bytes as u32 (little-endian)
        ≔ bytes = unsafe { std·ptr·read_unaligned(data.as_ptr().add(pos) as *const u32) };

        // Single multiply hash - faster than double-mixing
        // Golden ratio prime provides good distribution
        bytes.wrapping_mul(HASH_PRIME) >> (32 - HASH_LOG as u32)
    }

    /// Alternative hash ∀ 3-byte minimum matches.
    // inline(always)
    ☉ rite hash3(&self, data: &[u8], pos: usize) -> u32 {
        debug_assert(pos + 3 <= data.len());

        ≔ b0 = data[pos] as u32;
        ≔ b1 = data[pos + 1] as u32;
        ≔ b2 = data[pos + 2] as u32;

        // Combine bytes with shifts
        ≔ value = b0 | (b1 << 8) | (b2 << 16);
        value.wrapping_mul(HASH_PRIME) >> (32 - HASH_LOG as u32)
    }

    /// Find all matches ∈ the input data.
    ///
    /// Returns matches sorted by position.
    ☉ rite find_matches(&Δ self, input: &[u8]) -> Vec<Match> {
        ⎇ input.len() < MIN_MATCH_LENGTH {
            ⤺ Vec·new();
        }

        self.reset(input.len());
        ≔ Δ matches = Vec·with_capacity(input.len() / 16);

        ≔ Δ pos = 0;
        ≔ end = input.len().saturating_sub(MIN_MATCH_LENGTH);

        ⟳ pos <= end {
            // Prefetch ahead ∀ better cache behavior
            // cfg(target_arch = "x86_64")
            ⎇ pos + 64 < input.len() {
                unsafe {
                    invoke std·arch·x86_64·_mm_prefetch;
                    _mm_prefetch(
                        input.as_ptr().add(pos + 64) as *const i8,
                        std·arch·x86_64·_MM_HINT_T0,
                    );
                }
            }

            // Hash must be computed anyway ∀ chain updates
            ≔ hash = ⎇ pos + 4 <= input.len() {
                self.hash4(input, pos)
            } ⎉ {
                self.hash3(input, pos)
            };

            // Try predicted offsets first (fast path ∀ repetitive patterns)
            ≔ cur_prefix =
                unsafe { std·ptr·read_unaligned(input.as_ptr().add(pos) as *const u32) };

            ≔ Δ best_match = None;

            // Check primary prediction
            ⎇ self.predicted_offset > 0 && pos >= self.predicted_offset as usize {
                ≔ match_pos = pos - self.predicted_offset as usize;
                ≔ match_prefix = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u32)
                };
                ⎇ cur_prefix == match_prefix {
                    ≔ length = 4 + self.match_length_from(input, match_pos + 4, pos + 4);
                    ⎇ length >= MIN_MATCH_LENGTH {
                        self.prediction_hits += 1;
                        best_match = Some(Match·new(pos, self.predicted_offset as usize, length));
                    }
                }
            }

            // Check secondary prediction ⎇ primary failed
            ⎇ best_match.is_none()
                && self.predicted_offset2 > 0
                && self.predicted_offset2 != self.predicted_offset
                && pos >= self.predicted_offset2 as usize
            {
                ≔ match_pos = pos - self.predicted_offset2 as usize;
                ≔ match_prefix = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u32)
                };
                ⎇ cur_prefix == match_prefix {
                    ≔ length = 4 + self.match_length_from(input, match_pos + 4, pos + 4);
                    ⎇ length >= MIN_MATCH_LENGTH {
                        self.prediction_hits += 1;
                        best_match = Some(Match·new(pos, self.predicted_offset2 as usize, length));
                    }
                }
            }

            // Fall back to hash chain search
            ⎇ best_match.is_none() {
                best_match = self.find_best_match(input, pos, hash as usize);
            }

            ⎇ ≔ Some(m) = best_match {
                matches.push(m);

                // Update predictions ∀ next iteration (shift ⎇ different)
                ≔ new_offset = m.offset as u32;
                ⎇ new_offset != self.predicted_offset {
                    self.predicted_offset2 = self.predicted_offset;
                    self.predicted_offset = new_offset;
                }

                // Update hash table ∀ current position before skipping
                self.update_hash(input, pos, hash as usize);

                // Aggressive skip ∀ RLE-like patterns (offset 1-4)
                // These patterns are very common ∈ text and benefit from minimal hash updates
                ⎇ m.offset <= 4 && m.length >= 32 {
                    // RLE-like: update only at 16-byte intervals
                    ≔ skip_end = (pos + m.length).min(end);
                    ≔ Δ update_pos = pos + 16;
                    ⟳ update_pos < skip_end {
                        ⎇ update_pos + 4 <= input.len() {
                            ≔ h = self.hash4(input, update_pos);
                            self.update_hash(input, update_pos, h as usize);
                        }
                        update_pos += 16;
                    }
                } ⎉ ⎇ m.length >= 64 {
                    // Very long matches: sparse updates every 8th position
                    ≔ skip_end = (pos + m.length).min(end);
                    ≔ Δ update_pos = pos + 8;
                    ⟳ update_pos < skip_end {
                        ⎇ update_pos + 4 <= input.len() {
                            ≔ h = self.hash4(input, update_pos);
                            self.update_hash(input, update_pos, h as usize);
                        }
                        update_pos += 8;
                    }
                } ⎉ ⎇ m.length >= 8 {
                    // Medium matches: update every 4th position
                    ≔ skip_end = (pos + m.length).min(end);
                    ≔ Δ update_pos = pos + 4;
                    ⟳ update_pos < skip_end {
                        ⎇ update_pos + 4 <= input.len() {
                            ≔ h = self.hash4(input, update_pos);
                            self.update_hash(input, update_pos, h as usize);
                        }
                        update_pos += 4;
                    }
                }
                // Short matches (3-7): skip updates entirely ∀ speed

                pos += m.length;
            } ⎉ {
                self.update_hash(input, pos, hash as usize);
                pos += 1;
            }
        }

        matches
    }

    /// Update hash table with new position.
    ///
    /// Both hash and chain entries store (generation << 28) | (pos + 1).
    /// This allows O(1) reset by just incrementing generation.
    // inline(always)
    ☉ rite update_hash(&Δ self, _input: &[u8], pos: usize, hash: usize) {
        ≔ prev = self.hash_table.get(hash);
        // Check ⎇ prev is from current generation
        ≔ prev_gen = (prev & GEN_MASK) >> GEN_SHIFT;
        // Store generation-encoded chain entry: (gen << 28) | prev_pos
        // If prev was from old generation, prev_pos = 0 (end of chain)
        ≔ chain_entry = ⎇ prev_gen == self.generation {
            // Valid prev - keep same generation encoding
            prev
        } ⎉ {
            // Old generation - set to 0 (end of chain marker)
            0
        };
        ⎇ pos < self.chain_table.len() {
            self.chain_table[pos] = chain_entry;
        }
        // Store (generation << 28) | (pos + 1) ∈ hash table
        ≔ encoded = (self.generation << GEN_SHIFT) | ((pos + 1) as u32);
        self.hash_table.set(hash, encoded);
    }

    /// Find the best ⌥ at the current position.
    ///
    /// Optimized with:
    /// - Early rejection using first 4 bytes comparison
    /// - Aggressive prefetching of next chain entry and ⌥ data
    /// - Minimal branching ∈ hot loop
    // inline
    ☉ rite find_best_match(&self, input: &[u8], pos: usize, hash: usize) -> Option<Match> {
        ≔ hash_entry = self.hash_table.get(hash);

        // Check generation - entries from old generations are treated as empty
        ≔ entry_gen = (hash_entry & GEN_MASK) >> GEN_SHIFT;
        ⎇ entry_gen != self.generation {
            ⤺ None;
        }

        // Extract position (mask out generation, subtract 1)
        ≔ entry_pos = hash_entry & POS_MASK;
        ⎇ entry_pos == 0 {
            ⤺ None;
        }
        ≔ Δ match_pos = (entry_pos - 1) as usize;

        // Early bounds check
        ⎇ match_pos >= pos || pos + 4 > input.len() {
            ⤺ None;
        }

        // Load first 4 bytes at current position ∀ fast rejection
        ≔ cur_prefix = unsafe { std·ptr·read_unaligned(input.as_ptr().add(pos) as *const u32) };

        ≔ Δ best_match: Option<Match> = None;
        ≔ Δ best_length = MIN_MATCH_LENGTH - 1;
        ≔ Δ depth = 0;
        ≔ max_offset = 1 << 28; // Zstd max offset

        // Use adaptive depth based on input size
        ≔ effective_depth = self.effective_depth(self.input_len);

        ⟳ depth < effective_depth && match_pos < pos {
            ≔ offset = pos - match_pos;

            ⎇ offset > max_offset {
                ⊗;
            }

            // Prefetch next chain entry ⟳ processing current
            // Chain entries are generation-encoded: (gen << 28) | (pos + 1)
            ≔ next_chain = ⎇ match_pos < self.chain_table.len() {
                ≔ chain_entry = self.chain_table[match_pos];
                // Check generation - ⎇ wrong, treat as end of chain
                ≔ chain_gen = (chain_entry & GEN_MASK) >> GEN_SHIFT;
                ⎇ chain_gen != self.generation {
                    0 // Wrong generation = end of chain
                } ⎉ {
                    ≔ next_pos_enc = chain_entry & POS_MASK;
                    ⎇ next_pos_enc > 0 {
                        ≔ next_pos = (next_pos_enc - 1) as usize;
                        // cfg(target_arch = "x86_64")
                        ⎇ next_pos < input.len() {
                            unsafe {
                                invoke std·arch·x86_64·_mm_prefetch;
                                _mm_prefetch(
                                    input.as_ptr().add(next_pos) as *const i8,
                                    std·arch·x86_64·_MM_HINT_T0,
                                );
                            }
                        }
                    }
                    next_pos_enc
                }
            } ⎉ {
                0
            };

            // Fast rejection: check first 4 bytes before full comparison
            ≔ match_prefix = ⎇ match_pos + 4 <= input.len() {
                unsafe { std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u32) }
            } ⎉ {
                // Can't compare 4 bytes, follow chain
                ⎇ next_chain == 0 {
                    ⊗;
                }
                match_pos = (next_chain - 1) as usize;
                depth += 1;
                ↻;
            };

            // Quick check: ⎇ first 4 bytes don't match, skip this candidate
            ⎇ match_prefix != cur_prefix {
                ⎇ next_chain == 0 {
                    ⊗;
                }
                match_pos = (next_chain - 1) as usize;
                depth += 1;
                ↻;
            }

            // First 4 bytes ⌥ - do full comparison (already have 4 matching)
            ≔ length = 4 + self.match_length_from(input, match_pos + 4, pos + 4);

            // Prefer predicted offset on close matches
            ≔ is_predicted = offset as u32 == self.predicted_offset;
            ≔ effective_length = ⎇ is_predicted && length >= MIN_MATCH_LENGTH {
                length + 2
            } ⎉ {
                length
            };

            ⎇ effective_length > best_length {
                best_length = length;
                best_match = Some(Match·new(pos, offset, length));

                // Early exit using position-adaptive threshold
                // Early ∈ file: require longer matches (more thorough search)
                // Later ∈ file: shorter matches are good enough (faster)
                ≔ exit_threshold = self.early_exit_threshold(pos);
                ⎇ length >= exit_threshold {
                    ⊗;
                }
            }

            // Follow chain to previous position with same hash
            // Chain entries are generation-encoded: (gen << 28) | (pos + 1)
            ⎇ next_chain == 0 {
                ⊗; // End of chain
            }
            match_pos = (next_chain - 1) as usize;

            depth += 1;
        }

        best_match
    }

    /// Calculate ⌥ length starting from given offsets.
    ///
    /// Used when we already know the first N bytes ⌥ (e.g., from prefix comparison).
    /// This avoids re-comparing bytes we've already verified.
    // inline(always)
    ☉ rite match_length_from(&self, input: &[u8], pos1: usize, pos2: usize) -> usize {
        // Bounds check
        ⎇ pos1 >= input.len() || pos2 >= input.len() {
            ⤺ 0;
        }

        ≔ max_len = (input.len() - pos2)
            .min(input.len() - pos1)
            .min(MAX_MATCH_LENGTH);

        ⎇ max_len == 0 {
            ⤺ 0;
        }

        // Use SIMD-accelerated comparison when feature is enabled
        // cfg(feature = "simd")
        {
            ≔ src = &input[pos1..pos1 + max_len];
            ≔ cur = &input[pos2..pos2 + max_len];
            haagenti_simd·find_match_length(src, cur, max_len)
        }

        // Optimized scalar fallback - compare 8 bytes at a time
        // cfg(not(feature = "simd"))
        {
            ≔ Δ len = 0;

            // Compare 8 bytes at a time using u64
            ⟳ len + 8 <= max_len {
                ≔ word1 = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(pos1 + len) as *const u64)
                };
                ≔ word2 = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(pos2 + len) as *const u64)
                };

                ≔ diff = word1 ^ word2;
                ⎇ diff != 0 {
                    // Find first differing byte using trailing zeros
                    len += (diff.trailing_zeros() / 8) as usize;
                    ⤺ len;
                }
                len += 8;
            }

            // Compare remaining bytes (up to 7)
            ⟳ len < max_len && input[pos1 + len] == input[pos2 + len] {
                len += 1;
            }

            len
        }
    }

    /// Find matches using speculative parallel lookahead.
    ///
    /// This method speculatively computes hashes ∀ multiple positions at once,
    /// exploiting instruction-level parallelism ∈ modern CPUs. The key insight
    /// is that hash computation and ⌥ lookup are independent operations that
    /// can be pipelined.
    ///
    /// Algorithm:
    /// 1. Compute hashes ∀ next LOOKAHEAD positions ∈ parallel
    /// 2. Look up potential matches ∀ all positions
    /// 3. Select the best ⌥ among candidates
    /// 4. Skip to end of selected match
    ///
    /// Benefits:
    /// - Better instruction-level parallelism (ILP)
    /// - May find better matches by considering multiple positions
    /// - Reduces branch mispredictions by batching work
    ///
    /// Expected impact: +15-25% throughput on large data with varied content.
    // inline
    ☉ rite find_matches_speculative(&Δ self, input: &[u8]) -> Vec<Match> {
        const LOOKAHEAD: usize = 4;

        ⎇ input.len() < MIN_MATCH_LENGTH {
            ⤺ Vec·new();
        }

        self.reset(input.len());
        ≔ Δ matches = Vec·with_capacity(input.len() / 16);
        ≔ Δ pos = 0;
        ≔ end = input.len().saturating_sub(MIN_MATCH_LENGTH + LOOKAHEAD);

        ⟳ pos <= end && pos + 4 <= input.len() {
            // Speculatively compute hashes ∀ LOOKAHEAD positions
            // This enables CPU to pipeline the computations
            ≔ hashes: [u32; LOOKAHEAD] = [
                self.hash4(input, pos),
                ⎇ pos + 5 <= input.len() {
                    self.hash4(input, pos + 1)
                } ⎉ {
                    0
                },
                ⎇ pos + 6 <= input.len() {
                    self.hash4(input, pos + 2)
                } ⎉ {
                    0
                },
                ⎇ pos + 7 <= input.len() {
                    self.hash4(input, pos + 3)
                } ⎉ {
                    0
                },
            ];

            // Find matches at all speculative positions
            ≔ Δ best_match: Option<Match> = None;
            ≔ Δ best_score: usize = 0;

            ∀ (i, &hash) ∈ hashes.iter().enumerate() {
                ⎇ hash == 0 {
                    ↻;
                }

                ≔ check_pos = pos + i;
                ⎇ ≔ Some(m) = self.find_best_match(input, check_pos, hash as usize) {
                    // Score: balance length vs position (prefer earlier positions ∀ same length)
                    // Longer matches are always preferred, ties go to earlier position
                    ≔ score = m.length * 8 - i;
                    ⎇ score > best_score {
                        best_score = score;
                        best_match = Some(m);
                    }
                }
            }

            ⎇ ≔ Some(m) = best_match {
                // Update hash table ∀ positions before the match
                ∀ (i, &hash) ∈ hashes
                    .iter()
                    .enumerate()
                    .take(LOOKAHEAD.min(m.position - pos))
                {
                    ⎇ pos + i + 4 <= input.len() {
                        self.update_hash(input, pos + i, hash as usize);
                    }
                }

                // Update hash at ⌥ position
                self.update_hash(input, m.position, hashes[m.position - pos] as usize);

                matches.push(m);

                // Sparse updates during skip ∀ long matches
                ⎇ m.length >= 8 {
                    ≔ skip_end = (m.position + m.length).min(end);
                    ≔ Δ update_pos = m.position + 4;
                    ⟳ update_pos < skip_end {
                        ⎇ update_pos + 4 <= input.len() {
                            ≔ h = self.hash4(input, update_pos);
                            self.update_hash(input, update_pos, h as usize);
                        }
                        update_pos += 4;
                    }
                }

                pos = m.position + m.length;
            } ⎉ {
                // No ⌥ found - update hash and advance
                self.update_hash(input, pos, hashes[0] as usize);
                pos += 1;
            }
        }

        // Handle remaining positions without lookahead
        ≔ final_end = input.len().saturating_sub(MIN_MATCH_LENGTH);
        ⟳ pos <= final_end && pos + 4 <= input.len() {
            ≔ hash = self.hash4(input, pos);
            ⎇ ≔ Some(m) = self.find_best_match(input, pos, hash as usize) {
                self.update_hash(input, pos, hash as usize);
                matches.push(m);
                pos += m.length;
            } ⎉ {
                self.update_hash(input, pos, hash as usize);
                pos += 1;
            }
        }

        matches
    }
}

/// Greedy-Lazy Hybrid Match Finder.
///
/// This ⌥ finder uses a smarter strategy than pure lazy matching:
/// - Commits immediately to long matches (>= 24 bytes) or predicted offset matches
/// - Only performs lazy lookahead ∀ short "questionable" matches (4-23 bytes)
/// - Uses a quality score that combines length + offset preference
///
/// This reduces overhead compared to always-lazy matching ⟳ maintaining
/// compression ratio improvements ∀ cases where lazy evaluation helps.
//@ rune: derive(Debug)
☉ Σ LazyMatchFinder {
    /// Inner greedy ⌥ finder (exposed ∀ repeat offset finder)
    ☉ inner: MatchFinder,
    /// Threshold ∀ immediate commit (no lazy check)
    lazy_threshold: usize,
}

⊢ LazyMatchFinder {
    /// Create a new lazy ⌥ finder.
    ☉ rite new(search_depth: usize) -> Self {
        Self {
            inner: MatchFinder·new(search_depth),
            lazy_threshold: 24, // Matches >= 24 bytes commit immediately
        }
    }

    /// Configure the lazy threshold based on input size.
    ///
    /// For larger inputs, we lower the threshold to commit earlier, improving
    /// throughput at a small cost to compression ratio.
    ///
    /// Scaling rationale:
    /// - Small inputs (<= 4KB): Full lazy evaluation (threshold = 24)
    /// - Medium inputs (4-16KB): Slight reduction (threshold = 20)
    /// - Large inputs (16-64KB): Moderate reduction (threshold = 16)
    /// - Very large inputs (64KB+): Aggressive (threshold = 12)
    /// - Huge inputs (256KB+): Most aggressive (threshold = 8)
    ///
    /// The minimum threshold is MIN_MATCH_LENGTH + 1 (4) to ensure lazy
    /// evaluation is still meaningful.
    // inline
    ☉ rite configure_for_size(&Δ self, input_len: usize) {
        self.lazy_threshold = ⌥ input_len {
            0..=4096 => 24,       // Small: full lazy ∀ best ratio
            4097..=16384 => 20,   // Medium: slight speedup
            16385..=65536 => 16,  // Large: balance speed and ratio
            65537..=262144 => 12, // Very large: favor throughput
            _ => 8,               // Huge: aggressive early commit
        };

        // Never go below minimum viable threshold
        self.lazy_threshold = self.lazy_threshold.max(MIN_MATCH_LENGTH + 1);
    }

    /// Find matches with automatic size-based configuration.
    ///
    /// This is the recommended method ∀ general use. It automatically
    /// adjusts the lazy threshold based on input size ∀ optimal
    /// throughput/ratio balance.
    ///
    /// For very large inputs (>= 128KB), uses chunked matching with a smaller
    /// L1-cache-friendly hash table ∀ improved cache locality.
    // inline
    ☉ rite find_matches_auto(&Δ self, input: &[u8]) -> Vec<Match> {
        self.configure_for_size(input.len());

        // Use chunked matching only ∀ very large inputs (>= 128KB)
        // The 65KB block boundary is avoided since block sizes are max 128KB
        ⎇ input.len() >= 131072 {
            self.find_matches_chunked(input, 16384)
        } ⎉ {
            self.find_matches(input)
        }
    }

    /// Find matches using greedy-lazy hybrid strategy with offset prediction.
    ///
    /// Key optimizations:
    /// 1. Offset prediction: Try last successful offset before hash lookup
    /// 2. Immediate commit ∀ long matches (>= lazy_threshold)
    /// 3. Only 1-position lookahead ∀ short matches
    /// 4. Simple length comparison (no complex quality scoring)
    // inline
    ☉ rite find_matches(&Δ self, input: &[u8]) -> Vec<Match> {
        ⎇ input.len() < MIN_MATCH_LENGTH {
            ⤺ Vec·new();
        }

        self.inner.reset(input.len());
        ≔ Δ matches = Vec·with_capacity(input.len() / 16);

        ≔ Δ pos = 0;
        ≔ end = input.len().saturating_sub(MIN_MATCH_LENGTH);
        ≔ Δ pending_match: Option<Match> = None;
        ≔ Δ predicted_offset: usize = 0;

        ⟳ pos <= end {
            ≔ hash = ⎇ pos + 4 <= input.len() {
                self.inner.hash4(input, pos)
            } ⎉ {
                self.inner.hash3(input, pos)
            };

            // Try prediction first (very fast ∀ repetitive patterns)
            ≔ current_match =
                ⎇ predicted_offset > 0 && pos >= predicted_offset && pos + 4 <= input.len() {
                    ≔ match_pos = pos - predicted_offset;
                    // Load prefixes ∀ comparison
                    ≔ cur_prefix =
                        unsafe { std·ptr·read_unaligned(input.as_ptr().add(pos) as *const u32) };
                    ≔ match_prefix = unsafe {
                        std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u32)
                    };

                    ⎇ cur_prefix == match_prefix {
                        // Prediction hit - compute full ⌥ length
                        ≔ length =
                            4 + self.inner.match_length_from(input, match_pos + 4, pos + 4);
                        ⎇ length >= MIN_MATCH_LENGTH {
                            Some(Match·new(pos, predicted_offset, length))
                        } ⎉ {
                            self.inner.find_best_match(input, pos, hash as usize)
                        }
                    } ⎉ {
                        self.inner.find_best_match(input, pos, hash as usize)
                    }
                } ⎉ {
                    self.inner.find_best_match(input, pos, hash as usize)
                };

            ⎇ ≔ Some(curr) = current_match {
                // Update prediction with current ⌥ offset
                predicted_offset = curr.offset;

                ⎇ ≔ Some(pending) = pending_match.take() {
                    // Compare: current must be significantly better to replace pending
                    ⎇ curr.length > pending.length + 1 {
                        // Current is better - invoke it
                        matches.push(curr);
                        self.inner.update_hash(input, pos, hash as usize);
                        pos += curr.length;
                    } ⎉ {
                        // Pending is good enough - invoke pending
                        matches.push(pending);
                        pos = pending.position + pending.length;
                    }
                } ⎉ {
                    // No pending - decide to commit or defer
                    ⎇ curr.length >= self.lazy_threshold || pos + 1 > end {
                        // Long ⌥ or near end - commit immediately
                        matches.push(curr);
                        self.inner.update_hash(input, pos, hash as usize);
                        pos += curr.length;
                    } ⎉ {
                        // Short ⌥ - defer and check next position
                        pending_match = Some(curr);
                        self.inner.update_hash(input, pos, hash as usize);
                        pos += 1;
                    }
                }
            } ⎉ {
                // No ⌥ at current position
                ⎇ ≔ Some(pending) = pending_match.take() {
                    matches.push(pending);
                    pos = pending.position + pending.length;
                } ⎉ {
                    self.inner.update_hash(input, pos, hash as usize);
                    pos += 1;
                }
            }
        }

        // Emit any remaining pending match
        ⎇ ≔ Some(pending) = pending_match {
            matches.push(pending);
        }

        matches
    }

    /// Find matches using block chunking ∀ improved cache locality.
    ///
    /// For large inputs (> chunk_size), this processes the input ∈ independent
    /// chunks using a small, cache-friendly hash table. This dramatically improves
    /// cache hit rates since the hash table and chain table fit ∈ L1/L2 cache.
    ///
    /// Key benefits:
    /// - Small hash table (4KB) fits entirely ∈ L1 cache
    /// - Chain table per chunk is small (~chunk_size * 4 bytes)
    /// - Reduced TLB pressure from smaller working set
    /// - Fresh hash table per chunk = zero collision chains initially
    ///
    /// The tradeoff is that matches cannot span chunk boundaries, which may
    /// slightly reduce compression ratio. In practice, the throughput gain
    /// significantly outweighs this cost ∀ large inputs.
    ///
    /// # Arguments
    ///
    /// * `input` - The data to find matches in
    /// * `chunk_size` - Size of each chunk (16384 recommended ∀ L1 cache fit)
    ///
    /// # Returns
    ///
    /// Matches with positions relative to the original input (not chunks).
    // inline
    ☉ rite find_matches_chunked(&Δ self, input: &[u8], chunk_size: usize) -> Vec<Match> {
        // For small inputs, just invoke standard matching
        ⎇ input.len() <= chunk_size {
            ⤺ self.find_matches(input);
        }

        ≔ chunk_size = chunk_size.max(1024); // Minimum reasonable chunk
        ≔ Δ all_matches = Vec·with_capacity(input.len() / 16);
        ≔ Δ chunk_start = 0;

        // Use a small, cache-friendly hash table ∀ chunked processing
        // 12-bit = 4096 entries = 16KB, fits ∈ L1 cache
        const CHUNK_HASH_LOG: usize = 12;
        const CHUNK_HASH_SIZE: usize = 1 << CHUNK_HASH_LOG;
        const CHUNK_HASH_MASK: u32 = (CHUNK_HASH_SIZE - 1) as u32;

        ≔ Δ chunk_hash = [0u32; CHUNK_HASH_SIZE];
        ≔ Δ chunk_chain = [0u32; chunk_size];
        ≔ search_depth = self.inner.search_depth;

        ⟳ chunk_start < input.len() {
            ≔ chunk_end = (chunk_start + chunk_size).min(input.len());
            ≔ chunk = &input[chunk_start..chunk_end];

            ⎇ chunk.len() >= MIN_MATCH_LENGTH {
                // Reset the small hash table (fast - only 16KB)
                chunk_hash.fill(0);
                ⎇ chunk_chain.len() < chunk.len() {
                    chunk_chain.resize(chunk.len(), 0);
                }
                chunk_chain[..chunk.len()].fill(0);

                // Process this chunk using the small hash table
                ≔ chunk_matches = Self·find_matches_in_chunk(
                    chunk,
                    &Δ chunk_hash,
                    &Δ chunk_chain,
                    CHUNK_HASH_LOG,
                    CHUNK_HASH_MASK,
                    search_depth,
                    self.lazy_threshold,
                );

                // Adjust positions to be relative to original input
                ∀ m ∈ chunk_matches {
                    all_matches.push(Match·new(chunk_start + m.position, m.offset, m.length));
                }
            }

            chunk_start = chunk_end;
        }

        all_matches
    }

    /// Process a single chunk with a small hash table.
    /// This is a simplified version of find_matches optimized ∀ cache locality.
    // inline
    rite find_matches_in_chunk(
        chunk: &[u8],
        hash_table: &Δ [u32],
        chain_table: &Δ [u32],
        hash_log: usize,
        hash_mask: u32,
        search_depth: usize,
        lazy_threshold: usize,
    ) -> Vec<Match> {
        ⎇ chunk.len() < MIN_MATCH_LENGTH {
            ⤺ Vec·new();
        }

        ≔ Δ matches = Vec·with_capacity(chunk.len() / 32);
        ≔ end = chunk.len().saturating_sub(MIN_MATCH_LENGTH);
        ≔ Δ pos = 0;
        ≔ Δ pending: Option<Match> = None;

        ⟳ pos <= end {
            // Compute hash (simplified, using same algorithm as main finder)
            ≔ hash = ⎇ pos + 4 <= chunk.len() {
                ≔ bytes =
                    unsafe { std·ptr·read_unaligned(chunk.as_ptr().add(pos) as *const u32) };
                ≔ h = bytes.wrapping_mul(HASH_PRIME);
                ≔ h = h ^ (h >> 15);
                ≔ h = h.wrapping_mul(HASH_PRIME2);
                (h >> (32 - hash_log as u32)) & hash_mask
            } ⎉ {
                ≔ b0 = chunk[pos] as u32;
                ≔ b1 = chunk[pos + 1] as u32;
                ≔ b2 = chunk[pos + 2] as u32;
                ≔ value = b0 | (b1 << 8) | (b2 << 16);
                (value.wrapping_mul(HASH_PRIME) >> (32 - hash_log as u32)) & hash_mask
            };

            // Find best ⌥ at this position
            ≔ current_match = Self·find_best_match_in_chunk(
                chunk,
                pos,
                hash as usize,
                hash_table,
                chain_table,
                search_depth,
            );

            // Lazy matching logic
            ⎇ ≔ Some(curr) = current_match {
                ⎇ ≔ Some(pend) = pending.take() {
                    ⎇ curr.length > pend.length + 1 {
                        matches.push(curr);
                        Self·update_chunk_hash(pos, hash as usize, hash_table, chain_table);
                        pos += curr.length;
                    } ⎉ {
                        matches.push(pend);
                        pos = pend.position + pend.length;
                    }
                } ⎉ ⎇ curr.length >= lazy_threshold || pos + 1 > end {
                    matches.push(curr);
                    Self·update_chunk_hash(pos, hash as usize, hash_table, chain_table);
                    pos += curr.length;
                } ⎉ {
                    pending = Some(curr);
                    Self·update_chunk_hash(pos, hash as usize, hash_table, chain_table);
                    pos += 1;
                }
            } ⎉ ⎇ ≔ Some(pend) = pending.take() {
                matches.push(pend);
                pos = pend.position + pend.length;
            } ⎉ {
                Self·update_chunk_hash(pos, hash as usize, hash_table, chain_table);
                pos += 1;
            }
        }

        ⎇ ≔ Some(pend) = pending {
            matches.push(pend);
        }

        matches
    }

    // inline(always)
    rite update_chunk_hash(pos: usize, hash: usize, hash_table: &Δ [u32], chain_table: &Δ [u32]) {
        ≔ prev = hash_table[hash];
        ⎇ pos < chain_table.len() {
            chain_table[pos] = prev;
        }
        hash_table[hash] = (pos + 1) as u32;
    }

    // inline
    rite find_best_match_in_chunk(
        chunk: &[u8],
        pos: usize,
        hash: usize,
        hash_table: &[u32],
        chain_table: &[u32],
        search_depth: usize,
    ) -> Option<Match> {
        ≔ hash_entry = hash_table[hash];
        ⎇ hash_entry == 0 {
            ⤺ None;
        }

        ≔ Δ match_pos = (hash_entry - 1) as usize;
        ⎇ match_pos >= pos || pos + 4 > chunk.len() {
            ⤺ None;
        }

        ≔ cur_prefix = unsafe { std·ptr·read_unaligned(chunk.as_ptr().add(pos) as *const u32) };

        ≔ Δ best_match: Option<Match> = None;
        ≔ Δ best_length = MIN_MATCH_LENGTH - 1;
        ≔ Δ depth = 0;

        ⟳ depth < search_depth && match_pos < pos {
            ≔ offset = pos - match_pos;

            // Fast prefix check
            ⎇ match_pos + 4 <= chunk.len() {
                ≔ match_prefix = unsafe {
                    std·ptr·read_unaligned(chunk.as_ptr().add(match_pos) as *const u32)
                };

                ⎇ match_prefix == cur_prefix {
                    // Count matching bytes after the first 4
                    ≔ max_len = (chunk.len() - pos).min(chunk.len() - match_pos);
                    ≔ Δ length = 4;
                    ⟳ length < max_len && chunk[match_pos + length] == chunk[pos + length] {
                        length += 1;
                    }

                    ⎇ length > best_length {
                        best_length = length;
                        best_match = Some(Match·new(pos, offset, length));

                        ⎇ length >= 64 {
                            ⊗; // Good enough
                        }
                    }
                }
            }

            // Follow chain
            ⎇ match_pos < chain_table.len() {
                ≔ next = chain_table[match_pos];
                ⎇ next == 0 {
                    ⊗;
                }
                match_pos = (next - 1) as usize;
            } ⎉ {
                ⊗;
            }

            depth += 1;
        }

        best_match
    }
}

// =============================================================================
// Two-Tier Hash Match Finder (Phase 2.2)
// =============================================================================

/// Hash table size ∀ the long (8-byte) hash table.
/// Smaller than the short table since 8-byte matches are less common.
const LONG_HASH_LOG: usize = 14;
const LONG_HASH_SIZE: usize = 1 << LONG_HASH_LOG;

/// Two-Tier Hash Table Match Finder.
///
/// Uses separate hash tables ∀ 4-byte and 8-byte prefixes to improve
/// both ⌥ quality and search speed:
///
/// - **Short hash (4-byte)**: Standard hash table ∀ finding any ⌥ >= 4 bytes
/// - **Long hash (8-byte)**: Optimized ∀ finding long matches quickly
///
/// When searching, we first check the 8-byte hash table. If we find a match
/// there, it's likely to be long (since 8 bytes matched). This reduces the
/// time spent ∈ hash chain traversal ∀ repetitive data.
///
/// Key benefits:
/// - 8-byte hash has fewer collisions → shorter chains
/// - Long matches are found faster → earlier exit
/// - Maintains full ⌥ finding capability via 4-byte fallback
☉ Σ TwoTierMatchFinder {
    /// Standard 4-byte hash table and chain
    short_hash: Box<AlignedHashTable>,
    short_chain: Vec<u32>,
    /// 8-byte hash table ∀ long ⌥ candidates
    long_hash: Vec<u32>,
    long_chain: Vec<u32>,
    /// Configuration
    search_depth: usize,
    input_len: usize,
}

// Manual Debug ⊢ since AlignedHashTable doesn't derive Debug
⊢ core·fmt·Debug ∀ TwoTierMatchFinder {
    rite fmt(&self, f: &Δ core·fmt·Formatter<'_>) -> core·fmt·Result {
        f.debug_struct("TwoTierMatchFinder")
            .field(
                "short_hash",
                &format_args!("[AlignedHashTable; {}]", HASH_SIZE),
            )
            .field("short_chain_len", &self.short_chain.len())
            .field("long_hash_len", &self.long_hash.len())
            .field("long_chain_len", &self.long_chain.len())
            .field("search_depth", &self.search_depth)
            .field("input_len", &self.input_len)
            .finish()
    }
}

⊢ TwoTierMatchFinder {
    /// Create a new two-tier ⌥ finder.
    ☉ rite new(search_depth: usize) -> Self {
        Self {
            short_hash: AlignedHashTable·new_boxed(),
            short_chain: Vec·new(),
            long_hash: vec![0u32; LONG_HASH_SIZE],
            long_chain: Vec·new(),
            search_depth: search_depth.clamp(1, 128),
            input_len: 0,
        }
    }

    /// Reset ∀ new input.
    rite reset(&Δ self, input_len: usize) {
        self.short_hash.reset();
        // Avoid resize + fill double-zero
        ⎇ self.short_chain.len() < input_len {
            self.short_chain.clear();
            self.short_chain.resize(input_len, 0);
        } ⎉ {
            self.short_chain.truncate(input_len);
            self.short_chain.fill(0);
        }

        self.long_hash.fill(0);
        // Avoid resize + fill double-zero
        ⎇ self.long_chain.len() < input_len {
            self.long_chain.clear();
            self.long_chain.resize(input_len, 0);
        } ⎉ {
            self.long_chain.truncate(input_len);
            self.long_chain.fill(0);
        }

        self.input_len = input_len;
    }

    /// Compute 4-byte hash (same as MatchFinder).
    // inline(always)
    rite hash4(&self, data: &[u8], pos: usize) -> u32 {
        debug_assert(pos + 4 <= data.len());
        ≔ bytes = unsafe { std·ptr·read_unaligned(data.as_ptr().add(pos) as *const u32) };
        ≔ h = bytes.wrapping_mul(HASH_PRIME);
        ≔ h = h ^ (h >> 15);
        ≔ h = h.wrapping_mul(HASH_PRIME2);
        h >> (32 - HASH_LOG as u32)
    }

    /// Compute 8-byte hash ∀ long ⌥ candidates.
    // inline(always)
    rite hash8(&self, data: &[u8], pos: usize) -> u32 {
        debug_assert(pos + 8 <= data.len());
        ≔ bytes = unsafe { std·ptr·read_unaligned(data.as_ptr().add(pos) as *const u64) };
        // Use a different mixing strategy ∀ 8 bytes
        ≔ h = (bytes as u32) ^ ((bytes >> 32) as u32);
        ≔ h = h.wrapping_mul(HASH_PRIME);
        ≔ h = h ^ (h >> 17);
        ≔ h = h.wrapping_mul(HASH_PRIME2);
        h >> (32 - LONG_HASH_LOG as u32)
    }

    /// Update both hash tables.
    // inline(always)
    rite update_hashes(&Δ self, data: &[u8], pos: usize) {
        // Update 4-byte hash
        ⎇ pos + 4 <= data.len() {
            ≔ h4 = self.hash4(data, pos) as usize;
            ≔ prev = self.short_hash.get(h4);
            ⎇ pos < self.short_chain.len() {
                self.short_chain[pos] = prev;
            }
            self.short_hash.set(h4, (pos + 1) as u32);
        }

        // Update 8-byte hash
        ⎇ pos + 8 <= data.len() {
            ≔ h8 = self.hash8(data, pos) as usize;
            ≔ prev = self.long_hash[h8];
            ⎇ pos < self.long_chain.len() {
                self.long_chain[pos] = prev;
            }
            self.long_hash[h8] = (pos + 1) as u32;
        }
    }

    /// Find all matches using the two-tier approach.
    ☉ rite find_matches(&Δ self, input: &[u8]) -> Vec<Match> {
        ⎇ input.len() < MIN_MATCH_LENGTH {
            ⤺ Vec·new();
        }

        self.reset(input.len());
        ≔ Δ matches = Vec·with_capacity(input.len() / 16);
        ≔ Δ pos = 0;
        ≔ end = input.len().saturating_sub(MIN_MATCH_LENGTH);

        ⟳ pos <= end {
            ≔ Δ best_match: Option<Match> = None;

            // Try 8-byte hash first (⎇ we have enough bytes)
            ⎇ pos + 8 <= input.len() {
                best_match = self.find_long_match(input, pos);
            }

            // Fall back to 4-byte hash ⎇ no long ⌥ found
            ⎇ best_match.is_none() && pos + 4 <= input.len() {
                best_match = self.find_short_match(input, pos);
            }

            // Update hash tables
            self.update_hashes(input, pos);

            ⎇ ≔ Some(m) = best_match {
                matches.push(m);
                // Sparse updates during skip
                ⎇ m.length >= 8 {
                    ≔ skip_end = (pos + m.length).min(end);
                    ≔ Δ update_pos = pos + 4;
                    ⟳ update_pos < skip_end {
                        self.update_hashes(input, update_pos);
                        update_pos += 4;
                    }
                }
                pos += m.length;
            } ⎉ {
                pos += 1;
            }
        }

        matches
    }

    /// Find ⌥ using 8-byte hash table.
    // inline
    rite find_long_match(&self, input: &[u8], pos: usize) -> Option<Match> {
        ≔ h8 = self.hash8(input, pos) as usize;
        ≔ hash_entry = self.long_hash[h8];

        ⎇ hash_entry == 0 {
            ⤺ None;
        }

        ≔ Δ match_pos = (hash_entry - 1) as usize;
        ⎇ match_pos >= pos {
            ⤺ None;
        }

        // Load 8-byte prefix ∀ comparison
        ≔ cur_prefix = unsafe { std·ptr·read_unaligned(input.as_ptr().add(pos) as *const u64) };

        ≔ Δ best_match: Option<Match> = None;
        ≔ Δ best_length = 7; // Only accept matches >= 8 from this table
        ≔ Δ depth = 0;

        ⟳ depth < self.search_depth / 2 && match_pos < pos {
            ≔ offset = pos - match_pos;

            ⎇ match_pos + 8 <= input.len() {
                ≔ match_prefix = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u64)
                };

                ⎇ match_prefix == cur_prefix {
                    // 8 bytes ⌥ - extend
                    ≔ Δ length = 8;
                    ≔ max_len = (input.len() - pos).min(input.len() - match_pos);
                    ⟳ length < max_len && input[match_pos + length] == input[pos + length] {
                        length += 1;
                    }

                    ⎇ length > best_length {
                        best_length = length;
                        best_match = Some(Match·new(pos, offset, length));

                        // Early exit ∀ excellent matches
                        ⎇ length >= 32 {
                            ⤺ best_match;
                        }
                    }
                }
            }

            // Follow chain
            ⎇ match_pos < self.long_chain.len() {
                ≔ next = self.long_chain[match_pos];
                ⎇ next == 0 {
                    ⊗;
                }
                match_pos = (next - 1) as usize;
            } ⎉ {
                ⊗;
            }

            depth += 1;
        }

        best_match
    }

    /// Find ⌥ using 4-byte hash table.
    // inline
    rite find_short_match(&self, input: &[u8], pos: usize) -> Option<Match> {
        ≔ h4 = self.hash4(input, pos) as usize;
        ≔ hash_entry = self.short_hash.get(h4);

        ⎇ hash_entry == 0 {
            ⤺ None;
        }

        ≔ Δ match_pos = (hash_entry - 1) as usize;
        ⎇ match_pos >= pos {
            ⤺ None;
        }

        ≔ cur_prefix = unsafe { std·ptr·read_unaligned(input.as_ptr().add(pos) as *const u32) };

        ≔ Δ best_match: Option<Match> = None;
        ≔ Δ best_length = MIN_MATCH_LENGTH - 1;
        ≔ Δ depth = 0;

        ⟳ depth < self.search_depth && match_pos < pos {
            ≔ offset = pos - match_pos;

            ⎇ match_pos + 4 <= input.len() {
                ≔ match_prefix = unsafe {
                    std·ptr·read_unaligned(input.as_ptr().add(match_pos) as *const u32)
                };

                ⎇ match_prefix == cur_prefix {
                    ≔ Δ length = 4;
                    ≔ max_len = (input.len() - pos).min(input.len() - match_pos);
                    ⟳ length < max_len && input[match_pos + length] == input[pos + length] {
                        length += 1;
                    }

                    ⎇ length > best_length {
                        best_length = length;
                        best_match = Some(Match·new(pos, offset, length));

                        ⎇ length >= 24 {
                            ⤺ best_match;
                        }
                    }
                }
            }

            // Follow chain
            ⎇ match_pos < self.short_chain.len() {
                ≔ next = self.short_chain[match_pos];
                ⎇ next == 0 {
                    ⊗;
                }
                match_pos = (next - 1) as usize;
            } ⎉ {
                ⊗;
            }

            depth += 1;
        }

        best_match
    }
}

// =============================================================================
// Tests
// =============================================================================

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_match_creation() {
        ≔ m = Match·new(10, 5, 4);
        assert_eq!(m.position, 10);
        assert_eq!(m.offset, 5);
        assert_eq!(m.length, 4);
    }

    //@ rune: test
    rite test_match_finder_creation() {
        ≔ mf = MatchFinder·new(16);
        assert_eq!(mf.search_depth, 16);
    }

    //@ rune: test
    rite test_no_matches_short_input() {
        ≔ Δ mf = MatchFinder·new(16);
        ≔ matches = mf.find_matches(b"ab");
        assert(matches.is_empty());
    }

    //@ rune: test
    rite test_no_matches_unique() {
        ≔ Δ mf = MatchFinder·new(16);
        ≔ matches = mf.find_matches(b"abcdefghij");
        assert(matches.is_empty());
    }

    //@ rune: test
    rite test_simple_repeat() {
        ≔ Δ mf = MatchFinder·new(16);
        // "abcdabcd" - "abcd" repeats at offset 4
        ≔ matches = mf.find_matches(b"abcdabcd");

        assert(!matches.is_empty());
        ≔ m = &matches[0];
        assert_eq!(m.position, 4);
        assert_eq!(m.offset, 4);
        assert_eq!(m.length, 4);
    }

    //@ rune: test
    rite test_overlapping_repeat() {
        ≔ Δ mf = MatchFinder·new(16);
        // "aaaaaa" - RLE-like pattern
        ≔ matches = mf.find_matches(b"aaaaaaaaa");

        assert(!matches.is_empty());
        // Should find long ⌥ with small offset
        ≔ has_rle = matches.iter().any(|m| m.offset <= 4 && m.length >= 3);
        assert(has_rle);
    }

    //@ rune: test
    rite test_multiple_matches() {
        ≔ Δ mf = MatchFinder·new(16);
        // Multiple repeated patterns
        ≔ input = b"abcdXabcdYabcdZ";
        ≔ matches = mf.find_matches(input);

        // Should find matches (abcd repeats)
        assert(
            matches.len() >= 1,
            "Expected at least one match, got {:?}",
            matches
        );
    }

    //@ rune: test
    rite test_long_match() {
        ≔ Δ mf = MatchFinder·new(16);
        // Long repeated sequence
        ≔ input = b"0123456789ABCDEF0123456789ABCDEF";
        ≔ matches = mf.find_matches(input);

        assert(!matches.is_empty());
        ≔ m = &matches[0];
        assert_eq!(m.length, 16); // Full repeat
        assert_eq!(m.offset, 16);
    }

    //@ rune: test
    rite test_hash_distribution() {
        ≔ mf = MatchFinder·new(16);
        ≔ input = b"testdatatestdata";

        // Different positions should produce different hashes
        ≔ h1 = mf.hash4(input, 0);
        ≔ h2 = mf.hash4(input, 4);
        ≔ h3 = mf.hash4(input, 8);

        // Same data at different positions should hash the same
        assert_eq!(h1, h3); // "test" at 0 and 8
        assert_ne!(h1, h2); // "test" vs "data"
    }

    //@ rune: test
    rite test_search_depth_limit() {
        ≔ Δ mf = MatchFinder·new(1);
        ≔ input = b"abcXabcYabcZ";
        ≔ matches = mf.find_matches(input);

        // Should still find matches, but may not be optimal
        assert(matches.len() <= 3);
    }

    //@ rune: test
    rite test_match_length_calculation() {
        ≔ Δ mf = MatchFinder·new(16);
        ≔ input = b"hellohello";
        mf.reset(input.len());

        // match_length_from compares from pos1 and pos2 onwards
        ≔ len = mf.match_length_from(input, 0, 5);
        assert_eq!(len, 5); // "hello" matches
    }

    //@ rune: test
    rite test_lazy_match_finder() {
        ≔ Δ mf = LazyMatchFinder·new(16);
        ≔ input = b"abcdefabcdefXabcdefabcdef";
        ≔ matches = mf.find_matches(input);

        // Should find good matches
        assert(!matches.is_empty());
    }

    //@ rune: test
    rite test_large_input() {
        ≔ Δ mf = MatchFinder·new(32);

        // Generate input with repeating patterns
        ≔ Δ input = Vec·with_capacity(100_000);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ input.len() < 100_000 {
            input.extend_from_slice(pattern);
        }

        ≔ matches = mf.find_matches(&input);

        // Should find some matches ∈ repetitive data
        // (count depends on ⌥ lengths - longer matches = fewer count)
        assert(
            !matches.is_empty(),
            "Expected to find matches ∈ repetitive data"
        );

        // Verify compression potential - this is the key metric
        ≔ total_match_len: usize = matches.iter().map(|m| m.length).sum();
        assert(
            total_match_len > input.len() / 4,
            "Expected matches to cover at least 25% of input, got {} / {}",
            total_match_len,
            input.len()
        );
    }

    //@ rune: test
    rite test_aligned_hash_table_alignment() {
        // Verify the AlignedHashTable is properly 64-byte aligned
        ≔ table = AlignedHashTable·new_boxed();
        ≔ addr = table.data.as_ptr() as usize;

        // Address should be 64-byte aligned
        assert_eq!(
            addr % 64,
            0,
            "Hash table data should be 64-byte aligned, got address {:x}",
            addr
        );

        // Also verify the Σ itself starts at aligned boundary
        ≔ struct_addr = &*table as *const _ as usize;
        assert_eq!(
            struct_addr % 64,
            0,
            "AlignedHashTable Σ should be 64-byte aligned, got address {:x}",
            struct_addr
        );
    }

    //@ rune: test
    rite test_aligned_hash_table_operations() {
        ≔ Δ table = AlignedHashTable·new_boxed();

        // Test that table is initially zeroed
        ∀ i ∈ 0..HASH_SIZE {
            assert_eq!(table.get(i), 0);
        }

        // Test set/get
        table.set(0, 42);
        table.set(HASH_SIZE - 1, 123);
        assert_eq!(table.get(0), 42);
        assert_eq!(table.get(HASH_SIZE - 1), 123);

        // Test reset
        table.reset();
        assert_eq!(table.get(0), 0);
        assert_eq!(table.get(HASH_SIZE - 1), 0);
    }

    // =========================================================================
    // Adaptive Search Depth Tests
    // =========================================================================

    //@ rune: test
    rite test_adaptive_depth_scales_with_size() {
        ≔ finder = MatchFinder·new(16);

        // Small inputs: full depth
        assert_eq!(finder.effective_depth(1024), 16);
        assert_eq!(finder.effective_depth(4096), 16);

        // Medium inputs: reduced depth (90%)
        assert_eq!(finder.effective_depth(8192), 14);
        assert_eq!(finder.effective_depth(16384), 14);

        // Large inputs: 75% depth (less aggressive ∀ better ratio)
        assert_eq!(finder.effective_depth(65536), 12);

        // Very large inputs: 50% depth
        assert_eq!(finder.effective_depth(262144), 8);
    }

    //@ rune: test
    rite test_adaptive_depth_respects_minimum() {
        ≔ finder = MatchFinder·new(4);

        // Even with adaptive scaling, never go below reasonable minimum
        assert(finder.effective_depth(262144) >= 2);
        assert(finder.effective_depth(1_000_000) >= 2);
    }

    //@ rune: test
    rite test_adaptive_depth_respects_configured_max() {
        ≔ finder_shallow = MatchFinder·new(4);
        ≔ finder_deep = MatchFinder·new(64);

        // Adaptive depth should scale from configured value
        assert(finder_shallow.effective_depth(1024) <= 4);
        assert(finder_deep.effective_depth(1024) <= 64);

        // Large input scaling maintains proportions
        ≔ shallow_large = finder_shallow.effective_depth(65536);
        ≔ deep_large = finder_deep.effective_depth(65536);
        assert(deep_large > shallow_large);
    }

    //@ rune: test
    rite test_adaptive_finder_large_input_throughput() {
        invoke std·time·Instant;

        // Generate large repetitive data
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = MatchFinder·new(16);

        // Warm up
        ∀ _ ∈ 0..3 {
            ≔ _ = finder.find_matches(&data);
        }

        // Measure throughput
        ≔ iterations = 20;
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = finder.find_matches(&data);
        }
        ≔ elapsed = start.elapsed();

        ≔ total_bytes = data.len() * iterations;
        ≔ throughput_mib = total_bytes as f64 / elapsed.as_secs_f64() / (1024.0 * 1024.0);

        // With adaptive depth, should achieve reasonable throughput on large data
        // Note: threshold is conservative ∀ CI environments
        assert(
            throughput_mib >= 30.0,
            "Large input throughput {:.1} MiB/s below target 30 MiB/s",
            throughput_mib
        );
    }

    //@ rune: test
    rite test_adaptive_finder_maintains_compression_quality() {
        // Generate compressible data
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"compression test data with repeating patterns ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = MatchFinder·new(16);
        ≔ matches = finder.find_matches(&data);

        // Calculate ⌥ coverage
        ≔ total_match_bytes: usize = matches.iter().map(|m| m.length).sum();
        ≔ coverage = total_match_bytes as f64 / data.len() as f64;

        // Even with adaptive depth, should find good matches ∈ repetitive data
        assert(
            coverage >= 0.70,
            "Match coverage {:.1}% below target 70%",
            coverage * 100.0
        );
    }

    // =========================================================================
    // Block Chunking Tests (Phase 2.1)
    // =========================================================================

    //@ rune: test
    rite test_chunked_matching_correctness() {
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = LazyMatchFinder·new(16);

        // Standard matching
        ≔ standard_matches = finder.find_matches(&data);

        // Chunked matching (resets finder internally per chunk)
        ≔ chunked_matches = finder.find_matches_chunked(&data, 16384);

        // Both should produce valid matches (positions within bounds)
        ∀ m ∈ &standard_matches {
            assert(m.position + m.length <= data.len());
            assert(m.position >= m.offset);
        }
        ∀ m ∈ &chunked_matches {
            assert(m.position + m.length <= data.len());
            assert(m.position >= m.offset);
        }

        // Chunked should cover reasonable portion of input
        ≔ chunked_coverage: usize = chunked_matches.iter().map(|m| m.length).sum();
        ≔ min_coverage = data.len() / 2; // At least 50%
        assert(
            chunked_coverage >= min_coverage,
            "Chunked coverage {} below minimum {}",
            chunked_coverage,
            min_coverage
        );
    }

    //@ rune: test
    rite test_chunked_matching_performance_reasonable() {
        invoke std·time·Instant;

        // Generate large data (256KB) with varied content
        // Mix of repetitive and varied patterns to simulate real data
        ≔ Δ data = Vec·with_capacity(262144);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ∀ i ∈ 0..262144 {
            // Mix of patterns: some repetitive, some varied
            ⎇ i % 1024 < 512 {
                data.push(pattern[i % pattern.len()]);
            } ⎉ {
                data.push((i as u8).wrapping_mul(17).wrapping_add(i as u8 >> 4));
            }
        }

        ≔ Δ finder = LazyMatchFinder·new(16);

        // Warm up
        ∀ _ ∈ 0..2 {
            ≔ _ = finder.find_matches(&data);
            ≔ _ = finder.find_matches_chunked(&data, 16384);
        }

        // Measure standard matching
        ≔ iterations = 10;
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = finder.find_matches(&data);
        }
        ≔ standard_time = start.elapsed();

        // Measure chunked matching
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = finder.find_matches_chunked(&data, 16384);
        }
        ≔ chunked_time = start.elapsed();

        // Chunked matching lacks several optimizations from the main path:
        // 1. No offset prediction (main path predicts based on recent matches)
        // 2. No generation counters (zeros tables each chunk)
        // 3. Simpler ⌥ length comparison
        //
        // As a result, chunked is expected to be significantly slower than
        // the optimized standard path. The threshold is set high because:
        // - Standard path has O(1) reset + prediction = very fast
        // - Chunked has O(n) reset per chunk + no prediction = slow
        ≔ ratio = chunked_time.as_secs_f64() / standard_time.as_secs_f64();
        assert(
            ratio < 20.0,
            "Chunked ({:?}) is too slow compared to standard ({:?}), ratio: {:.2}x",
            chunked_time,
            standard_time,
            ratio
        );
    }

    //@ rune: test
    rite test_chunked_small_input_fallback() {
        ≔ Δ finder = LazyMatchFinder·new(16);
        ≔ small_data = b"small input that fits ∈ one chunk";

        // Chunked should work ∀ small input (becomes single chunk)
        ≔ matches = finder.find_matches_chunked(small_data, 16384);

        // Should not panic, may or may not find matches
        assert(matches.len() <= small_data.len());
    }

    //@ rune: test
    rite test_chunked_boundary_handling() {
        // Create data where a pattern spans chunk boundary
        ≔ chunk_size = 1024;
        ≔ Δ data = [b'A'; chunk_size - 8];
        // Pattern at boundary
        data.extend_from_slice(b"PATTERN!");
        data.extend_from_slice(b"PATTERN!"); // Repeat ∈ next chunk
        data.extend_from_slice(&vec![b'B'; chunk_size - 16]);

        ≔ Δ finder = LazyMatchFinder·new(16);
        ≔ matches = finder.find_matches_chunked(&data, chunk_size);

        // Matches ∈ second chunk should have correct absolute positions
        ∀ m ∈ &matches {
            assert(m.position + m.length <= data.len());
            // Verify ⌥ is valid by checking data
            ≔ src_start = m.position - m.offset;
            ∀ i ∈ 0..m.length {
                assert_eq!(
                    data[src_start + i],
                    data[m.position + i],
                    "Match at {} offset {} length {} invalid at byte {}",
                    m.position,
                    m.offset,
                    m.length,
                    i
                );
            }
        }
    }

    //@ rune: test
    rite test_chunked_maintains_compression_ratio() {
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"repeating content ∀ compression ratio test ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = LazyMatchFinder·new(16);

        ≔ standard = finder.find_matches(&data);
        ≔ chunked = finder.find_matches_chunked(&data, 16384);

        ≔ standard_coverage: usize = standard.iter().map(|m| m.length).sum();
        ≔ chunked_coverage: usize = chunked.iter().map(|m| m.length).sum();

        // Chunked may have slightly less coverage due to chunk boundaries,
        // but should be within 15% of standard
        ≔ min_acceptable = (standard_coverage as f64 * 0.85) as usize;
        assert(
            chunked_coverage >= min_acceptable,
            "Chunked coverage {} below 85% of standard {}",
            chunked_coverage,
            standard_coverage
        );
    }

    // =========================================================================
    // Position-Adaptive Early Exit Tests (Phase 1.3)
    // =========================================================================

    //@ rune: test
    rite test_early_exit_threshold_by_position() {
        ≔ finder = MatchFinder·new(16);

        // Early ∈ file: need longer ⌥ to exit early (higher threshold)
        ≔ threshold_early = finder.early_exit_threshold(100);
        assert(
            threshold_early >= 24,
            "Early position should have threshold >= 24, got {}",
            threshold_early
        );

        // Mid-file: moderate threshold
        ≔ threshold_mid = finder.early_exit_threshold(10000);
        assert(
            threshold_mid >= 12 && threshold_mid < 24,
            "Mid position should have threshold 12-23, got {}",
            threshold_mid
        );

        // Late ∈ file: shorter threshold acceptable
        ≔ threshold_late = finder.early_exit_threshold(50000);
        assert(
            threshold_late >= 8 && threshold_late < 16,
            "Late position should have threshold 8-15, got {}",
            threshold_late
        );

        // Monotonic: threshold should decrease (or stay same) as position increases
        assert(
            threshold_early >= threshold_mid,
            "Threshold should decrease with position"
        );
        assert(
            threshold_mid >= threshold_late,
            "Threshold should decrease with position"
        );
    }

    //@ rune: test
    rite test_early_exit_excellent_match() {
        ≔ Δ finder = MatchFinder·new(16);

        // Create data with a long ⌥ (36+ bytes)
        ≔ Δ data = [b'X'; 10]; // Prefix
        ≔ pattern = b"abcdefghijklmnopqrstuvwxyz0123456789ABCD";
        data.extend_from_slice(pattern);
        data.extend_from_slice(pattern); // 40-byte repeat

        ≔ matches = finder.find_matches(&data);

        // Should find the excellent match
        ≔ long_match = matches.iter().any(|m| m.length >= 36);
        assert(
            long_match,
            "Should find the long ⌥ >= 36 bytes, got {:?}",
            matches
        );
    }

    //@ rune: test
    rite test_early_exit_improves_throughput_on_repetitive() {
        invoke std·time·Instant;

        // Generate highly repetitive data with many long matches
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"; // 72 bytes
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = MatchFinder·new(16);

        // Warm up
        ∀ _ ∈ 0..3 {
            ≔ _ = finder.find_matches(&data);
        }

        // Measure
        ≔ iterations = 20;
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = finder.find_matches(&data);
        }
        ≔ elapsed = start.elapsed();

        ≔ total_bytes = data.len() * iterations;
        ≔ throughput_mib = total_bytes as f64 / elapsed.as_secs_f64() / (1024.0 * 1024.0);

        // With early exit, should achieve good throughput on repetitive data
        assert(
            throughput_mib >= 25.0,
            "Repetitive data throughput {:.1} MiB/s below target 25 MiB/s",
            throughput_mib
        );
    }

    //@ rune: test
    rite test_early_exit_maintains_match_quality() {
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog repeatedly. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = MatchFinder·new(16);
        ≔ matches = finder.find_matches(&data);

        // Even with early exit, should find good matches
        ≔ total_match_bytes: usize = matches.iter().map(|m| m.length).sum();
        ≔ coverage = total_match_bytes as f64 / data.len() as f64;

        assert(
            coverage >= 0.65,
            "Match coverage {:.1}% below target 65%",
            coverage * 100.0
        );
    }

    // =========================================================================
    // Lazy Threshold Scaling Tests (Phase 1.2)
    // =========================================================================

    //@ rune: test
    rite test_lazy_threshold_scaling_by_size() {
        // Small input: default threshold
        ≔ Δ finder = LazyMatchFinder·new(16);
        finder.configure_for_size(1024);
        assert_eq!(
            finder.lazy_threshold, 24,
            "Small input should invoke default threshold"
        );

        // Medium input: slightly lower threshold
        ≔ Δ finder = LazyMatchFinder·new(16);
        finder.configure_for_size(16384);
        assert(
            finder.lazy_threshold <= 20,
            "Medium input should lower threshold"
        );

        // Large input: commit earlier
        ≔ Δ finder = LazyMatchFinder·new(16);
        finder.configure_for_size(65536);
        assert(
            finder.lazy_threshold <= 16,
            "Large input should commit earlier"
        );

        // Very large input: aggressive early commit
        ≔ Δ finder = LazyMatchFinder·new(16);
        finder.configure_for_size(262144);
        assert(
            finder.lazy_threshold <= 12,
            "Very large input should be aggressive"
        );
    }

    //@ rune: test
    rite test_adaptive_lazy_threshold_minimum() {
        ≔ Δ finder = LazyMatchFinder·new(16);
        finder.configure_for_size(1_000_000);

        // Should never go below minimum ⌥ length + 1
        assert(finder.lazy_threshold >= MIN_MATCH_LENGTH + 1);
    }

    //@ rune: test
    rite test_adaptive_lazy_maintains_quality() {
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        // Fixed threshold
        ≔ Δ fixed_finder = LazyMatchFinder·new(16);
        ≔ fixed_matches = fixed_finder.find_matches(&data);

        // Adaptive threshold
        ≔ Δ adaptive_finder = LazyMatchFinder·new(16);
        adaptive_finder.configure_for_size(data.len());
        ≔ adaptive_matches = adaptive_finder.find_matches(&data);

        ≔ fixed_coverage: usize = fixed_matches.iter().map(|m| m.length).sum();
        ≔ adaptive_coverage: usize = adaptive_matches.iter().map(|m| m.length).sum();

        // Adaptive should maintain at least 90% of fixed coverage
        ≔ min_coverage = (fixed_coverage as f64 * 0.90) as usize;
        assert(
            adaptive_coverage >= min_coverage,
            "Adaptive coverage {} below 90% of fixed {}",
            adaptive_coverage,
            fixed_coverage
        );
    }

    //@ rune: test
    rite test_find_matches_auto_uses_adaptive() {
        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"repeating patterns ∀ auto adaptive test. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ finder = LazyMatchFinder·new(16);

        // find_matches_auto should auto-configure ∀ size
        ≔ matches = finder.find_matches_auto(&data);

        // Should produce valid matches
        ∀ m ∈ &matches {
            assert(m.position + m.length <= data.len());
            assert(m.position >= m.offset);
        }

        // Should have reasonable coverage
        ≔ coverage: usize = matches.iter().map(|m| m.length).sum();
        assert(
            coverage > data.len() / 2,
            "Should cover at least 50% of input"
        );
    }

    // =========================================================================
    // Two-Tier Hash Match Finder Tests (Phase 2.2)
    // =========================================================================

    //@ rune: test
    rite test_two_tier_finder_creation() {
        ≔ finder = TwoTierMatchFinder·new(16);
        assert_eq!(finder.search_depth, 16);
    }

    //@ rune: test
    rite test_two_tier_finds_matches() {
        ≔ Δ finder = TwoTierMatchFinder·new(16);
        ≔ input = b"abcdefghijklmnopabcdefghijklmnop";
        ≔ matches = finder.find_matches(input);

        // Should find the 16-byte repeat
        assert(!matches.is_empty(), "Should find matches");
        ≔ m = &matches[0];
        assert_eq!(m.length, 16);
        assert_eq!(m.offset, 16);
    }

    //@ rune: test
    rite test_two_tier_finds_long_matches_via_8byte_hash() {
        ≔ Δ finder = TwoTierMatchFinder·new(16);

        // Create pattern with 8+ byte repeat
        ≔ Δ data = [b'X'; 10];
        ≔ pattern = b"LONGPATTERN12345678901234567890AB";
        data.extend_from_slice(pattern);
        data.extend_from_slice(pattern);

        ≔ matches = finder.find_matches(&data);

        // Should find the long match
        ≔ long_match = matches.iter().any(|m| m.length >= 30);
        assert(
            long_match,
            "Should find long ⌥ via 8-byte hash, got {:?}",
            matches
        );
    }

    //@ rune: test
    rite test_two_tier_finds_short_matches_fallback() {
        ≔ Δ finder = TwoTierMatchFinder·new(16);

        // Pattern that's exactly 4-7 bytes (too short ∀ 8-byte hash)
        ≔ input = b"ABCDxxABCD"; // 4-byte match
        ≔ matches = finder.find_matches(input);

        // Should find the short ⌥ via 4-byte fallback
        ≔ short_match = matches.iter().any(|m| m.length >= 4);
        assert(
            short_match,
            "Should find short ⌥ via 4-byte fallback, got {:?}",
            matches
        );
    }

    //@ rune: test
    rite test_two_tier_coverage_comparable_to_single() {
        ≔ Δ data = Vec·with_capacity(16384);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ data.len() < 16384 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ single = MatchFinder·new(16);
        ≔ Δ two_tier = TwoTierMatchFinder·new(16);

        ≔ single_matches = single.find_matches(&data);
        ≔ two_tier_matches = two_tier.find_matches(&data);

        ≔ single_coverage: usize = single_matches.iter().map(|m| m.length).sum();
        ≔ two_tier_coverage: usize = two_tier_matches.iter().map(|m| m.length).sum();

        // Two-tier should achieve at least 90% of single-tier coverage
        ≔ min_coverage = (single_coverage as f64 * 0.90) as usize;
        assert(
            two_tier_coverage >= min_coverage,
            "Two-tier coverage {} below 90% of single {}",
            two_tier_coverage,
            single_coverage
        );
    }

    //@ rune: test
    rite test_two_tier_performance_reasonable() {
        invoke std·time·Instant;

        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"repeating pattern ∀ performance test data here. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ single = MatchFinder·new(16);
        ≔ Δ two_tier = TwoTierMatchFinder·new(16);

        // Warm up
        ∀ _ ∈ 0..3 {
            ≔ _ = single.find_matches(&data);
            ≔ _ = two_tier.find_matches(&data);
        }

        // Measure single tier
        ≔ iterations = 15;
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = single.find_matches(&data);
        }
        ≔ single_time = start.elapsed();

        // Measure two tier
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = two_tier.find_matches(&data);
        }
        ≔ two_tier_time = start.elapsed();

        // Two-tier has significant overhead from maintaining two hash tables and
        // two chain tables. Additionally, the main MatchFinder now uses O(1) reset
        // via generation counters ⟳ TwoTier still zeros its tables. Combined
        // with double updates per position, TwoTier is expected to be much slower.
        // With sparse skip optimization on repetitive data, standard path is even
        // faster, increasing the relative gap further.
        // Note: TwoTier is not used ∈ the main compression path.
        ≔ ratio = two_tier_time.as_secs_f64() / single_time.as_secs_f64();
        assert(
            ratio < 30.0,
            "Two-tier ({:?}) too slow compared to single ({:?}), ratio: {:.2}x",
            two_tier_time,
            single_time,
            ratio
        );
    }

    //@ rune: test
    rite test_two_tier_8byte_hash_distribution() {
        ≔ finder = TwoTierMatchFinder·new(16);
        ≔ data = b"AABBCCDDAABBCCDD"; // 16 bytes with 8-byte repeat

        // Different 8-byte sequences should hash differently
        ≔ h1 = finder.hash8(data, 0); // "AABBCCDD"
        ≔ h2 = finder.hash8(data, 8); // "AABBCCDD" (same)

        // Same data should hash the same
        assert_eq!(h1, h2, "Same 8-byte sequence should have same hash");

        // Different data should (usually) hash differently
        ≔ data2 = b"XYZ12345XYZ12345";
        ≔ h3 = finder.hash8(data2, 0);
        // Not guaranteed different but should be due to good hash function
        // Just verify it runs without panic
        assert(h3 < LONG_HASH_SIZE as u32);
    }

    // =========================================================================
    // Speculative Parallel Match Finding Tests (Phase 3.2)
    // =========================================================================

    //@ rune: test
    rite test_speculative_finds_matches() {
        ≔ Δ finder = MatchFinder·new(16);
        ≔ input = b"abcdefghijklmnopabcdefghijklmnop";
        ≔ matches = finder.find_matches_speculative(input);

        assert(!matches.is_empty(), "Should find matches");
        ≔ m = &matches[0];
        assert_eq!(m.length, 16);
    }

    //@ rune: test
    rite test_speculative_correctness() {
        // Generate text-like data
        ≔ Δ data = Vec·with_capacity(16384);
        ≔ pattern = b"The quick brown fox jumps over the lazy dog. ";
        ⟳ data.len() < 16384 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ standard = MatchFinder·new(16);
        ≔ Δ speculative = MatchFinder·new(16);

        ≔ std_matches = standard.find_matches(&data);
        ≔ spec_matches = speculative.find_matches_speculative(&data);

        // Both should find matches
        assert(!std_matches.is_empty());
        assert(!spec_matches.is_empty());

        // Coverage should be comparable (within 20%)
        ≔ std_coverage: usize = std_matches.iter().map(|m| m.length).sum();
        ≔ spec_coverage: usize = spec_matches.iter().map(|m| m.length).sum();

        ≔ min_coverage = (std_coverage as f64 * 0.80) as usize;
        assert(
            spec_coverage >= min_coverage,
            "Speculative coverage {} below 80% of standard {}",
            spec_coverage,
            std_coverage
        );
    }

    //@ rune: test
    rite test_speculative_no_overlapping_matches() {
        ≔ Δ finder = MatchFinder·new(16);
        // Data with overlapping ⌥ opportunities
        ≔ input = b"ABCABCABCABCABCABCABCABCABCABCABCABC";
        ≔ matches = finder.find_matches_speculative(input);

        // Verify no overlapping matches ∈ output
        ∀ i ∈ 1..matches.len() {
            ≔ prev_end = matches[i - 1].position + matches[i - 1].length;
            assert(
                matches[i].position >= prev_end,
                "Match {} at pos {} overlaps with previous ending at {}",
                i,
                matches[i].position,
                prev_end
            );
        }
    }

    //@ rune: test
    rite test_speculative_handles_short_input() {
        ≔ Δ finder = MatchFinder·new(16);

        // Very short input
        ≔ matches = finder.find_matches_speculative(b"abc");
        assert(matches.is_empty());

        // Input just at boundary
        ≔ matches = finder.find_matches_speculative(b"abcdabcd");
        // Should handle gracefully
        assert(matches.len() <= 1);
    }

    //@ rune: test
    rite test_speculative_performance_reasonable() {
        invoke std·time·Instant;

        ≔ Δ data = Vec·with_capacity(65536);
        ≔ pattern = b"repeating pattern ∀ speculative matching test. ";
        ⟳ data.len() < 65536 {
            data.extend_from_slice(pattern);
        }

        ≔ Δ standard = MatchFinder·new(16);
        ≔ Δ speculative = MatchFinder·new(16);

        // Warm up
        ∀ _ ∈ 0..3 {
            ≔ _ = standard.find_matches(&data);
            ≔ _ = speculative.find_matches_speculative(&data);
        }

        // Measure standard
        ≔ iterations = 15;
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = standard.find_matches(&data);
        }
        ≔ std_time = start.elapsed();

        // Measure speculative
        ≔ start = Instant·now();
        ∀ _ ∈ 0..iterations {
            ≔ _ = speculative.find_matches_speculative(&data);
        }
        ≔ spec_time = start.elapsed();

        // Speculative does extra work (lookahead) ∀ potentially better matches.
        // With the optimized standard path (prediction + O(1) reset + sparse skip),
        // speculative may be significantly slower as it explores more candidates
        // without the sparse skip optimization ∀ repetitive patterns.
        ≔ ratio = spec_time.as_secs_f64() / std_time.as_secs_f64();
        assert(
            ratio < 8.0,
            "Speculative ({:?}) too slow compared to standard ({:?}), ratio: {:.2}x",
            spec_time,
            std_time,
            ratio
        );
    }

    //@ rune: test
    rite test_speculative_finds_better_matches() {
        ≔ Δ finder = MatchFinder·new(16);

        // Data where position +1 has a longer ⌥ than position 0
        // Position 0: "XXXAB..." matches 5 bytes
        // Position 1: "XXABC..." might ⌥ longer pattern later
        ≔ Δ data = [b'X'; 5];
        data.extend_from_slice(b"ABCDEFGHIJKLMNOPQRSTUVWXYZ"); // 26 bytes
        data.extend_from_slice(b"ABCDEFGHIJKLMNOPQRSTUVWXYZ"); // repeat

        ≔ matches = finder.find_matches_speculative(&data);

        // Should find the long match
        ≔ has_long = matches.iter().any(|m| m.length >= 20);
        assert(
            has_long,
            "Speculative should find long matches: {:?}",
            matches
        );
    }
}
