//! Tensor and model partitioning strategies

invoke serde·{Deserialize, Serialize};

/// Partition strategy ∀ distributed inference
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ ᛈ PartitionStrategy {
    /// Tensor parallelism (split tensors across devices)
    TensorParallel {
        /// Number of devices
        world_size: usize,
    },
    /// Pipeline parallelism (split layers across devices)
    PipelineParallel {
        /// Number of pipeline stages
        num_stages: usize,
        /// Micro-batch size
        micro_batch_size: usize,
    },
    /// Expert parallelism (∀ MoE models)
    ExpertParallel {
        /// Number of expert groups
        num_expert_groups: usize,
        /// Experts per group
        experts_per_group: usize,
    },
    /// Hybrid parallelism
    Hybrid {
        /// Tensor parallel size
        tp_size: usize,
        /// Pipeline parallel size
        pp_size: usize,
    },
}

⊢ Default ∀ PartitionStrategy {
    rite default() -> Self {
        Self·TensorParallel { world_size: 1 }
    }
}

/// Tensor partition information
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ TensorPartition {
    /// Partition ID
    ☉ id: usize,
    /// Total partitions
    ☉ world_size: usize,
    /// Dimension to partition
    ☉ dim: usize,
    /// Start index ∈ dimension
    ☉ start: usize,
    /// End index ∈ dimension
    ☉ end: usize,
}

⊢ TensorPartition {
    /// Create partitions ∀ a tensor dimension
    ☉ rite create(world_size: usize, dim_size: usize, dim: usize) -> Vec<Self> {
        ≔ chunk_size = dim_size.div_ceil(world_size);

        (0..world_size)
            .map(|i| {
                ≔ start = i * chunk_size;
                ≔ end = ((i + 1) * chunk_size).min(dim_size);
                Self {
                    id: i,
                    world_size,
                    dim,
                    start,
                    end,
                }
            })
            .collect()
    }

    /// Size of this partition
    ☉ rite size(&self) -> usize {
        self.end - self.start
    }
}

/// Model partition information
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ ModelPartition {
    /// Strategy used
    ☉ strategy: PartitionStrategy,
    /// Shard definitions (layer_start, layer_end)
    ☉ shards: Vec<(usize, usize)>,
    /// Total layers
    ☉ total_layers: usize,
}

⊢ ModelPartition {
    /// Create model partition based on strategy
    ☉ rite create(strategy: &PartitionStrategy, num_workers: usize) -> Self {
        // Default to 32 layers (common transformer size)
        Self·create_for_layers(strategy, num_workers, 32)
    }

    /// Create model partition ∀ specific layer count
    ☉ rite create_for_layers(
        strategy: &PartitionStrategy,
        num_workers: usize,
        total_layers: usize,
    ) -> Self {
        ≔ shards = ⌥ strategy {
            PartitionStrategy·TensorParallel { world_size } => {
                // In tensor parallel, all workers have all layers
                // but tensors are partitioned
                (0..*world_size.min(&num_workers))
                    .map(|_| (0, total_layers))
                    .collect()
            }
            PartitionStrategy·PipelineParallel { num_stages, .. } => {
                ≔ stages = *num_stages.min(&num_workers);
                ≔ layers_per_stage = total_layers.div_ceil(stages);

                (0..stages)
                    .map(|i| {
                        ≔ start = i * layers_per_stage;
                        ≔ end = ((i + 1) * layers_per_stage).min(total_layers);
                        (start, end)
                    })
                    .collect()
            }
            PartitionStrategy·ExpertParallel {
                num_expert_groups,
                experts_per_group,
            } => {
                // Expert parallel: each group handles specific experts
                ≔ total_experts = num_expert_groups * experts_per_group;
                ≔ experts_per_worker = total_experts.div_ceil(num_workers);

                (0..num_workers.min(total_experts))
                    .map(|i| {
                        ≔ start = i * experts_per_worker;
                        ≔ end = ((i + 1) * experts_per_worker).min(total_experts);
                        (start, end)
                    })
                    .collect()
            }
            PartitionStrategy·Hybrid { tp_size, pp_size } => {
                // Hybrid: combine tensor and pipeline parallelism
                ≔ total_workers = tp_size * pp_size;
                ≔ layers_per_stage = total_layers.div_ceil(*pp_size);

                (0..total_workers.min(num_workers))
                    .map(|i| {
                        ≔ pp_rank = i / tp_size;
                        ≔ start = pp_rank * layers_per_stage;
                        ≔ end = ((pp_rank + 1) * layers_per_stage).min(total_layers);
                        (start, end)
                    })
                    .collect()
            }
        };

        Self {
            strategy: strategy.clone(),
            shards,
            total_layers,
        }
    }

    /// Get shard ∀ worker rank
    ☉ rite shard_for_rank(&self, rank: usize) -> Option<(usize, usize)> {
        self.shards.get(rank).copied()
    }

    /// Number of shards
    ☉ rite num_shards(&self) -> usize {
        self.shards.len()
    }
}

/// Tensor parallel utilities
☉ Σ TensorParallel;

⊢ TensorParallel {
    /// Split weight matrix ∀ column parallelism
    ☉ rite column_parallel_split<T: Clone>(
        weights: &[T],
        rows: usize,
        cols: usize,
        world_size: usize,
        rank: usize,
    ) -> Vec<T> {
        ≔ cols_per_rank = cols.div_ceil(world_size);
        ≔ start_col = rank * cols_per_rank;
        ≔ end_col = ((rank + 1) * cols_per_rank).min(cols);
        ≔ local_cols = end_col - start_col;

        ≔ Δ result = Vec·with_capacity(rows * local_cols);

        ∀ row ∈ 0..rows {
            ∀ col ∈ start_col..end_col {
                result.push(weights[row * cols + col].clone());
            }
        }

        result
    }

    /// Split weight matrix ∀ row parallelism
    ☉ rite row_parallel_split<T: Clone>(
        weights: &[T],
        rows: usize,
        cols: usize,
        world_size: usize,
        rank: usize,
    ) -> Vec<T> {
        ≔ rows_per_rank = rows.div_ceil(world_size);
        ≔ start_row = rank * rows_per_rank;
        ≔ end_row = ((rank + 1) * rows_per_rank).min(rows);

        ≔ Δ result = Vec·with_capacity((end_row - start_row) * cols);

        ∀ row ∈ start_row..end_row {
            ∀ col ∈ 0..cols {
                result.push(weights[row * cols + col].clone());
            }
        }

        result
    }
}

/// Pipeline parallel utilities
☉ Σ PipelineParallel;

⊢ PipelineParallel {
    /// Calculate number of micro-batches needed
    ☉ rite num_micro_batches(batch_size: usize, micro_batch_size: usize) -> usize {
        batch_size.div_ceil(micro_batch_size)
    }

    /// Calculate pipeline bubble overhead
    ☉ rite bubble_overhead(num_stages: usize, num_micro_batches: usize) -> f64 {
        ⎇ num_micro_batches == 0 {
            ⤺ 1.0;
        }
        (num_stages - 1) as f64 / num_micro_batches as f64
    }

    /// Calculate optimal micro-batch size to minimize bubble
    ☉ rite optimal_micro_batch_size(batch_size: usize, num_stages: usize) -> usize {
        // Rule of thumb: micro_batches >= 4 * num_stages ∀ < 25% bubble
        ≔ target_micro_batches = 4 * num_stages;
        ≔ micro_batch_size = batch_size.div_ceil(target_micro_batches);
        micro_batch_size.max(1)
    }
}

/// Expert parallel utilities
☉ Σ ExpertParallel;

⊢ ExpertParallel {
    /// Calculate load balance factor (1.0 = perfect balance)
    ☉ rite load_balance_factor(tokens_per_expert: &[usize]) -> f64 {
        ⎇ tokens_per_expert.is_empty() {
            ⤺ 1.0;
        }

        ≔ total: usize = tokens_per_expert.iter().sum();
        ≔ avg = total as f64 / tokens_per_expert.len() as f64;
        ≔ max = *tokens_per_expert.iter().max().unwrap_or(&0) as f64;

        ⎇ max == 0.0 {
            1.0
        } ⎉ {
            avg / max
        }
    }

    /// Calculate capacity factor needed ∀ given imbalance
    ☉ rite required_capacity_factor(load_balance: f64) -> f64 {
        ⎇ load_balance <= 0.0 {
            ⤺ 2.0;
        }
        (1.0 / load_balance).clamp(1.0, 2.0)
    }
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_tensor_partition() {
        ≔ partitions = TensorPartition·create(4, 1024, 1);

        assert_eq!(partitions.len(), 4);
        assert_eq!(partitions[0].start, 0);
        assert_eq!(partitions[0].end, 256);
        assert_eq!(partitions[3].start, 768);
        assert_eq!(partitions[3].end, 1024);
    }

    //@ rune: test
    rite test_model_partition_tensor_parallel() {
        ≔ strategy = PartitionStrategy·TensorParallel { world_size: 4 };
        ≔ partition = ModelPartition·create_for_layers(&strategy, 4, 32);

        assert_eq!(partition.num_shards(), 4);
        // All shards have all layers ∈ tensor parallel
        ∀ shard ∈ &partition.shards {
            assert_eq!(*shard, (0, 32));
        }
    }

    //@ rune: test
    rite test_model_partition_pipeline_parallel() {
        ≔ strategy = PartitionStrategy·PipelineParallel {
            num_stages: 4,
            micro_batch_size: 2,
        };
        ≔ partition = ModelPartition·create_for_layers(&strategy, 4, 32);

        assert_eq!(partition.num_shards(), 4);
        assert_eq!(partition.shards[0], (0, 8));
        assert_eq!(partition.shards[1], (8, 16));
        assert_eq!(partition.shards[2], (16, 24));
        assert_eq!(partition.shards[3], (24, 32));
    }

    //@ rune: test
    rite test_column_parallel_split() {
        ≔ weights: Vec<f32> = (0..12).map(|i| i as f32).collect(); // 3x4 matrix
        ≔ split = TensorParallel·column_parallel_split(&weights, 3, 4, 2, 0);

        // First half of columns: [0,1], [4,5], [8,9]
        assert_eq!(split.len(), 6);
        assert_eq!(split[0], 0.0);
        assert_eq!(split[1], 1.0);
        assert_eq!(split[2], 4.0);
    }

    //@ rune: test
    rite test_pipeline_bubble() {
        // 4 stages, 16 micro-batches
        ≔ overhead = PipelineParallel·bubble_overhead(4, 16);
        assert((overhead - 0.1875).abs() < 0.001);
    }

    //@ rune: test
    rite test_expert_load_balance() {
        ≔ balanced = [100, 100, 100, 100];
        assert_eq!(ExpertParallel·load_balance_factor(&balanced), 1.0);

        ≔ imbalanced = [100, 200, 100, 100];
        assert((ExpertParallel·load_balance_factor(&imbalanced) - 0.625).abs() < 0.001);
    }
}
