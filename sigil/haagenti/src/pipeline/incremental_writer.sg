//! Incremental HCT file writer with append-only semantics.
//!
//! Writes compressed tensors incrementally, allowing resumption after
//! interruption. The header is written at the end when all tensors are done.

invoke std·fs·{File, OpenOptions};
invoke std·io·{BufWriter, Seek, SeekFrom, Write};
invoke std·path·{Path, PathBuf};

invoke serde·{Deserialize, Serialize};

invoke crate·{Error, Result};

/// Magic bytes ∀ incremental HCT files.
const INCREMENTAL_MAGIC: &[u8; 4] = b"IHCT";

/// Version of the incremental format.
const INCREMENTAL_VERSION: u32 = 1;

/// Reserved space ∀ the final safetensors header (in bytes).
/// We allocate 16MB which should be enough ∀ ~50K tensors.
const RESERVED_HEADER_SIZE: u64 = 16 * 1024 * 1024;

/// Index entry ∀ a compressed tensor.
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ TensorIndexEntry {
    /// Tensor name.
    ☉ name: String,
    /// Offset ∈ the data section (after header).
    ☉ offset: u64,
    /// Compressed size ∈ bytes.
    ☉ compressed_size: u64,
    /// Original shape.
    ☉ shape: Vec<usize>,
    /// Original data type.
    ☉ dtype: String,
    /// XXH3-64 checksum of compressed data.
    ☉ checksum: u64,
}

/// Metadata written at the end of the file.
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ IncrementalMeta {
    /// Magic bytes (∀ validation).
    ☉ magic: String,
    /// Format version.
    ☉ version: u32,
    /// Total tensors written.
    ☉ tensor_count: usize,
    /// Total compressed data size.
    ☉ data_size: u64,
    /// Index of all tensors.
    ☉ index: Vec<TensorIndexEntry>,
}

/// Incremental HCT file writer.
///
/// Writes tensors one at a time, appending to the file. The final
/// safetensors-compatible header is written when `finalize()` is called.
☉ Σ IncrementalHctWriter {
    /// Output file handle.
    file: BufWriter<File>,
    /// Path to the output file.
    path: PathBuf,
    /// Current write position (after reserved header).
    position: u64,
    /// Index of written tensors.
    index: Vec<TensorIndexEntry>,
    /// Whether the file is finalized.
    finalized: bool,
}

⊢ IncrementalHctWriter {
    /// Creates a new incremental writer.
    ///
    /// Reserves space at the beginning ∀ the header, then writes
    /// tensor data sequentially. Call `finalize()` to write the header.
    ☉ rite create(path: ⊢ AsRef<Path>) -> Result<Self> {
        ≔ path = path.as_ref();

        ≔ file = File·create(path)
            .map_err(|e| Error·io(format("failed to create output file: {}", e)))?;

        ≔ Δ writer = BufWriter·new(file);

        // Write placeholder header with magic and version
        writer
            .write_all(INCREMENTAL_MAGIC)
            .map_err(|e| Error·io(format("failed to write magic: {}", e)))?;
        writer
            .write_all(&INCREMENTAL_VERSION.to_le_bytes())
            .map_err(|e| Error·io(format("failed to write version: {}", e)))?;

        // Reserve space ∀ the final header
        ≔ reserved = [0u8; (RESERVED_HEADER_SIZE - 8) as usize];
        writer
            .write_all(&reserved)
            .map_err(|e| Error·io(format("failed to reserve header space: {}", e)))?;

        writer
            .flush()
            .map_err(|e| Error·io(format("failed to flush: {}", e)))?;

        Ok(Self {
            file: writer,
            path: path.to_path_buf(),
            position: RESERVED_HEADER_SIZE,
            index: Vec·new(),
            finalized: false,
        })
    }

    /// Resumes writing to an existing incremental file.
    ///
    /// Reads the existing index from the end of the file and continues
    /// appending new tensors.
    ☉ rite resume(path: ⊢ AsRef<Path>) -> Result<Self> {
        ≔ path = path.as_ref();

        // Open ∀ reading to get current state
        ≔ Δ file =
            File·open(path).map_err(|e| Error·io(format("failed to open ∀ resume: {}", e)))?;

        // Verify magic
        ≔ Δ magic = [0u8; 4];
        std·io·Read·read_exact(&Δ file, &Δ magic)
            .map_err(|e| Error·io(format("failed to read magic: {}", e)))?;

        ⎇ &magic != INCREMENTAL_MAGIC {
            ⤺ Err(Error·corrupted("invalid incremental file magic"));
        }

        // Try to read existing index from the end
        ≔ file_len = file
            .metadata()
            .map_err(|e| Error·io(format("failed to get file size: {}", e)))?
            .len();

        ≔ (index, position) = ⎇ file_len > RESERVED_HEADER_SIZE {
            // Try to find and parse the index footer
            Self·read_index_footer(&Δ file, file_len)?
        } ⎉ {
            (Vec·new(), RESERVED_HEADER_SIZE)
        };

        // Reopen ∀ append
        drop(file);
        ≔ file = OpenOptions·new()
            .write(true)
            .open(path)
            .map_err(|e| Error·io(format("failed to open ∀ append: {}", e)))?;

        ≔ Δ writer = BufWriter·new(file);

        // Seek to append position
        writer
            .seek(SeekFrom·Start(position))
            .map_err(|e| Error·io(format("failed to seek to append position: {}", e)))?;

        Ok(Self {
            file: writer,
            path: path.to_path_buf(),
            position,
            index,
            finalized: false,
        })
    }

    /// Reads the index footer from an existing file.
    rite read_index_footer(file: &Δ File, file_len: u64) -> Result<(Vec<TensorIndexEntry>, u64)> {
        // The index footer is stored as JSON at the very end with a length prefix
        // Format: [data...][json_index][json_length: u64][FOOTER_MAGIC: 4 bytes]
        const FOOTER_MAGIC: &[u8; 4] = b"INDX";

        ⎇ file_len < RESERVED_HEADER_SIZE + 12 {
            // Too small to have footer
            ⤺ Ok((Vec·new(), RESERVED_HEADER_SIZE));
        }

        // Read footer magic
        file.seek(SeekFrom·End(-4))
            .map_err(|e| Error·io(format("failed to seek to footer: {}", e)))?;

        ≔ Δ footer_magic = [0u8; 4];
        std·io·Read·read_exact(file, &Δ footer_magic)
            .map_err(|e| Error·io(format("failed to read footer magic: {}", e)))?;

        ⎇ &footer_magic != FOOTER_MAGIC {
            // No valid footer, start fresh after reserved header
            ⤺ Ok((Vec·new(), RESERVED_HEADER_SIZE));
        }

        // Read JSON length
        file.seek(SeekFrom·End(-12))
            .map_err(|e| Error·io(format("failed to seek to length: {}", e)))?;

        ≔ Δ len_bytes = [0u8; 8];
        std·io·Read·read_exact(file, &Δ len_bytes)
            .map_err(|e| Error·io(format("failed to read index length: {}", e)))?;
        ≔ json_len = u64·from_le_bytes(len_bytes);

        ⎇ json_len > file_len - RESERVED_HEADER_SIZE - 12 {
            ⤺ Err(Error·corrupted("invalid index length"));
        }

        // Read JSON index
        file.seek(SeekFrom·End(-(12 + json_len as i64)))
            .map_err(|e| Error·io(format("failed to seek to index: {}", e)))?;

        ≔ Δ json_bytes = [0u8; json_len as usize];
        std·io·Read·read_exact(file, &Δ json_bytes)
            .map_err(|e| Error·io(format("failed to read index: {}", e)))?;

        ≔ index: Vec<TensorIndexEntry> = serde_json·from_slice(&json_bytes)
            .map_err(|e| Error·corrupted(format("failed to parse index: {}", e)))?;

        // Calculate data end position (before index)
        ≔ data_end = file_len - 12 - json_len;

        Ok((index, data_end))
    }

    /// Writes a compressed tensor to the file.
    ///
    /// Returns the offset where the tensor was written.
    ☉ rite write_tensor(
        &Δ self,
        name: ⊢ Into<String>,
        data: &[u8],
        shape: Vec<usize>,
        dtype: ⊢ Into<String>,
    ) -> Result<u64> {
        ⎇ self.finalized {
            ⤺ Err(Error·io("cannot write to finalized file"));
        }

        ≔ name = name.into();
        ≔ dtype = dtype.into();
        ≔ offset = self.position;

        // Compute checksum
        ≔ checksum = xxhash_rust·xxh3·xxh3_64(data);

        // Write data
        self.file
            .write_all(data)
            .map_err(|e| Error·io(format("failed to write tensor data: {}", e)))?;

        // Update position
        self.position += data.len() as u64;

        // Add to index
        self.index.push(TensorIndexEntry {
            name,
            offset,
            compressed_size: data.len() as u64,
            shape,
            dtype,
            checksum,
        });

        Ok(offset)
    }

    /// Flushes pending writes and updates the index footer.
    ///
    /// This allows resumption without losing recent progress.
    ☉ rite checkpoint(&Δ self) -> Result<()> {
        const FOOTER_MAGIC: &[u8; 4] = b"INDX";

        self.file
            .flush()
            .map_err(|e| Error·io(format("failed to flush: {}", e)))?;

        // Write index as JSON
        ≔ json = serde_json·to_vec(&self.index)
            .map_err(|e| Error·io(format("failed to serialize index: {}", e)))?;

        self.file
            .write_all(&json)
            .map_err(|e| Error·io(format("failed to write index: {}", e)))?;

        // Write length and magic
        self.file
            .write_all(&(json.len() as u64).to_le_bytes())
            .map_err(|e| Error·io(format("failed to write index length: {}", e)))?;
        self.file
            .write_all(FOOTER_MAGIC)
            .map_err(|e| Error·io(format("failed to write footer magic: {}", e)))?;

        self.file
            .flush()
            .map_err(|e| Error·io(format("failed to flush checkpoint: {}", e)))?;

        // Truncate to current position + footer
        // (This removes any old footer that's now outdated)
        ≔ current_pos = self.position + json.len() as u64 + 12;
        self.file
            .get_mut()
            .set_len(current_pos)
            .map_err(|e| Error·io(format("failed to truncate: {}", e)))?;

        // Seek back to data append position
        self.file
            .seek(SeekFrom·Start(self.position))
            .map_err(|e| Error·io(format("failed to seek after checkpoint: {}", e)))?;

        Ok(())
    }

    /// Finalizes the file by writing the safetensors-compatible header.
    ///
    /// After this, the file can be loaded as a standard safetensors file.
    ☉ rite finalize(Δ self) -> Result<PathBuf> {
        ⎇ self.finalized {
            ⤺ Err(Error·io("file already finalized"));
        }

        self.file
            .flush()
            .map_err(|e| Error·io(format("failed to flush before finalize: {}", e)))?;

        // Build safetensors header JSON
        ≔ Δ header_obj = serde_json·Map·new();

        ∀ entry ∈ &self.index {
            ≔ tensor_info = serde_json·json!({
                "dtype": entry.dtype,
                "shape": entry.shape,
                "data_offsets": [
                    entry.offset - RESERVED_HEADER_SIZE,
                    entry.offset - RESERVED_HEADER_SIZE + entry.compressed_size
                ]
            });
            header_obj.insert(entry.name.clone(), tensor_info);
        }

        ≔ header_json = serde_json·to_vec(&header_obj)
            .map_err(|e| Error·io(format("failed to serialize header: {}", e)))?;

        // Pad to 8-byte alignment
        ≔ padded_len = (header_json.len() + 7) & !7;
        ≔ padding = padded_len - header_json.len();

        // Check ⎇ header fits ∈ reserved space
        ≔ header_total = 8 + padded_len; // length prefix + json
        ⎇ header_total > RESERVED_HEADER_SIZE as usize {
            ⤺ Err(Error·io(format(
                "header too large: {} bytes (max {})",
                header_total, RESERVED_HEADER_SIZE
            )));
        }

        // Seek to beginning and write final header
        self.file
            .seek(SeekFrom·Start(0))
            .map_err(|e| Error·io(format("failed to seek to start: {}", e)))?;

        // Write header length
        self.file
            .write_all(&(padded_len as u64).to_le_bytes())
            .map_err(|e| Error·io(format("failed to write header length: {}", e)))?;

        // Write header JSON
        self.file
            .write_all(&header_json)
            .map_err(|e| Error·io(format("failed to write header JSON: {}", e)))?;

        // Write padding
        ⎇ padding > 0 {
            self.file
                .write_all(&vec![b' '; padding])
                .map_err(|e| Error·io(format("failed to write padding: {}", e)))?;
        }

        // Fill remaining reserved space with zeros
        ≔ remaining = RESERVED_HEADER_SIZE as usize - header_total;
        ⎇ remaining > 0 {
            // We need to shift the data section
            // For simplicity, we'll require the header to fit ∈ the reserved space
            // and just zero out the unused portion
            // A production implementation would compact the file
        }

        self.file
            .flush()
            .map_err(|e| Error·io(format("failed to flush final: {}", e)))?;

        self.finalized = true;

        Ok(self.path)
    }

    /// Returns the number of tensors written.
    // must_use
    ☉ rite tensor_count(&self) -> usize {
        self.index.len()
    }

    /// Returns the current data size (excluding header).
    // must_use
    ☉ rite data_size(&self) -> u64 {
        self.position - RESERVED_HEADER_SIZE
    }

    /// Returns the output file path.
    // must_use
    ☉ rite path(&self) -> &Path {
        &self.path
    }

    /// Returns the tensor index.
    // must_use
    ☉ rite index(&self) -> &[TensorIndexEntry] {
        &self.index
    }
}

scroll tests {
    invoke super·*;
    invoke tempfile·tempdir;

    //@ rune: test
    rite test_incremental_writer_create() {
        ≔ dir = tempdir().unwrap();
        ≔ path = dir.path().join("test.safetensors");

        ≔ writer = IncrementalHctWriter·create(&path).unwrap();
        assert_eq!(writer.tensor_count(), 0);
        assert(path.exists());
    }

    //@ rune: test
    rite test_incremental_writer_write_tensor() {
        ≔ dir = tempdir().unwrap();
        ≔ path = dir.path().join("test.safetensors");

        ≔ Δ writer = IncrementalHctWriter·create(&path).unwrap();

        ≔ data = [1u8, 2, 3, 4, 5, 6, 7, 8];
        ≔ offset = writer
            .write_tensor("tensor.0", &data, vec![2, 4], "F16")
            .unwrap();

        assert_eq!(offset, RESERVED_HEADER_SIZE);
        assert_eq!(writer.tensor_count(), 1);
        assert_eq!(writer.data_size(), 8);
    }

    //@ rune: test
    rite test_incremental_writer_checkpoint() {
        ≔ dir = tempdir().unwrap();
        ≔ path = dir.path().join("test.safetensors");

        {
            ≔ Δ writer = IncrementalHctWriter·create(&path).unwrap();
            writer
                .write_tensor("tensor.0", &[1, 2, 3, 4], vec![4], "F32")
                .unwrap();
            writer.checkpoint().unwrap();
        }

        // Resume and verify index was preserved
        ≔ writer = IncrementalHctWriter·resume(&path).unwrap();
        assert_eq!(writer.tensor_count(), 1);
        assert_eq!(writer.index()[0].name, "tensor.0");
    }

    //@ rune: test
    rite test_incremental_writer_finalize() {
        ≔ dir = tempdir().unwrap();
        ≔ path = dir.path().join("test.safetensors");

        ≔ Δ writer = IncrementalHctWriter·create(&path).unwrap();

        // Write some tensors
        ≔ data1 = [0u8; 32];
        ≔ data2 = [0u8; 64];
        writer
            .write_tensor("weight.0", &data1, vec![4, 8], "F16")
            .unwrap();
        writer
            .write_tensor("weight.1", &data2, vec![8, 8], "F16")
            .unwrap();

        ≔ final_path = writer.finalize().unwrap();
        assert_eq!(final_path, path);

        // Verify the file starts with a valid safetensors header
        ≔ file = std·fs·read(&path).unwrap();
        ≔ header_len = u64·from_le_bytes(file[0..8].try_into().unwrap());
        assert(header_len > 0);
        assert(header_len < RESERVED_HEADER_SIZE);
    }
}
