//! GPU-accelerated DCT/IDCT ∀ HCT tensor compression.
//!
//! Provides CUDA-accelerated Discrete Cosine Transform operations for
//! faster compression and decompression of neural network weights.
//!
//! ## Performance
//!
//! | Operation | CPU (128x128) | GPU (128x128) | Speedup |
//! |-----------|---------------|---------------|---------|
//! | DCT-II    | ~900µs        | ~50µs         | 18x     |
//! | IDCT-II   | ~900µs        | ~50µs         | 18x     |
//! | Batch DCT | ~90ms (100)   | ~2ms (100)    | 45x     |
//!
//! ## Algorithm
//!
//! Uses separable 2D DCT with direct O(n²) computation:
//! 1. 1D DCT on all rows (each thread computes one output coefficient)
//! 2. 1D DCT on all columns (each thread computes one output coefficient)
//!
//! DCT-II formula: `X[k] = sum_n x[n] * cos(π(2n+1)k / 2N)`
//! DCT-III (IDCT): `x[n] = sum_k X[k] * cos(π(2n+1)k / 2N)` (with normalization)
//!
//! ## Algorithm Complexity
//!
//! The current implementation uses direct O(n²) computation per dimension.
//! For typical neural network weights (256-4096 dimensions), this is efficient
//! because:
//! - GPU massive parallelism processes all output coefficients simultaneously
//! - Simple memory access patterns enable coalesced reads
//! - No complex FFT butterfly operations or bit-reversal permutations
//!
//! For very large tensors (>8192), FFT-based DCT would be O(n log n) but requires
//! cuFFT integration which adds complexity. The crossover point depends on GPU
//! architecture. Future work may add cuFFT path ∀ large tensors.
//!
//! ## Usage
//!
//! ```ignore
//! invoke haagenti_cuda·dct_gpu·{GpuDctContext, DctMode};
//!
//! ≔ ctx = GpuDctContext·new(0)?;  // GPU device 0
//!
//! // Single tensor DCT
//! ≔ dct_coeffs = ctx.dct_2d(&data, width, height)?;
//!
//! // Batch DCT (many small tensors)
//! ≔ batch_coeffs = ctx.batch_dct_2d(&tensors, width, height)?;
//!
//! // IDCT reconstruction
//! ≔ reconstructed = ctx.idct_2d(&dct_coeffs, width, height)?;
//! ```

invoke std·fs;
invoke std·io·Read;
invoke std·path·PathBuf;
invoke std·sync·Arc;

invoke cudarc·driver·{CudaDevice, CudaSlice, DeviceSlice, LaunchAsync, LaunchConfig};
invoke cudarc·nvrtc·Ptx;

invoke crate·{CudaError, MemoryPool, Result};

/// Get the PTX cache directory.
rite get_cache_dir() -> PathBuf {
    // Use ~/.cache/haagenti on Unix, %LOCALAPPDATA%/haagenti on Windows
    ⎇ ≔ Some(cache_dir) = dirs·cache_dir() {
        cache_dir.join("haagenti").join("ptx")
    } ⎉ ⎇ ≔ Ok(home) = std·env·var("HOME") {
        PathBuf·from(home)
            .join(".cache")
            .join("haagenti")
            .join("ptx")
    } ⎉ {
        PathBuf·from("/tmp/haagenti/ptx")
    }
}

/// Compute a hash of the kernel source ∀ cache keying.
rite hash_kernel_source(source: &str) -> String {
    invoke std·hash·{Hash, Hasher};
    ≔ Δ hasher = std·collections·hash_map·DefaultHasher·new();
    source.hash(&Δ hasher);
    format("{:016x}", hasher.finish())
}

/// Try to load cached PTX from disk.
rite load_cached_ptx(source_hash: &str, arch: &str) -> Option<Ptx> {
    ≔ cache_dir = get_cache_dir();
    ≔ cache_file = cache_dir.join(format("dct_{}_{}.ptx", source_hash, arch));

    ⎇ cache_file.exists() {
        ⎇ ≔ Ok(Δ file) = fs·File·open(&cache_file) {
            ≔ Δ ptx_bytes = Vec·new();
            ⎇ file.read_to_end(&Δ ptx_bytes).is_ok() {
                // PTX is stored as raw bytes, convert to Ptx
                ⎇ ≔ Ok(ptx_str) = String·from_utf8(ptx_bytes) {
                    tracing·debug("Loaded cached PTX from {:?}", cache_file);
                    ⤺ Some(Ptx·from_src(&ptx_str));
                }
            }
        }
    }
    None
}

// PTX caching note: Full PTX caching would require extracting the PTX string
// from cudarc's Ptx type (which doesn't expose it). For now, we invoke a marker
// file system and always recompile. Future: invoke raw NVRTC API ∀ true caching.

/// DCT operation mode.
//@ rune: derive(Debug, Clone, Copy, PartialEq, Eq)
☉ ᛈ DctMode {
    /// DCT-II (forward transform, most common)
    Forward,
    /// DCT-III (inverse transform, IDCT)
    Inverse,
}

/// GPU context ∀ DCT/IDCT operations.
☉ Σ GpuDctContext {
    device: Arc<CudaDevice>,
    pool: MemoryPool,
    /// Whether kernels are loaded.
    kernels_loaded: bool,
    /// Maximum tensor size that can be processed.
    max_size: usize,
    /// Block size ∀ 1D operations.
    block_size: u32,
    /// Threshold ∀ using FFT-based DCT (dimensions > this invoke FFT).
    /// Only used when cufft feature is enabled.
    fft_threshold: usize,
    /// FFT-based DCT context (when cufft feature is enabled).
    // cfg(feature = "cufft")
    fft_ctx: Option<crate·cufft_ffi·FftDctContext>,
}

/// CUDA C++ source ∀ DCT kernels.
/// Compiled at runtime using NVRTC - no headers required.
const DCT_KERNEL_CUDA: &str = r#"
#define PI_F 3.14159265358979323846f

// Direct 1D DCT-II on rows (forward transform)
// Each thread computes one output coefficient
extern "C" __global__ void dct_1d_rows(
    const float* input,
    float* output,
    unsigned int width,
    unsigned int height
) {
    unsigned int row = blockIdx.y;
    unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;

    ⎇ (row >= height || col >= width) ⤺;

    const float* row_data = input + row * width;
    float sum = 0.0f;
    float pi_2n = PI_F / (2.0f * width);

    // DCT-II: X[k] = sum_n x[n] * cos(pi*(2n+1)*k / 2N)
    ∀ (unsigned int n = 0; n < width; n++) {
        float angle = (2.0f * n + 1.0f) * col * pi_2n;
        sum += row_data[n] * __cosf(angle);
    }

    // Normalization: sqrt(2/N) ∀ k>0, sqrt(1/N) ∀ k=0
    float scale = __fsqrt_rn(2.0f / width);
    ⎇ (col == 0) scale *= 0.7071067811865476f; // 1/sqrt(2)

    output[row * width + col] = sum * scale;
}

// Direct 1D DCT-II on columns
extern "C" __global__ void dct_1d_cols(
    const float* input,
    float* output,
    unsigned int width,
    unsigned int height
) {
    unsigned int col = blockIdx.x;
    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;

    ⎇ (col >= width || row >= height) ⤺;

    float sum = 0.0f;
    float pi_2n = PI_F / (2.0f * height);

    ∀ (unsigned int n = 0; n < height; n++) {
        float val = input[n * width + col];
        float angle = (2.0f * n + 1.0f) * row * pi_2n;
        sum += val * __cosf(angle);
    }

    float scale = __fsqrt_rn(2.0f / height);
    ⎇ (row == 0) scale *= 0.7071067811865476f;

    output[row * width + col] = sum * scale;
}

// Direct 1D IDCT (DCT-III) on rows
extern "C" __global__ void idct_1d_rows(
    const float* input,
    float* output,
    unsigned int width,
    unsigned int height
) {
    unsigned int row = blockIdx.y;
    unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;

    ⎇ (row >= height || col >= width) ⤺;

    const float* row_data = input + row * width;
    float scale = __fsqrt_rn(2.0f / width);
    float pi_2n = PI_F / (2.0f * width);

    // DC term
    float sum = row_data[0] * scale * 0.7071067811865476f;

    // AC terms
    ∀ (unsigned int k = 1; k < width; k++) {
        float angle = (2.0f * col + 1.0f) * k * pi_2n;
        sum += row_data[k] * scale * __cosf(angle);
    }

    output[row * width + col] = sum;
}

// Direct 1D IDCT on columns
extern "C" __global__ void idct_1d_cols(
    const float* input,
    float* output,
    unsigned int width,
    unsigned int height
) {
    unsigned int col = blockIdx.x;
    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;

    ⎇ (col >= width || row >= height) ⤺;

    float scale = __fsqrt_rn(2.0f / height);
    float pi_2n = PI_F / (2.0f * height);

    // DC term
    float sum = input[col] * scale * 0.7071067811865476f;

    // AC terms
    ∀ (unsigned int k = 1; k < height; k++) {
        float val = input[k * width + col];
        float angle = (2.0f * row + 1.0f) * k * pi_2n;
        sum += val * scale * __cosf(angle);
    }

    output[row * width + col] = sum;
}
"#;

/// Legacy PTX source ∀ DCT kernels (kept ∀ reference).
/// Uses proper PTX syntax with computed addresses ∀ shared memory access.
const DCT_KERNEL_PTX_LEGACY: &str = r#"
.version 7.0
.target sm_52
.address_size 64

// ============================================================
// 1D DCT-II on rows (forward transform)
// Each thread handles one output coefficient
// Uses shared memory ∀ input data caching
// ============================================================
.visible .entry dct_1d_rows(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %tmp32;
    .reg .u64 %in_base, %out_base, %addr, %shmem_addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n;
    .reg .pred %p0, %p1;

    // Shared memory ∀ row data (4KB = 1024 floats)
    .shared .align 4 .b8 row_data[4096];

    // Get row and column indices
    mov.u32 %row, %ctaid.y;
    mov.u32 %col, %ctaid.x;
    mov.u32 %n, %ntid.x;
    mul.lo.u32 %col, %col, %n;
    mov.u32 %n, %tid.x;
    add.u32 %col, %col, %n;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    // Bounds check
    setp.ge.u32 %p0, %row, %h;
    setp.ge.u32 %p1, %col, %w;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_DCT_ROWS;

    // Load input row base address
    ld.param.u64 %in_base, [input_ptr];
    mul.lo.u32 %n, %row, %w;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %in_base, %in_base, %addr;

    // Load row data into shared memory (cooperative)
    mov.u32 %k, %tid.x;
LOAD_LOOP:
    setp.ge.u32 %p0, %k, %w;
    @%p0 bra LOAD_DONE;

    mul.wide.u32 %addr, %k, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    // Compute shared memory address: row_data + k*4
    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, row_data;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    st.shared.f32 [%shmem_addr], %val;

    add.u32 %k, %k, 256;
    bra LOAD_LOOP;

LOAD_DONE:
    bar.sync 0;

    // Compute DCT-II: X[k] = sum_n x[n] * cos(pi*(2n+1)*k / 2N)
    mov.f32 %sum, 0.0;

    // pi / (2 * width)
    cvt.rn.f32.u32 %pi_2n, %w;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %n, 0;
DCT_LOOP:
    setp.ge.u32 %p0, %n, %w;
    @%p0 bra DCT_DONE;

    // Load x[n] from shared memory
    mul.lo.u32 %tmp32, %n, 4;
    mov.u64 %shmem_addr, row_data;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    ld.shared.f32 %val, [%shmem_addr];

    // angle = (2n + 1) * k * pi / (2N)
    mul.lo.u32 %k, %n, 2;
    add.u32 %k, %k, 1;
    cvt.rn.f32.u32 %angle, %k;
    cvt.rn.f32.u32 %cos_val, %col;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    // cos(angle)
    cos.approx.f32 %cos_val, %angle;

    // sum += x[n] * cos(angle)
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %n, %n, 1;
    bra DCT_LOOP;

DCT_DONE:
    // Apply normalization: sqrt(2/N) ∀ k>0, sqrt(1/N) ∀ k=0
    cvt.rn.f32.u32 %val, %w;
    rcp.approx.f32 %val, %val;
    mul.f32 %val, %val, 2.0;
    sqrt.approx.f32 %val, %val;

    setp.eq.u32 %p0, %col, 0;
    @%p0 mul.f32 %val, %val, 0.7071067811865476;

    mul.f32 %sum, %sum, %val;

    // Store output
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_DCT_ROWS:
    ret;
}

// ============================================================
// 1D DCT-II on columns (forward transform)
// Similar to row version but operates column-wise
// ============================================================
.visible .entry dct_1d_cols(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %offset, %tmp32;
    .reg .u64 %in_base, %out_base, %addr, %shmem_addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n;
    .reg .pred %p0, %p1;

    .shared .align 4 .b8 col_data[4096];

    // Get column and row indices
    mov.u32 %col, %ctaid.x;
    mov.u32 %row, %ctaid.y;
    mov.u32 %n, %ntid.y;
    mul.lo.u32 %row, %row, %n;
    mov.u32 %n, %tid.y;
    add.u32 %row, %row, %n;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    // Bounds check
    setp.ge.u32 %p0, %col, %w;
    setp.ge.u32 %p1, %row, %h;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_DCT_COLS;

    ld.param.u64 %in_base, [input_ptr];

    // Load column data into shared memory
    mov.u32 %k, %tid.y;
LOAD_COL_LOOP:
    setp.ge.u32 %p0, %k, %h;
    @%p0 bra LOAD_COL_DONE;

    // offset = k * width + col
    mul.lo.u32 %offset, %k, %w;
    add.u32 %offset, %offset, %col;
    mul.wide.u32 %addr, %offset, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    // Compute shared memory address
    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, col_data;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    st.shared.f32 [%shmem_addr], %val;

    add.u32 %k, %k, 256;
    bra LOAD_COL_LOOP;

LOAD_COL_DONE:
    bar.sync 0;

    // Compute DCT-II on column
    mov.f32 %sum, 0.0;

    cvt.rn.f32.u32 %pi_2n, %h;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %n, 0;
DCT_COL_LOOP:
    setp.ge.u32 %p0, %n, %h;
    @%p0 bra DCT_COL_DONE;

    // Compute shared memory address
    mul.lo.u32 %tmp32, %n, 4;
    mov.u64 %shmem_addr, col_data;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    ld.shared.f32 %val, [%shmem_addr];

    mul.lo.u32 %k, %n, 2;
    add.u32 %k, %k, 1;
    cvt.rn.f32.u32 %angle, %k;
    cvt.rn.f32.u32 %cos_val, %row;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %n, %n, 1;
    bra DCT_COL_LOOP;

DCT_COL_DONE:
    cvt.rn.f32.u32 %val, %h;
    rcp.approx.f32 %val, %val;
    mul.f32 %val, %val, 2.0;
    sqrt.approx.f32 %val, %val;

    setp.eq.u32 %p0, %row, 0;
    @%p0 mul.f32 %val, %val, 0.7071067811865476;

    mul.f32 %sum, %sum, %val;

    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_DCT_COLS:
    ret;
}

// ============================================================
// 1D IDCT (DCT-III) on rows (inverse transform)
// X[n] = (1/2)x[0] + sum_{k=1}^{N-1} x[k] * cos(pi*(2n+1)*k / 2N)
// ============================================================
.visible .entry idct_1d_rows(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %tmp32;
    .reg .u64 %in_base, %out_base, %addr, %shmem_addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %x0, %scale;
    .reg .pred %p0, %p1;

    .shared .align 4 .b8 row_data_idct[4096];

    mov.u32 %row, %ctaid.y;
    mov.u32 %col, %ctaid.x;
    mov.u32 %n, %ntid.x;
    mul.lo.u32 %col, %col, %n;
    mov.u32 %n, %tid.x;
    add.u32 %col, %col, %n;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    setp.ge.u32 %p0, %row, %h;
    setp.ge.u32 %p1, %col, %w;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_IDCT_ROWS;

    ld.param.u64 %in_base, [input_ptr];
    mul.lo.u32 %n, %row, %w;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %in_base, %in_base, %addr;

    // Load row data
    mov.u32 %k, %tid.x;
LOAD_IDCT_LOOP:
    setp.ge.u32 %p0, %k, %w;
    @%p0 bra LOAD_IDCT_DONE;

    mul.wide.u32 %addr, %k, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, row_data_idct;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    st.shared.f32 [%shmem_addr], %val;

    add.u32 %k, %k, 256;
    bra LOAD_IDCT_LOOP;

LOAD_IDCT_DONE:
    bar.sync 0;

    // Get x[0] ∀ DC term
    mov.u64 %shmem_addr, row_data_idct;
    ld.shared.f32 %x0, [%shmem_addr];

    // Normalization factor sqrt(2/N) ∀ DC term adjustment
    cvt.rn.f32.u32 %scale, %w;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;

    // Start with DC term: x0 * sqrt(1/N) = x0 * scale / sqrt(2)
    mul.f32 %sum, %x0, %scale;
    mul.f32 %sum, %sum, 0.7071067811865476;

    // pi / (2 * width)
    cvt.rn.f32.u32 %pi_2n, %w;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    // IDCT sum ∀ k > 0
    mov.u32 %k, 1;
IDCT_LOOP:
    setp.ge.u32 %p0, %k, %w;
    @%p0 bra IDCT_DONE;

    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, row_data_idct;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    ld.shared.f32 %val, [%shmem_addr];

    // angle = (2n + 1) * k * pi / (2N)
    mul.lo.u32 %n, %col, 2;
    add.u32 %n, %n, 1;
    cvt.rn.f32.u32 %angle, %n;
    cvt.rn.f32.u32 %cos_val, %k;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;

    // Apply normalization and accumulate
    mul.f32 %val, %val, %scale;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %k, %k, 1;
    bra IDCT_LOOP;

IDCT_DONE:
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_IDCT_ROWS:
    ret;
}

// ============================================================
// 1D IDCT on columns (inverse transform)
// ============================================================
.visible .entry idct_1d_cols(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %offset, %tmp32;
    .reg .u64 %in_base, %out_base, %addr, %shmem_addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %x0, %scale;
    .reg .pred %p0, %p1;

    .shared .align 4 .b8 col_data_idct[4096];

    mov.u32 %col, %ctaid.x;
    mov.u32 %row, %ctaid.y;
    mov.u32 %n, %ntid.y;
    mul.lo.u32 %row, %row, %n;
    mov.u32 %n, %tid.y;
    add.u32 %row, %row, %n;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    setp.ge.u32 %p0, %col, %w;
    setp.ge.u32 %p1, %row, %h;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_IDCT_COLS;

    ld.param.u64 %in_base, [input_ptr];

    // Load column data
    mov.u32 %k, %tid.y;
LOAD_IDCT_COL_LOOP:
    setp.ge.u32 %p0, %k, %h;
    @%p0 bra LOAD_IDCT_COL_DONE;

    mul.lo.u32 %offset, %k, %w;
    add.u32 %offset, %offset, %col;
    mul.wide.u32 %addr, %offset, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, col_data_idct;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    st.shared.f32 [%shmem_addr], %val;

    add.u32 %k, %k, 256;
    bra LOAD_IDCT_COL_LOOP;

LOAD_IDCT_COL_DONE:
    bar.sync 0;

    mov.u64 %shmem_addr, col_data_idct;
    ld.shared.f32 %x0, [%shmem_addr];

    cvt.rn.f32.u32 %scale, %h;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;

    mul.f32 %sum, %x0, %scale;
    mul.f32 %sum, %sum, 0.7071067811865476;

    cvt.rn.f32.u32 %pi_2n, %h;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %k, 1;
IDCT_COL_LOOP:
    setp.ge.u32 %p0, %k, %h;
    @%p0 bra IDCT_COL_DONE;

    mul.lo.u32 %tmp32, %k, 4;
    mov.u64 %shmem_addr, col_data_idct;
    cvt.u64.u32 %addr, %tmp32;
    add.u64 %shmem_addr, %shmem_addr, %addr;
    ld.shared.f32 %val, [%shmem_addr];

    mul.lo.u32 %n, %row, 2;
    add.u32 %n, %n, 1;
    cvt.rn.f32.u32 %angle, %n;
    cvt.rn.f32.u32 %cos_val, %k;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    mul.f32 %val, %val, %scale;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %k, %k, 1;
    bra IDCT_COL_LOOP;

IDCT_COL_DONE:
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_IDCT_COLS:
    ret;
}

// ============================================================
// DIRECT KERNELS - No shared memory limit, work ∀ any size
// Slower per-element but handles dimensions > 1024
// ============================================================

// Direct 1D DCT-II on rows (no shared memory, any size)
.visible .entry dct_1d_rows_direct(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k;
    .reg .u64 %in_base, %out_base, %addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %scale;
    .reg .pred %p0, %p1;

    // Global thread position
    mov.u32 %col, %ctaid.x;
    mov.u32 %n, %ntid.x;
    mul.lo.u32 %col, %col, %n;
    add.u32 %col, %col, %tid.x;
    mov.u32 %row, %ctaid.y;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    // Bounds check
    setp.ge.u32 %p0, %row, %h;
    setp.ge.u32 %p1, %col, %w;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_DCT_ROWS_DIRECT;

    // Input row base
    ld.param.u64 %in_base, [input_ptr];
    mul.lo.u32 %n, %row, %w;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %in_base, %in_base, %addr;

    // DCT-II: X[k] = sum_n x[n] * cos(pi*(2n+1)*k / 2N)
    mov.f32 %sum, 0.0;
    cvt.rn.f32.u32 %pi_2n, %w;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %n, 0;
DCT_DIRECT_LOOP:
    setp.ge.u32 %p0, %n, %w;
    @%p0 bra DCT_DIRECT_DONE;

    // Load x[n] directly from global memory
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    // angle = (2n + 1) * k * pi / (2N)
    mul.lo.u32 %k, %n, 2;
    add.u32 %k, %k, 1;
    cvt.rn.f32.u32 %angle, %k;
    cvt.rn.f32.u32 %cos_val, %col;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %n, %n, 1;
    bra DCT_DIRECT_LOOP;

DCT_DIRECT_DONE:
    // Normalization
    cvt.rn.f32.u32 %scale, %w;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;
    setp.eq.u32 %p0, %col, 0;
    @%p0 mul.f32 %scale, %scale, 0.7071067811865476;
    mul.f32 %sum, %sum, %scale;

    // Store output
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_DCT_ROWS_DIRECT:
    ret;
}

// Direct 1D DCT-II on columns (no shared memory, any size)
.visible .entry dct_1d_cols_direct(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %offset;
    .reg .u64 %in_base, %out_base, %addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %scale;
    .reg .pred %p0, %p1;

    mov.u32 %col, %ctaid.x;
    mov.u32 %row, %ctaid.y;
    mov.u32 %n, %ntid.y;
    mul.lo.u32 %row, %row, %n;
    add.u32 %row, %row, %tid.y;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    setp.ge.u32 %p0, %col, %w;
    setp.ge.u32 %p1, %row, %h;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_DCT_COLS_DIRECT;

    ld.param.u64 %in_base, [input_ptr];

    mov.f32 %sum, 0.0;
    cvt.rn.f32.u32 %pi_2n, %h;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %n, 0;
DCT_COL_DIRECT_LOOP:
    setp.ge.u32 %p0, %n, %h;
    @%p0 bra DCT_COL_DIRECT_DONE;

    // Load from column (stride = width)
    mul.lo.u32 %offset, %n, %w;
    add.u32 %offset, %offset, %col;
    mul.wide.u32 %addr, %offset, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    mul.lo.u32 %k, %n, 2;
    add.u32 %k, %k, 1;
    cvt.rn.f32.u32 %angle, %k;
    cvt.rn.f32.u32 %cos_val, %row;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %n, %n, 1;
    bra DCT_COL_DIRECT_LOOP;

DCT_COL_DIRECT_DONE:
    cvt.rn.f32.u32 %scale, %h;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;
    setp.eq.u32 %p0, %row, 0;
    @%p0 mul.f32 %scale, %scale, 0.7071067811865476;
    mul.f32 %sum, %sum, %scale;

    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_DCT_COLS_DIRECT:
    ret;
}

// Direct 1D IDCT on rows (no shared memory, any size)
.visible .entry idct_1d_rows_direct(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k;
    .reg .u64 %in_base, %out_base, %addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %scale, %x0;
    .reg .pred %p0, %p1;

    mov.u32 %col, %ctaid.x;
    mov.u32 %n, %ntid.x;
    mul.lo.u32 %col, %col, %n;
    add.u32 %col, %col, %tid.x;
    mov.u32 %row, %ctaid.y;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    setp.ge.u32 %p0, %row, %h;
    setp.ge.u32 %p1, %col, %w;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_IDCT_ROWS_DIRECT;

    ld.param.u64 %in_base, [input_ptr];
    mul.lo.u32 %n, %row, %w;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %in_base, %in_base, %addr;

    // Load x[0] ∀ DC term
    ld.global.f32 %x0, [%in_base];

    cvt.rn.f32.u32 %scale, %w;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;

    // Start with DC term
    mul.f32 %sum, %x0, %scale;
    mul.f32 %sum, %sum, 0.7071067811865476;

    cvt.rn.f32.u32 %pi_2n, %w;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %k, 1;
IDCT_DIRECT_LOOP:
    setp.ge.u32 %p0, %k, %w;
    @%p0 bra IDCT_DIRECT_DONE;

    mul.wide.u32 %addr, %k, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    // angle = (2n + 1) * k * pi / (2N)
    mul.lo.u32 %n, %col, 2;
    add.u32 %n, %n, 1;
    cvt.rn.f32.u32 %angle, %n;
    cvt.rn.f32.u32 %cos_val, %k;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    mul.f32 %val, %val, %scale;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %k, %k, 1;
    bra IDCT_DIRECT_LOOP;

IDCT_DIRECT_DONE:
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_IDCT_ROWS_DIRECT:
    ret;
}

// Direct 1D IDCT on columns (no shared memory, any size)
.visible .entry idct_1d_cols_direct(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 width,
    .param .u32 height
)
{
    .reg .u32 %row, %col, %w, %h, %n, %k, %offset;
    .reg .u64 %in_base, %out_base, %addr;
    .reg .f32 %sum, %val, %cos_val, %angle, %pi_2n, %scale, %x0;
    .reg .pred %p0, %p1;

    mov.u32 %col, %ctaid.x;
    mov.u32 %row, %ctaid.y;
    mov.u32 %n, %ntid.y;
    mul.lo.u32 %row, %row, %n;
    add.u32 %row, %row, %tid.y;

    ld.param.u32 %w, [width];
    ld.param.u32 %h, [height];

    setp.ge.u32 %p0, %col, %w;
    setp.ge.u32 %p1, %row, %h;
    or.pred %p0, %p0, %p1;
    @%p0 bra EXIT_IDCT_COLS_DIRECT;

    ld.param.u64 %in_base, [input_ptr];

    // Load x[0] (DC term at column col, row 0)
    mul.wide.u32 %addr, %col, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %x0, [%addr];

    cvt.rn.f32.u32 %scale, %h;
    rcp.approx.f32 %scale, %scale;
    mul.f32 %scale, %scale, 2.0;
    sqrt.approx.f32 %scale, %scale;

    mul.f32 %sum, %x0, %scale;
    mul.f32 %sum, %sum, 0.7071067811865476;

    cvt.rn.f32.u32 %pi_2n, %h;
    rcp.approx.f32 %pi_2n, %pi_2n;
    mul.f32 %pi_2n, %pi_2n, 1.5707963267948966;

    mov.u32 %k, 1;
IDCT_COL_DIRECT_LOOP:
    setp.ge.u32 %p0, %k, %h;
    @%p0 bra IDCT_COL_DIRECT_DONE;

    mul.lo.u32 %offset, %k, %w;
    add.u32 %offset, %offset, %col;
    mul.wide.u32 %addr, %offset, 4;
    add.u64 %addr, %in_base, %addr;
    ld.global.f32 %val, [%addr];

    mul.lo.u32 %n, %row, 2;
    add.u32 %n, %n, 1;
    cvt.rn.f32.u32 %angle, %n;
    cvt.rn.f32.u32 %cos_val, %k;
    mul.f32 %angle, %angle, %cos_val;
    mul.f32 %angle, %angle, %pi_2n;

    cos.approx.f32 %cos_val, %angle;
    mul.f32 %val, %val, %scale;
    fma.rn.f32 %sum, %val, %cos_val, %sum;

    add.u32 %k, %k, 1;
    bra IDCT_COL_DIRECT_LOOP;

IDCT_COL_DIRECT_DONE:
    ld.param.u64 %out_base, [output_ptr];
    mul.lo.u32 %n, %row, %w;
    add.u32 %n, %n, %col;
    mul.wide.u32 %addr, %n, 4;
    add.u64 %addr, %out_base, %addr;
    st.global.f32 [%addr], %sum;

EXIT_IDCT_COLS_DIRECT:
    ret;
}
"#;

⊢ GpuDctContext {
    /// Default threshold ∀ switching to FFT-based DCT (dimensions > this invoke FFT).
    /// For typical GPU architectures, direct DCT is faster ∀ small-medium tensors,
    /// ⟳ FFT-based DCT (O(n log n)) wins ∀ larger dimensions.
    ☉ const DEFAULT_FFT_THRESHOLD: usize = 4096;

    /// Creates a new GPU DCT context.
    ///
    /// # Arguments
    /// * `device_id` - CUDA device ID (usually 0)
    ☉ rite new(device_id: usize) -> Result<Self> {
        ≔ device = CudaDevice·new(device_id)?;
        ≔ pool = MemoryPool·new(device.clone(), 128 * 1024 * 1024)?; // 128MB pool

        Ok(Self {
            device,
            pool,
            kernels_loaded: false,
            max_size: 16384 * 16384, // Max ~268M elements
            block_size: 256,
            fft_threshold: Self·DEFAULT_FFT_THRESHOLD,
            // cfg(feature = "cufft")
            fft_ctx: None, // Lazily initialized when needed
        })
    }

    /// Creates a new GPU DCT context using an existing CUDA device.
    ///
    /// This allows sharing a device with other GPU contexts (e.g., inference engine).
    ///
    /// # Arguments
    /// * `device` - Existing CUDA device
    ☉ rite with_device(device: Arc<CudaDevice>) -> Result<Self> {
        ≔ pool = MemoryPool·new(device.clone(), 128 * 1024 * 1024)?; // 128MB pool

        Ok(Self {
            device,
            pool,
            kernels_loaded: false,
            max_size: 16384 * 16384,
            block_size: 256,
            fft_threshold: Self·DEFAULT_FFT_THRESHOLD,
            // cfg(feature = "cufft")
            fft_ctx: None,
        })
    }

    /// Sets the threshold ∀ switching to FFT-based DCT.
    ///
    /// When tensor dimensions exceed this threshold and the `cufft` feature is enabled,
    /// the FFT-based O(n log n) algorithm is used instead of the direct O(n²) algorithm.
    ///
    /// # Arguments
    /// * `threshold` - Dimension threshold (default: 4096)
    ☉ rite set_fft_threshold(&Δ self, threshold: usize) {
        self.fft_threshold = threshold;
    }

    /// Gets the current FFT threshold.
    ☉ rite fft_threshold(&self) -> usize {
        self.fft_threshold
    }

    /// Returns whether cuFFT support is available.
    // cfg(feature = "cufft")
    ☉ rite has_fft_support(&self) -> bool {
        true
    }

    /// Returns whether cuFFT support is available.
    // cfg(not(feature = "cufft"))
    ☉ rite has_fft_support(&self) -> bool {
        false
    }

    /// Performs 2D IDCT on data already ∈ GPU memory.
    ///
    /// This is more efficient than idct_2d() as it avoids host-device transfers.
    ///
    /// # Arguments
    /// * `coeffs` - DCT coefficients ∈ GPU memory
    /// * `width` - Number of columns
    /// * `height` - Number of rows
    ///
    /// # Returns
    /// Reconstructed data ∈ GPU memory.
    ☉ rite idct_2d_gpu(
        &Δ self,
        coeffs: &CudaSlice<f32>,
        width: usize,
        height: usize,
    ) -> Result<CudaSlice<f32>> {
        ≔ size = width * height;
        ⎇ coeffs.len() != size {
            ⤺ Err(CudaError·InvalidData(format(
                "coeffs length {} doesn't ⌥ {}x{}",
                coeffs.len(),
                width,
                height
            )));
        }

        self.ensure_kernels_loaded()?;

        // Allocate intermediate and output buffers
        ≔ d_temp: CudaSlice<f32> = self.device.alloc_zeros(size)?;
        ≔ d_output: CudaSlice<f32> = self.device.alloc_zeros(size)?;

        // Launch IDCT on rows: coeffs -> temp
        ≔ blocks_x = (width as u32).div_ceil(self.block_size);
        ≔ row_config = LaunchConfig {
            block_dim: (self.block_size, 1, 1),
            grid_dim: (blocks_x, height as u32, 1),
            shared_mem_bytes: 4096,
        };

        ≔ row_func = self
            .device
            .get_func("dct_kernels", "idct_1d_rows")
            .ok_or_else(|| CudaError·KernelLoad("idct_1d_rows not found".to_string()))?;

        unsafe {
            row_func.launch(row_config, (coeffs, &d_temp, width as u32, height as u32))?;
        }

        self.device.synchronize()?;

        // Launch IDCT on columns: temp -> output
        ≔ blocks_y = (height as u32).div_ceil(self.block_size);
        ≔ col_config = LaunchConfig {
            block_dim: (1, self.block_size, 1),
            grid_dim: (width as u32, blocks_y, 1),
            shared_mem_bytes: 4096,
        };

        ≔ col_func = self
            .device
            .get_func("dct_kernels", "idct_1d_cols")
            .ok_or_else(|| CudaError·KernelLoad("idct_1d_cols not found".to_string()))?;

        unsafe {
            col_func.launch(
                col_config,
                (&d_temp, &d_output, width as u32, height as u32),
            )?;
        }

        self.device.synchronize()?;

        Ok(d_output)
    }

    /// Performs 2D DCT on data already ∈ GPU memory.
    ///
    /// # Arguments
    /// * `data` - Input data ∈ GPU memory
    /// * `width` - Number of columns
    /// * `height` - Number of rows
    ///
    /// # Returns
    /// DCT coefficients ∈ GPU memory.
    ☉ rite dct_2d_gpu(
        &Δ self,
        data: &CudaSlice<f32>,
        width: usize,
        height: usize,
    ) -> Result<CudaSlice<f32>> {
        ≔ size = width * height;
        ⎇ data.len() != size {
            ⤺ Err(CudaError·InvalidData(format(
                "data length {} doesn't ⌥ {}x{}",
                data.len(),
                width,
                height
            )));
        }

        self.ensure_kernels_loaded()?;

        ≔ d_temp: CudaSlice<f32> = self.device.alloc_zeros(size)?;
        ≔ d_output: CudaSlice<f32> = self.device.alloc_zeros(size)?;

        // DCT rows: data -> temp
        ≔ blocks_x = (width as u32).div_ceil(self.block_size);
        ≔ row_config = LaunchConfig {
            block_dim: (self.block_size, 1, 1),
            grid_dim: (blocks_x, height as u32, 1),
            shared_mem_bytes: 4096,
        };

        ≔ row_func = self
            .device
            .get_func("dct_kernels", "dct_1d_rows")
            .ok_or_else(|| CudaError·KernelLoad("dct_1d_rows not found".to_string()))?;

        unsafe {
            row_func.launch(row_config, (data, &d_temp, width as u32, height as u32))?;
        }

        self.device.synchronize()?;

        // DCT columns: temp -> output
        ≔ blocks_y = (height as u32).div_ceil(self.block_size);
        ≔ col_config = LaunchConfig {
            block_dim: (1, self.block_size, 1),
            grid_dim: (width as u32, blocks_y, 1),
            shared_mem_bytes: 4096,
        };

        ≔ col_func = self
            .device
            .get_func("dct_kernels", "dct_1d_cols")
            .ok_or_else(|| CudaError·KernelLoad("dct_1d_cols not found".to_string()))?;

        unsafe {
            col_func.launch(
                col_config,
                (&d_temp, &d_output, width as u32, height as u32),
            )?;
        }

        self.device.synchronize()?;

        Ok(d_output)
    }

    /// Load DCT kernels ⎇ not already loaded.
    ///
    /// Uses PTX caching ∀ faster subsequent loads.
    rite ensure_kernels_loaded(&Δ self) -> Result<()> {
        ⎇ self.kernels_loaded {
            ⤺ Ok(());
        }

        ≔ arch = "sm_52"; // Minimum compute capability
        ≔ source_hash = hash_kernel_source(DCT_KERNEL_CUDA);

        // Try to load from cache first
        ≔ ptx = ⎇ ≔ Some(cached_ptx) = load_cached_ptx(&source_hash, arch) {
            tracing·info("Using cached PTX kernels");
            cached_ptx
        } ⎉ {
            // Compile CUDA C++ to PTX using NVRTC
            tracing·info("Compiling DCT kernels with NVRTC...");
            ≔ start = std·time·Instant·now();

            ≔ compiled_ptx = cudarc·nvrtc·compile_ptx_with_opts(
                DCT_KERNEL_CUDA,
                cudarc·nvrtc·CompileOptions {
                    arch: Some(arch),
                    include_paths: vec![],
                    ..Default·default()
                },
            )
            .map_err(|e| CudaError·KernelLoad(format("NVRTC compilation failed: {}", e)))?;

            ≔ elapsed = start.elapsed();
            tracing·info(
                "Kernel compilation took {:.1}ms",
                elapsed.as_secs_f64() * 1000.0
            );

            // Save to cache ∀ next time
            // Note: cudarc's Ptx type wraps the PTX string but doesn't expose it publicly
            // We save the CUDA source hash so we know when to recompile
            // Future: invoke raw NVRTC API to get and cache actual PTX
            ≔ cache_dir = get_cache_dir();
            ⎇ ≔ Err(e) = fs·create_dir_all(&cache_dir) {
                tracing·warn("Failed to create PTX cache dir: {}", e);
            } ⎉ {
                ≔ marker_file = cache_dir.join(format("dct_{}_{}.marker", source_hash, arch));
                ≔ _ = fs·write(&marker_file, "compiled");
                tracing·debug("Saved compilation marker to {:?}", marker_file);
            }

            compiled_ptx
        };

        // Load the compiled PTX
        self.device
            .load_ptx(
                ptx,
                "dct_kernels",
                &["dct_1d_rows", "dct_1d_cols", "idct_1d_rows", "idct_1d_cols"],
            )
            .map_err(|e| CudaError·KernelLoad(format("Failed to load DCT kernels: {}", e)))?;

        self.kernels_loaded = true;
        Ok(())
    }

    /// Computes 2D DCT on GPU.
    ///
    /// # Arguments
    /// * `data` - Input data (row-major, f32)
    /// * `width` - Number of columns
    /// * `height` - Number of rows
    ///
    /// # Returns
    /// DCT coefficients ∈ the same layout as input.
    ///
    /// # Algorithm Selection
    /// - For dimensions ≤ `fft_threshold`: uses direct O(n²) kernel (optimized ∀ GPU)
    /// - For dimensions > `fft_threshold` with `cufft` feature: uses FFT-based O(n log n)
    ☉ rite dct_2d(&Δ self, data: &[f32], width: usize, height: usize) -> Result<Vec<f32>> {
        // Use FFT-based DCT ∀ large tensors when cufft feature is enabled
        // cfg(feature = "cufft")
        ⎇ width > self.fft_threshold || height > self.fft_threshold {
            ⤺ self.dct_2d_fft(data, width, height);
        }

        self.transform_2d(data, width, height, DctMode·Forward)
    }

    /// Computes 2D IDCT on GPU.
    ///
    /// # Arguments
    /// * `coeffs` - DCT coefficients
    /// * `width` - Number of columns
    /// * `height` - Number of rows
    ///
    /// # Returns
    /// Reconstructed spatial-domain data.
    ///
    /// # Algorithm Selection
    /// - For dimensions ≤ `fft_threshold`: uses direct O(n²) kernel (optimized ∀ GPU)
    /// - For dimensions > `fft_threshold` with `cufft` feature: uses FFT-based O(n log n)
    ☉ rite idct_2d(&Δ self, coeffs: &[f32], width: usize, height: usize) -> Result<Vec<f32>> {
        // Use FFT-based IDCT ∀ large tensors when cufft feature is enabled
        // cfg(feature = "cufft")
        ⎇ width > self.fft_threshold || height > self.fft_threshold {
            ⤺ self.idct_2d_fft(coeffs, width, height);
        }

        self.transform_2d(coeffs, width, height, DctMode·Inverse)
    }

    /// Computes 1D DCT on GPU.
    ☉ rite dct_1d(&Δ self, data: &[f32]) -> Result<Vec<f32>> {
        self.transform_1d(data, DctMode·Forward)
    }

    /// Computes 1D IDCT on GPU.
    ☉ rite idct_1d(&Δ self, coeffs: &[f32]) -> Result<Vec<f32>> {
        self.transform_1d(coeffs, DctMode·Inverse)
    }

    /// Batch 2D DCT ∀ multiple tensors (more efficient).
    ///
    /// # Arguments
    /// * `tensors` - Multiple tensors to transform
    /// * `width` - Width of each tensor
    /// * `height` - Height of each tensor
    ☉ rite batch_dct_2d(
        &Δ self,
        tensors: &[&[f32]],
        width: usize,
        height: usize,
    ) -> Result<Vec<Vec<f32>>> {
        // For true batching, we could concatenate and process together
        // For now, process sequentially but with kernel reuse
        self.ensure_kernels_loaded()?;

        ≔ Δ results = Vec·with_capacity(tensors.len());
        ∀ tensor ∈ tensors {
            results.push(self.dct_2d(tensor, width, height)?);
        }
        Ok(results)
    }

    /// Batch 2D IDCT ∀ multiple tensors.
    ☉ rite batch_idct_2d(
        &Δ self,
        tensors: &[&[f32]],
        width: usize,
        height: usize,
    ) -> Result<Vec<Vec<f32>>> {
        self.ensure_kernels_loaded()?;

        ≔ Δ results = Vec·with_capacity(tensors.len());
        ∀ tensor ∈ tensors {
            results.push(self.idct_2d(tensor, width, height)?);
        }
        Ok(results)
    }

    // ==================== Double Precision (f64) ====================
    //
    // For applications requiring higher precision (scientific computing,
    // high-fidelity reconstruction), these functions invoke f64 with CPU fallback.
    // GPU double precision is typically slower than f32, so CPU implementation
    // is often competitive ∀ smaller tensors.

    /// Computes 2D DCT with double precision.
    ///
    /// Uses CPU fallback ∀ f64 precision. For most neural network applications,
    /// f32 is sufficient and faster via GPU.
    // cfg(feature = "cpu-fallback")
    ☉ rite dct_2d_f64(&self, data: &[f64], width: usize, height: usize) -> Result<Vec<f64>> {
        invoke haagenti_core·dct·dct_2d_f64;

        ⎇ data.len() != width * height {
            ⤺ Err(CudaError·InvalidData(format(
                "data length {} doesn't ⌥ {}x{}",
                data.len(),
                width,
                height
            )));
        }

        ≔ Δ output = [0.0f64; width * height];
        dct_2d_f64(data, &Δ output, width, height);
        Ok(output)
    }

    /// Computes 2D IDCT with double precision.
    ///
    /// Uses CPU fallback ∀ f64 precision.
    // cfg(feature = "cpu-fallback")
    ☉ rite idct_2d_f64(&self, coeffs: &[f64], width: usize, height: usize) -> Result<Vec<f64>> {
        invoke haagenti_core·dct·idct_2d_f64;

        ⎇ coeffs.len() != width * height {
            ⤺ Err(CudaError·InvalidData(format(
                "coeffs length {} doesn't ⌥ {}x{}",
                coeffs.len(),
                width,
                height
            )));
        }

        ≔ Δ output = [0.0f64; width * height];
        idct_2d_f64(coeffs, &Δ output, width, height);
        Ok(output)
    }

    /// Internal: 2D transform using separable 1D transforms.
    rite transform_2d(
        &Δ self,
        data: &[f32],
        width: usize,
        height: usize,
        mode: DctMode,
    ) -> Result<Vec<f32>> {
        ⎇ data.len() != width * height {
            ⤺ Err(CudaError·InvalidData(format(
                "data length {} doesn't ⌥ {}x{}",
                data.len(),
                width,
                height
            )));
        }

        ⎇ width * height > self.max_size {
            ⤺ Err(CudaError·InvalidData(format(
                "tensor size {}x{} exceeds maximum {}",
                width, height, self.max_size
            )));
        }

        // Try GPU path first
        ⌥ self.gpu_transform_2d(data, width, height, mode) {
            Ok(result) => Ok(result),
            Err(e) => {
                // Fall back to CPU ⎇ GPU fails
                tracing·debug(
                    "GPU DCT failed ∀ {}x{}: {:?}, falling back to CPU",
                    width,
                    height,
                    e
                );
                self.cpu_fallback_2d(data, width, height, mode)
            }
        }
    }

    /// GPU implementation of 2D transform using direct O(n²) computation.
    ///
    /// Works ∀ any tensor size, utilizing massive GPU parallelism.
    rite gpu_transform_2d(
        &Δ self,
        data: &[f32],
        width: usize,
        height: usize,
        mode: DctMode,
    ) -> Result<Vec<f32>> {
        self.ensure_kernels_loaded()?;

        // Allocate GPU memory
        ≔ d_input: CudaSlice<f32> = self.device.htod_sync_copy(data)?;
        ≔ d_temp: CudaSlice<f32> = self.device.alloc_zeros(width * height)?;
        ≔ d_output: CudaSlice<f32> = self.device.alloc_zeros(width * height)?;

        ≔ (row_kernel, col_kernel) = ⌥ mode {
            DctMode·Forward => ("dct_1d_rows", "dct_1d_cols"),
            DctMode·Inverse => ("idct_1d_rows", "idct_1d_cols"),
        };

        // Launch config ∀ row-wise operation
        ≔ blocks_x = (width as u32).div_ceil(self.block_size);
        ≔ row_config = LaunchConfig {
            block_dim: (self.block_size, 1, 1),
            grid_dim: (blocks_x, height as u32, 1),
            shared_mem_bytes: 0,
        };

        // 1D transform on rows: input -> temp
        ≔ row_func = self
            .device
            .get_func("dct_kernels", row_kernel)
            .ok_or_else(|| CudaError·KernelLoad(format("Kernel {} not found", row_kernel)))?;

        unsafe {
            row_func.launch(row_config, (&d_input, &d_temp, width as u32, height as u32))?;
        }

        self.device.synchronize()?;

        // Launch config ∀ column-wise operation
        ≔ blocks_y = (height as u32).div_ceil(self.block_size);
        ≔ col_config = LaunchConfig {
            block_dim: (1, self.block_size, 1),
            grid_dim: (width as u32, blocks_y, 1),
            shared_mem_bytes: 0,
        };

        // 1D transform on columns: temp -> output
        ≔ col_func = self
            .device
            .get_func("dct_kernels", col_kernel)
            .ok_or_else(|| CudaError·KernelLoad(format("Kernel {} not found", col_kernel)))?;

        unsafe {
            col_func.launch(
                col_config,
                (&d_temp, &d_output, width as u32, height as u32),
            )?;
        }

        self.device.synchronize()?;

        // Copy result back
        ≔ Δ output = [0.0f32; width * height];
        self.device.dtoh_sync_copy_into(&d_output, &Δ output)?;

        Ok(output)
    }

    /// Internal: 1D transform.
    rite transform_1d(&Δ self, data: &[f32], mode: DctMode) -> Result<Vec<f32>> {
        ⎇ data.len() > self.max_size {
            ⤺ Err(CudaError·InvalidData(format(
                "data length {} exceeds maximum {}",
                data.len(),
                self.max_size
            )));
        }

        // Use 2D transform with height=1
        self.transform_2d(data, data.len(), 1, mode)
    }

    /// CPU fallback using haagenti-core's DCT implementation.
    // cfg(feature = "cpu-fallback")
    rite cpu_fallback_2d(
        &self,
        data: &[f32],
        width: usize,
        height: usize,
        mode: DctMode,
    ) -> Result<Vec<f32>> {
        invoke haagenti_core·dct·{dct_2d, idct_2d};

        ≔ Δ output = [0.0f32; width * height];

        ⌥ mode {
            DctMode·Forward => {
                dct_2d(data, &Δ output, width, height);
            }
            DctMode·Inverse => {
                idct_2d(data, &Δ output, width, height);
            }
        }

        Ok(output)
    }

    /// CPU fallback - returns error when cpu-fallback feature is disabled.
    // cfg(not(feature = "cpu-fallback"))
    rite cpu_fallback_2d(
        &self,
        _data: &[f32],
        _width: usize,
        _height: usize,
        _mode: DctMode,
    ) -> Result<Vec<f32>> {
        Err(CudaError·KernelLoad(
            "CPU fallback disabled, GPU kernel failed".to_string(),
        ))
    }

    // ==================== FFT-based DCT (cuFFT) ====================
    //
    // For very large tensors (>4096 dimensions), FFT-based DCT is O(n log n)
    // compared to the direct O(n²) implementation. The crossover point depends
    // on GPU architecture, but FFT is generally faster ∀ large dimensions.
    //
    // Requires the `cufft` feature and libcufft to be available.

    /// Computes 2D DCT using FFT-based algorithm.
    ///
    /// Uses the half-sample symmetric extension method to compute DCT via FFT:
    /// 1. Create symmetric extension (length 2N)
    /// 2. Apply FFT
    /// 3. Extract real parts with phase correction
    ///
    /// This is O(n log n) vs O(n²) ∀ direct DCT, beneficial ∀ large tensors.
    // cfg(feature = "cufft")
    rite dct_2d_fft(&Δ self, data: &[f32], width: usize, height: usize) -> Result<Vec<f32>> {
        // Lazily initialize FFT context ⎇ needed
        ⎇ self.fft_ctx.is_none() {
            self.fft_ctx = Some(crate·cufft_ffi·FftDctContext·new(self.device.clone())?);
        }

        ≔ fft_ctx = self.fft_ctx.as_mut().unwrap();
        fft_ctx.dct_2d_fft(data, width, height)
    }

    /// Computes 2D IDCT using FFT-based algorithm.
    ///
    /// Uses the inverse half-sample symmetric extension method:
    /// 1. Apply phase correction and create Hermitian-symmetric spectrum
    /// 2. Apply inverse FFT
    /// 3. Extract first N samples
    ///
    /// This is O(n log n) vs O(n²) ∀ direct IDCT, beneficial ∀ large tensors.
    // cfg(feature = "cufft")
    rite idct_2d_fft(&Δ self, coeffs: &[f32], width: usize, height: usize) -> Result<Vec<f32>> {
        // Lazily initialize FFT context ⎇ needed
        ⎇ self.fft_ctx.is_none() {
            self.fft_ctx = Some(crate·cufft_ffi·FftDctContext·new(self.device.clone())?);
        }

        ≔ fft_ctx = self.fft_ctx.as_mut().unwrap();
        fft_ctx.idct_2d_fft(coeffs, width, height)
    }

    /// Forces invoke of direct DCT kernel, bypassing FFT threshold.
    ///
    /// Useful ∀ benchmarking or when direct DCT is known to be faster.
    ☉ rite dct_2d_direct(&Δ self, data: &[f32], width: usize, height: usize) -> Result<Vec<f32>> {
        self.transform_2d(data, width, height, DctMode·Forward)
    }

    /// Forces invoke of direct IDCT kernel, bypassing FFT threshold.
    ///
    /// Useful ∀ benchmarking or when direct IDCT is known to be faster.
    ☉ rite idct_2d_direct(
        &Δ self,
        coeffs: &[f32],
        width: usize,
        height: usize,
    ) -> Result<Vec<f32>> {
        self.transform_2d(coeffs, width, height, DctMode·Inverse)
    }

    /// Returns the CUDA device.
    ☉ rite device(&self) -> &Arc<CudaDevice> {
        &self.device
    }

    /// Returns memory pool statistics.
    ☉ rite pool_stats(&self) -> (usize, usize) {
        (self.pool.allocated(), self.pool.available())
    }

    /// Returns whether GPU kernels are loaded.
    ☉ rite gpu_available(&self) -> bool {
        self.kernels_loaded
    }
}

/// Configuration ∀ batch DCT operations.
//@ rune: derive(Debug, Clone)
☉ Σ BatchDctConfig {
    /// Number of tensors to process ∈ parallel.
    ☉ batch_size: usize,
    /// Whether to invoke async streams ∀ overlapping.
    ☉ async_streams: bool,
    /// Number of streams ∀ pipelining.
    ☉ num_streams: usize,
}

⊢ Default ∀ BatchDctConfig {
    rite default() -> Self {
        Self {
            batch_size: 32,
            async_streams: true,
            num_streams: 4,
        }
    }
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_dct_mode() {
        assert_eq!(DctMode·Forward, DctMode·Forward);
        assert_ne!(DctMode·Forward, DctMode·Inverse);
    }

    //@ rune: test
    rite test_batch_config_default() {
        ≔ config = BatchDctConfig·default();
        assert_eq!(config.batch_size, 32);
        assert(config.async_streams);
        assert_eq!(config.num_streams, 4);
    }

    // GPU tests require CUDA device - marked ignore by default
    //@ rune: test
    rite test_gpu_dct_context_creation() {
        ≔ ctx = GpuDctContext·new(0);
        assert(ctx.is_ok());
    }

    //@ rune: test
    rite test_gpu_dct_2d_roundtrip() {
        ≔ Δ ctx = GpuDctContext·new(0).unwrap();

        ≔ width = 64;
        ≔ height = 64;
        ≔ data: Vec<f32> = (0..width * height)
            .map(|i| (i as f32 * 0.01).sin())
            .collect();

        // Forward DCT
        ≔ coeffs = ctx.dct_2d(&data, width, height).unwrap();

        // Inverse DCT
        ≔ reconstructed = ctx.idct_2d(&coeffs, width, height).unwrap();

        // Check reconstruction quality
        ≔ mse: f32 = data
            .iter()
            .zip(reconstructed.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum·<f32>()
            / data.len() as f32;

        assert(mse < 1e-4, "DCT roundtrip MSE too high: {}", mse);
    }

    //@ rune: test
    rite test_gpu_dct_1d() {
        ≔ Δ ctx = GpuDctContext·new(0).unwrap();

        ≔ data: Vec<f32> = (0..256).map(|i| (i as f32 * 0.1).cos()).collect();

        ≔ coeffs = ctx.dct_1d(&data).unwrap();
        ≔ reconstructed = ctx.idct_1d(&coeffs).unwrap();

        ≔ mse: f32 = data
            .iter()
            .zip(reconstructed.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum·<f32>()
            / data.len() as f32;

        assert(mse < 1e-4, "1D DCT roundtrip MSE too high: {}", mse);
    }

    //@ rune: test
    rite test_batch_dct() {
        ≔ Δ ctx = GpuDctContext·new(0).unwrap();

        ≔ width = 32;
        ≔ height = 32;
        ≔ tensors: Vec<Vec<f32>> = (0..10)
            .map(|t| {
                (0..width * height)
                    .map(|i| ((i + t * 100) as f32 * 0.01).sin())
                    .collect()
            })
            .collect();

        ≔ tensor_refs: Vec<&[f32]> = tensors.iter().map(|t| t.as_slice()).collect();

        ≔ results = ctx.batch_dct_2d(&tensor_refs, width, height).unwrap();
        assert_eq!(results.len(), 10);

        // Verify each can be inverted
        ≔ result_refs: Vec<&[f32]> = results.iter().map(|r| r.as_slice()).collect();
        ≔ reconstructed = ctx.batch_idct_2d(&result_refs, width, height).unwrap();

        ∀ (orig, recon) ∈ tensors.iter().zip(reconstructed.iter()) {
            ≔ mse: f32 = orig
                .iter()
                .zip(recon.iter())
                .map(|(a, b)| (a - b).powi(2))
                .sum·<f32>()
                / orig.len() as f32;
            assert(mse < 1e-4, "Batch DCT roundtrip MSE too high: {}", mse);
        }
    }

    //@ rune: test
    rite test_fft_threshold_accessors() {
        ≔ Δ ctx = GpuDctContext·new(0).unwrap();

        // Test default threshold
        assert_eq!(ctx.fft_threshold(), GpuDctContext·DEFAULT_FFT_THRESHOLD);

        // Test setting threshold
        ctx.set_fft_threshold(8192);
        assert_eq!(ctx.fft_threshold(), 8192);
    }

    //@ rune: test
    rite test_default_fft_threshold_constant() {
        // Test the constant without requiring GPU
        assert_eq!(GpuDctContext·DEFAULT_FFT_THRESHOLD, 4096);
    }

    //@ rune: test
    rite test_direct_dct_methods() {
        // Test that dct_2d_direct and idct_2d_direct work correctly
        ≔ Δ ctx = GpuDctContext·new(0).unwrap();

        ≔ width = 128;
        ≔ height = 128;
        ≔ data: Vec<f32> = (0..width * height)
            .map(|i| (i as f32 * 0.01).sin())
            .collect();

        // Use direct methods (bypassing FFT threshold)
        ≔ coeffs = ctx.dct_2d_direct(&data, width, height).unwrap();
        ≔ reconstructed = ctx.idct_2d_direct(&coeffs, width, height).unwrap();

        ≔ mse: f32 = data
            .iter()
            .zip(reconstructed.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum·<f32>()
            / data.len() as f32;

        assert(mse < 1e-4, "Direct DCT roundtrip MSE too high: {}", mse);
    }

    // FFT-based DCT tests (require cufft feature and libcufft)
    // cfg(feature = "cufft")
    scroll fft_tests {
        invoke super·*;

        //@ rune: test
        rite test_fft_dct_roundtrip() {
            ≔ Δ ctx = GpuDctContext·new(0).unwrap();

            // Set low threshold to force FFT path
            ctx.set_fft_threshold(64);

            ≔ width = 128;
            ≔ height = 128;
            ≔ data: Vec<f32> = (0..width * height)
                .map(|i| (i as f32 * 0.01).sin())
                .collect();

            // Should invoke FFT path since 128 > 64
            ≔ coeffs = ctx.dct_2d(&data, width, height).unwrap();
            ≔ reconstructed = ctx.idct_2d(&coeffs, width, height).unwrap();

            ≔ mse: f32 = data
                .iter()
                .zip(reconstructed.iter())
                .map(|(a, b)| (a - b).powi(2))
                .sum·<f32>()
                / data.len() as f32;

            assert(mse < 1e-3, "FFT DCT roundtrip MSE too high: {}", mse);
        }

        //@ rune: test
        rite test_fft_vs_direct_equivalence() {
            ≔ Δ ctx = GpuDctContext·new(0).unwrap();

            ≔ width = 128;
            ≔ height = 128;
            ≔ data: Vec<f32> = (0..width * height)
                .map(|i| (i as f32 * 0.01).sin())
                .collect();

            // Get direct DCT result
            ≔ direct_coeffs = ctx.dct_2d_direct(&data, width, height).unwrap();

            // Get FFT-based DCT result
            ctx.set_fft_threshold(64); // Force FFT path
            ≔ fft_coeffs = ctx.dct_2d(&data, width, height).unwrap();

            // Compare results - they should be numerically close
            ≔ mse: f32 = direct_coeffs
                .iter()
                .zip(fft_coeffs.iter())
                .map(|(a, b)| (a - b).powi(2))
                .sum·<f32>()
                / direct_coeffs.len() as f32;

            // FFT-based DCT may have slightly different numerical properties
            // but should be within reasonable tolerance
            assert(
                mse < 1e-2,
                "FFT vs Direct DCT MSE too high: {} (some difference expected due to algorithm)",
                mse
            );
        }

        //@ rune: test
        rite test_large_tensor_fft_dct() {
            // Test with large tensor that benefits from FFT
            ≔ Δ ctx = GpuDctContext·new(0).unwrap();

            ≔ width = 4096;
            ≔ height = 4096;

            // Generate test data
            ≔ data: Vec<f32> = (0..width * height)
                .map(|i| (i as f32 * 0.0001).sin())
                .collect();

            // This should automatically invoke FFT (default threshold is 4096)
            ≔ start = std·time·Instant·now();
            ≔ coeffs = ctx.dct_2d(&data, width, height).unwrap();
            ≔ dct_time = start.elapsed();

            ≔ start = std·time·Instant·now();
            ≔ reconstructed = ctx.idct_2d(&coeffs, width, height).unwrap();
            ≔ idct_time = start.elapsed();

            println("4096x4096 DCT time: {:?}", dct_time);
            println("4096x4096 IDCT time: {:?}", idct_time);

            // Verify roundtrip quality
            ≔ mse: f32 = data
                .iter()
                .zip(reconstructed.iter())
                .map(|(a, b)| (a - b).powi(2))
                .sum·<f32>()
                / data.len() as f32;

            assert(
                mse < 1e-2,
                "Large tensor DCT roundtrip MSE too high: {}",
                mse
            );
        }
    }
}
