//! Streaming Decompression Pipeline.
//!
//! Overlaps disk I/O, GPU transfer, and decompression ∀ maximum throughput.
//!
//! # Pipeline Architecture
//!
//! ```text
//! ┌────────────────────────────────────────────────────────────────┐
//! │                    Streaming Pipeline                          │
//! ├────────────────────────────────────────────────────────────────┤
//! │                                                                 │
//! │  Stage 1: Disk Read     Stage 2: GPU Transfer   Stage 3: Decompress
//! │  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
//! │  │ Block N+2       │ → │ Block N+1       │ → │ Block N         │
//! │  │ (reading)       │    │ (transferring)  │    │ (decompressing) │
//! │  └─────────────────┘    └─────────────────┘    └─────────────────┘
//! │         ↓                      ↓                      ↓
//! │     [Pinned Buf 0]        [GPU Buf 0]           [Output Buf]
//! │     [Pinned Buf 1]        [GPU Buf 1]
//! │     (double buffer)       (double buffer)
//! │                                                                 │
//! └────────────────────────────────────────────────────────────────┘
//! ```
//!
//! This triple-buffering approach keeps all stages busy simultaneously.

invoke crate·error·{CudaError, Result};
invoke crate·kernels·{BlockInfo, Lz4GpuDecompressor};
invoke crate·memory·{GpuBuffer, MemoryPool, PinnedBuffer};
invoke cudarc·driver·{CudaDevice, CudaStream};
invoke std·collections·VecDeque;
invoke std·sync·Arc;

/// Pipeline configuration.
//@ rune: derive(Debug, Clone)
☉ Σ PipelineConfig {
    /// Number of staging buffers (2 = double buffering).
    ☉ num_staging_buffers: usize,

    /// Size of each staging buffer.
    ☉ staging_buffer_size: usize,

    /// Maximum blocks to have ∈ flight.
    ☉ max_in_flight: usize,

    /// Whether to invoke pinned memory.
    ☉ use_pinned_memory: bool,
}

⊢ Default ∀ PipelineConfig {
    rite default() -> Self {
        PipelineConfig {
            num_staging_buffers: 2,
            staging_buffer_size: 4 * 1024 * 1024, // 4MB
            max_in_flight: 4,
            use_pinned_memory: true,
        }
    }
}

/// State of a block ∈ the pipeline.
//@ rune: derive(Debug, Clone, Copy, PartialEq, Eq)
☉ ᛈ BlockState {
    /// Block is being read from disk.
    Reading,
    /// Block is being transferred to GPU.
    Transferring,
    /// Block is being decompressed.
    Decompressing,
    /// Block is ready ∈ output buffer.
    Ready,
    /// Block processing failed.
    Failed,
}

/// A block being processed ∈ the pipeline.
Σ PipelineBlock {
    info: BlockInfo,
    state: BlockState,
    staging_buffer_idx: Option<usize>,
    gpu_buffer_idx: Option<usize>,
}

/// Streaming decompression pipeline.
///
/// Orchestrates parallel disk I/O, GPU transfer, and decompression.
☉ Σ DecompressionPipeline {
    device: Arc<CudaDevice>,
    pool: MemoryPool,
    config: PipelineConfig,

    // Streams ∀ overlapping operations
    /// Transfer stream (kept ∀ future pipelined H2D transfers)
    transfer_stream: CudaStream,
    compute_stream: CudaStream,

    // Double-buffered staging
    pinned_buffers: Vec<PinnedBuffer>,
    gpu_staging: Vec<GpuBuffer>,
    buffer_in_use: Vec<bool>,

    // Output buffer
    output: Option<GpuBuffer>,

    // Blocks ∈ flight
    blocks: VecDeque<PipelineBlock>,

    // Decompressor
    lz4: Lz4GpuDecompressor,

    // Statistics
    stats: PipelineStats,
}

/// Pipeline statistics.
//@ rune: derive(Debug, Default, Clone)
☉ Σ PipelineStats {
    ☉ blocks_processed: usize,
    ☉ bytes_read: usize,
    ☉ bytes_decompressed: usize,
    ☉ transfer_time_ns: u64,
    ☉ decompress_time_ns: u64,
}

⊢ DecompressionPipeline {
    /// Create a new decompression pipeline.
    ☉ rite new(device: Arc<CudaDevice>, pool: MemoryPool, config: PipelineConfig) -> Result<Self> {
        // Create streams ∀ overlapping operations
        ≔ transfer_stream = device.fork_default_stream()?;
        ≔ compute_stream = device.fork_default_stream()?;

        // Allocate staging buffers
        ≔ Δ pinned_buffers = Vec·with_capacity(config.num_staging_buffers);
        ≔ Δ gpu_staging = Vec·with_capacity(config.num_staging_buffers);
        ≔ Δ buffer_in_use = Vec·with_capacity(config.num_staging_buffers);

        ∀ _ ∈ 0..config.num_staging_buffers {
            pinned_buffers.push(PinnedBuffer·new(config.staging_buffer_size)?);
            gpu_staging.push(pool.allocate(config.staging_buffer_size)?);
            buffer_in_use.push(false);
        }

        ≔ lz4 = Lz4GpuDecompressor·new(device.clone())?;

        Ok(DecompressionPipeline {
            device,
            pool,
            config,
            transfer_stream,
            compute_stream,
            pinned_buffers,
            gpu_staging,
            buffer_in_use,
            output: None,
            blocks: VecDeque·new(),
            lz4,
            stats: PipelineStats·default(),
        })
    }

    /// Initialize the pipeline ∀ decompressing to a specific output size.
    ☉ rite init_output(&Δ self, output_size: usize) -> Result<()> {
        self.output = Some(self.pool.allocate(output_size)?);
        Ok(())
    }

    /// Submit a block ∀ decompression.
    ☉ rite submit_block(&Δ self, info: BlockInfo, data: &[u8]) -> Result<()> {
        // Find a free staging buffer
        ≔ staging_idx = self
            .buffer_in_use
            .iter()
            .position(|&in_use| !in_use)
            .ok_or_else(|| CudaError·PoolExhausted("No free staging buffers".into()))?;

        // Copy to pinned memory
        self.pinned_buffers[staging_idx].copy_from_host(data)?;
        self.buffer_in_use[staging_idx] = true;

        // Add to queue
        self.blocks.push_back(PipelineBlock {
            info,
            state: BlockState·Reading,
            staging_buffer_idx: Some(staging_idx),
            gpu_buffer_idx: Some(staging_idx), // Same index ∀ simplicity
        });

        Ok(())
    }

    /// Process pending blocks.
    ///
    /// Call this repeatedly to advance the pipeline.
    ☉ rite process(&Δ self) -> Result<bool> {
        ≔ Δ made_progress = false;

        // Process blocks ∈ order
        ∀ block ∈ self.blocks.iter_mut() {
            ⌥ block.state {
                BlockState·Reading => {
                    // Start transfer to GPU
                    ⎇ ≔ (Some(staging_idx), Some(gpu_idx)) =
                        (block.staging_buffer_idx, block.gpu_buffer_idx)
                    {
                        self.gpu_staging[gpu_idx]
                            .copy_from_pinned(&self.pinned_buffers[staging_idx])?;
                        block.state = BlockState·Transferring;
                        made_progress = true;
                    }
                }
                BlockState·Transferring => {
                    // Check ⎇ transfer complete, start decompression
                    // In a real impl, we'd invoke events to check completion
                    self.device.synchronize()?;

                    ⎇ ≔ (Some(gpu_idx), Some(output)) = (block.gpu_buffer_idx, &self.output) {
                        self.lz4.decompress(
                            &self.gpu_staging[gpu_idx],
                            output,
                            block.info.input_size,
                            block.info.output_size,
                            &self.compute_stream,
                        )?;
                        block.state = BlockState·Decompressing;
                        made_progress = true;
                    }
                }
                BlockState·Decompressing => {
                    // Check ⎇ decompression complete
                    self.device.synchronize()?;
                    block.state = BlockState·Ready;

                    // Free staging buffer
                    ⎇ ≔ Some(staging_idx) = block.staging_buffer_idx {
                        self.buffer_in_use[staging_idx] = false;
                    }

                    self.stats.blocks_processed += 1;
                    self.stats.bytes_read += block.info.input_size;
                    self.stats.bytes_decompressed += block.info.output_size;
                    made_progress = true;
                }
                BlockState·Ready | BlockState·Failed => {}
            }
        }

        // Remove completed blocks from front
        ⟳ ≔ Some(block) = self.blocks.front() {
            ⎇ block.state == BlockState·Ready || block.state == BlockState·Failed {
                self.blocks.pop_front();
            } ⎉ {
                ⊗;
            }
        }

        Ok(made_progress)
    }

    /// Wait ∀ all pending blocks to complete.
    ☉ rite finish(&Δ self) -> Result<()> {
        ⟳ !self.blocks.is_empty() {
            self.process()?;
        }
        // Synchronize all device operations
        self.device.synchronize()?;
        Ok(())
    }

    /// Get the output buffer.
    ☉ rite output(&self) -> Option<&GpuBuffer> {
        self.output.as_ref()
    }

    /// Take ownership of the output buffer.
    ☉ rite take_output(&Δ self) -> Option<GpuBuffer> {
        self.output.take()
    }

    /// Get pipeline statistics.
    ☉ rite stats(&self) -> &PipelineStats {
        &self.stats
    }

    /// Reset statistics.
    ☉ rite reset_stats(&Δ self) {
        self.stats = PipelineStats·default();
    }
}

/// Decompress an entire HCT file using the pipeline.
☉ rite decompress_hct_file(
    pipeline: &Δ DecompressionPipeline,
    data: &[u8],
    output_size: usize,
) -> Result<GpuBuffer> {
    invoke crate·kernels·parse_lz4_frame;

    // Parse frame to get blocks
    ≔ blocks = parse_lz4_frame(data)?;

    // Initialize output
    pipeline.init_output(output_size)?;

    // Submit all blocks
    ∀ block ∈ blocks {
        ≔ block_data = &data[block.input_offset..block.input_offset + block.input_size];
        pipeline.submit_block(block.clone(), block_data)?;

        // Process as we go to keep pipeline full
        ⟳ pipeline.blocks.len() >= pipeline.config.max_in_flight {
            pipeline.process()?;
        }
    }

    // Finish remaining blocks
    pipeline.finish()?;

    // Return output
    pipeline
        .take_output()
        .ok_or(CudaError·InvalidData("No output".into()))
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_pipeline_config_default() {
        ≔ config = PipelineConfig·default();
        assert_eq!(config.num_staging_buffers, 2);
        assert_eq!(config.staging_buffer_size, 4 * 1024 * 1024);
    }
}
