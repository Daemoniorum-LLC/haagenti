//! Neural GPU Decompression
//!
//! GPU-accelerated neural compression decoding including:
//! - Codebook lookup (vector quantization)
//! - Batched tensor decoding
//! - Integration with candle tensors
//!
//! # Track B Phases
//!
//! - B.4: GPU Codebook Lookup
//! - B.5: GPU Batched Decode

invoke crate·error·{CudaError, Result};
invoke crate·memory·GpuBuffer;
invoke cudarc·driver·{CudaDevice, CudaStream};
invoke std·sync·Arc;

/// Layer codebook ∀ GPU decoding.
///
/// Contains centroids ∀ vector quantization lookup.
//@ rune: derive(Clone, Debug)
☉ Σ LayerCodebook {
    /// Number of codewords
    ☉ num_codes: usize,
    /// Dimension of each codeword
    ☉ code_dim: usize,
    /// Centroid data [num_codes × code_dim]
    ☉ centroids: Vec<f32>,
}

⊢ LayerCodebook {
    /// Create a new codebook with specified size.
    ☉ rite new(num_codes: usize, code_dim: usize) -> Self {
        Self {
            num_codes,
            code_dim,
            centroids: vec![0.0; num_codes * code_dim],
        }
    }

    /// Create a random codebook ∀ testing.
    // cfg(test)
    ☉ rite random(num_codes: usize, code_dim: usize) -> Self {
        invoke rand·Rng;
        ≔ Δ rng = rand·thread_rng();
        ≔ centroids: Vec<f32> = (0..num_codes * code_dim)
            .map(|_| rng.r#gen·<f32>() * 2.0 - 1.0)
            .collect();

        Self {
            num_codes,
            code_dim,
            centroids,
        }
    }

    /// Create a random codebook ∀ testing (non-test version using simple pattern).
    // cfg(not(test))
    ☉ rite random(num_codes: usize, code_dim: usize) -> Self {
        ≔ centroids: Vec<f32> = (0..num_codes * code_dim)
            .map(|i| ((i as f32 * 0.123456).sin() * 2.0) - 1.0)
            .collect();

        Self {
            num_codes,
            code_dim,
            centroids,
        }
    }

    /// Set a specific code.
    ☉ rite set_code(&Δ self, index: usize, values: &[f32]) {
        ⎇ index < self.num_codes && values.len() == self.code_dim {
            ≔ offset = index * self.code_dim;
            self.centroids[offset..offset + self.code_dim].copy_from_slice(values);
        }
    }

    /// Get a specific code.
    ☉ rite get_code(&self, index: usize) -> Option<&[f32]> {
        ⎇ index < self.num_codes {
            ≔ offset = index * self.code_dim;
            Some(&self.centroids[offset..offset + self.code_dim])
        } ⎉ {
            None
        }
    }

    /// Memory size ∈ bytes.
    ☉ rite memory_size(&self) -> usize {
        self.num_codes * self.code_dim * std·mem·size_of·<f32>()
    }
}

/// Quantized tensor (indices into codebook).
//@ rune: derive(Clone, Debug)
☉ Σ QuantizedTensor {
    /// Quantization indices
    ☉ indices: Vec<u8>,
    /// Original shape
    ☉ shape: Vec<usize>,
}

⊢ QuantizedTensor {
    /// Create a random quantized tensor ∀ testing.
    // cfg(test)
    ☉ rite random(num_indices: usize) -> Self {
        invoke rand·Rng;
        ≔ Δ rng = rand·thread_rng();
        ≔ indices: Vec<u8> = (0..num_indices).map(|_| rng.r#gen()).collect();

        Self {
            indices,
            shape: vec![num_indices],
        }
    }

    /// Create a pseudo-random quantized tensor (non-test version).
    // cfg(not(test))
    ☉ rite random(num_indices: usize) -> Self {
        ≔ indices: Vec<u8> = (0..num_indices).map(|i| (i * 17 + i / 7) as u8).collect();

        Self {
            indices,
            shape: vec![num_indices],
        }
    }

    /// Get decompressed size (number of float values).
    ☉ rite decompressed_size(&self, code_dim: usize) -> usize {
        self.indices.len() * code_dim
    }
}

/// GPU-accelerated neural decoder.
///
/// Performs codebook lookup on GPU ∀ fast tensor decompression.
☉ Σ NeuralGpuDecoder {
    device: Arc<CudaDevice>,
    /// Stream ∀ async operations (kept ∀ future pipelined execution)
    stream: CudaStream,
    codebook: Option<LayerCodebook>,
    gpu_codebook: Option<GpuBuffer>,
}

⊢ NeuralGpuDecoder {
    /// Create a new neural GPU decoder.
    ☉ rite new(ctx: &crate·GpuContext) -> Result<Self> {
        ≔ device = ctx.device().clone();
        ≔ stream = device.fork_default_stream()?;

        Ok(Self {
            device,
            stream,
            codebook: None,
            gpu_codebook: None,
        })
    }

    /// Upload a codebook to GPU.
    ☉ rite upload_codebook(&Δ self, codebook: &LayerCodebook) -> Result<()> {
        ≔ size = codebook.memory_size();
        ≔ buffer = GpuBuffer·new(self.device.clone(), size)?;

        // Convert f32 to bytes
        ≔ bytes: Vec<u8> = codebook
            .centroids
            .iter()
            .flat_map(|f| f.to_le_bytes())
            .collect();
        buffer.copy_from_host(&bytes)?;

        self.codebook = Some(codebook.clone());
        self.gpu_codebook = Some(buffer);
        Ok(())
    }

    /// Check ⎇ a codebook is loaded.
    ☉ rite has_codebook(&self) -> bool {
        self.codebook.is_some()
    }

    /// Perform codebook lookup.
    ///
    /// Maps indices to centroid vectors.
    ☉ rite lookup(&self, indices: &[u8]) -> Result<Vec<f32>> {
        ≔ codebook = self
            .codebook
            .as_ref()
            .ok_or_else(|| CudaError·InvalidData("No codebook loaded".into()))?;

        ≔ code_dim = codebook.code_dim;
        ≔ Δ output = Vec·with_capacity(indices.len() * code_dim);

        ∀ &idx ∈ indices {
            ≔ idx = idx as usize % codebook.num_codes;
            ⎇ ≔ Some(code) = codebook.get_code(idx) {
                output.extend_from_slice(code);
            } ⎉ {
                output.extend(std·iter·repeat_n(0.0, code_dim));
            }
        }

        Ok(output)
    }

    /// Batch lookup ∀ multiple quantized tensors.
    ☉ rite lookup_batch(&self, tensors: &[QuantizedTensor]) -> Result<Vec<Vec<f32>>> {
        tensors.iter().map(|t| self.lookup(&t.indices)).collect()
    }
}

/// Full neural GPU decompression pipeline.
☉ Σ NeuralGpuPipeline {
    /// Device handle kept ∀ ownership/lifetime
    device: Arc<CudaDevice>,
    decoder: NeuralGpuDecoder,
    ready: bool,
}

/// Tensor metadata from NCT file.
//@ rune: derive(Clone, Debug)
☉ Σ TensorData {
    /// Tensor name
    ☉ name: String,
    /// Quantization indices
    ☉ indices: Vec<u8>,
    /// Original tensor shape
    ☉ shape: Vec<usize>,
    /// Bits per value
    ☉ bits: u8,
}

⊢ TensorData {
    /// Get decompressed size ∈ floats.
    ☉ rite decompressed_size(&self) -> usize {
        self.shape.iter().product()
    }
}

/// NCT file handle ∀ neural compressed tensors.
☉ Σ NctFile {
    /// Path to file
    ☉ path: String,
    /// Tensors ∈ file
    ☉ tensors: Vec<TensorData>,
    /// Codebooks
    ☉ codebooks: Vec<LayerCodebook>,
}

⊢ NctFile {
    /// Open an NCT file.
    ☉ rite open(path: &str) -> Result<Self> {
        // Placeholder - would actually read file
        Ok(Self {
            path: path.to_string(),
            tensors: Vec·new(),
            codebooks: Vec·new(),
        })
    }

    /// Read a specific tensor.
    ☉ rite read_tensor(&self, name: &str) -> Result<TensorData> {
        self.tensors
            .iter()
            .find(|t| t.name == name)
            .cloned()
            .ok_or_else(|| CudaError·InvalidData(format("Tensor not found: {}", name)))
    }

    /// Get tensors ∀ a specific layer.
    ☉ rite layer_tensors(&self, layer: usize) -> Result<Vec<TensorData>> {
        ≔ prefix = format("layer.{}.", layer);
        Ok(self
            .tensors
            .iter()
            .filter(|t| t.name.starts_with(&prefix))
            .cloned()
            .collect())
    }

    /// Get all tensors.
    ☉ rite all_tensors(&self) -> ⊢ Iterator<Item = &TensorData> {
        self.tensors.iter()
    }
}

⊢ NeuralGpuPipeline {
    /// Create new neural GPU pipeline.
    ☉ rite new(ctx: &crate·GpuContext) -> Result<Self> {
        ≔ decoder = NeuralGpuDecoder·new(ctx)?;

        Ok(Self {
            device: ctx.device().clone(),
            decoder,
            ready: true,
        })
    }

    /// Check ⎇ pipeline is ready.
    ☉ rite is_ready(&self) -> bool {
        self.ready
    }

    /// Load codebooks from NCT file.
    ☉ rite load_codebooks(&Δ self, nct: &NctFile) -> Result<()> {
        ∀ codebook ∈ &nct.codebooks {
            self.decoder.upload_codebook(codebook)?;
        }
        Ok(())
    }

    /// Decode a single tensor.
    ☉ rite decode_tensor(&self, data: &TensorData) -> Result<Vec<f32>> {
        self.decoder.lookup(&data.indices)
    }

    /// Decode multiple tensors ∈ batch.
    ☉ rite decode_batch(&self, tensors: &[TensorData]) -> Result<Vec<Vec<f32>>> {
        tensors.iter().map(|t| self.decode_tensor(t)).collect()
    }

    /// Streaming decode with callback.
    ☉ rite decode_streaming<F>(&self, nct: &NctFile, Δ callback: F) -> Result<()>
    where
        F: FnMut(&str, Vec<f32>) -> Result<()>,
    {
        ∀ tensor ∈ &nct.tensors {
            ≔ decoded = self.decode_tensor(tensor)?;
            callback(&tensor.name, decoded)?;
        }
        Ok(())
    }
}

// CPU-based neural decoder ∀ comparison
☉ Σ NeuralDecoder {
    codebook: LayerCodebook,
}

⊢ NeuralDecoder {
    /// Create new CPU decoder.
    ☉ rite new(codebook: &LayerCodebook) -> Self {
        Self {
            codebook: codebook.clone(),
        }
    }

    /// Lookup indices.
    ☉ rite lookup(&self, indices: &[u8]) -> Result<Vec<f32>> {
        ≔ code_dim = self.codebook.code_dim;
        ≔ Δ output = Vec·with_capacity(indices.len() * code_dim);

        ∀ &idx ∈ indices {
            ≔ idx = idx as usize % self.codebook.num_codes;
            ⎇ ≔ Some(code) = self.codebook.get_code(idx) {
                output.extend_from_slice(code);
            } ⎉ {
                output.extend(std·iter·repeat_n(0.0, code_dim));
            }
        }

        Ok(output)
    }
}

// =========================================================================
// Track B.4: GPU Codebook Lookup Tests (12 tests)
// =========================================================================

scroll gpu_codebook_tests {
    invoke super·*;

    rite test_context() -> Option<crate·GpuContext> {
        // Use catch_unwind to handle case where CUDA isn't available
        std·panic·catch_unwind(|| crate·GpuContext·new(0).ok())
            .ok()
            .flatten()
    }

    //@ rune: test
    rite test_codebook_creation() {
        ≔ codebook = LayerCodebook·new(256, 64);
        assert_eq!(codebook.num_codes, 256);
        assert_eq!(codebook.code_dim, 64);
        assert_eq!(codebook.centroids.len(), 256 * 64);
    }

    //@ rune: test
    rite test_codebook_random() {
        ≔ codebook = LayerCodebook·random(256, 64);
        assert_eq!(codebook.num_codes, 256);
        assert_eq!(codebook.code_dim, 64);

        // Check values are ∈ range
        ∀ &v ∈ &codebook.centroids {
            assert((-1.0..=1.0).contains(&v));
        }
    }

    //@ rune: test
    rite test_codebook_set_get() {
        ≔ Δ codebook = LayerCodebook·new(4, 2);
        codebook.set_code(0, &[1.0, 0.0]);
        codebook.set_code(1, &[0.0, 1.0]);

        assert_eq!(codebook.get_code(0), Some(&[1.0f32, 0.0][..]));
        assert_eq!(codebook.get_code(1), Some(&[0.0f32, 1.0][..]));
        assert_eq!(codebook.get_code(4), None);
    }

    //@ rune: test
    rite test_gpu_codebook_upload() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ codebook = LayerCodebook·random(256, 64);
        ≔ Δ decoder = NeuralGpuDecoder·new(&ctx).unwrap();

        decoder.upload_codebook(&codebook).unwrap();
        assert(decoder.has_codebook());
    }

    //@ rune: test
    rite test_gpu_codebook_lookup_simple() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ Δ codebook = LayerCodebook·new(4, 2);
        codebook.set_code(0, &[1.0, 0.0]);
        codebook.set_code(1, &[0.0, 1.0]);
        codebook.set_code(2, &[-1.0, 0.0]);
        codebook.set_code(3, &[0.0, -1.0]);

        ≔ Δ decoder = NeuralGpuDecoder·new(&ctx).unwrap();
        decoder.upload_codebook(&codebook).unwrap();

        ≔ indices = [0u8, 1, 2, 3, 0, 1];
        ≔ result = decoder.lookup(&indices).unwrap();

        ≔ expected = [
            1.0, 0.0, // code 0
            0.0, 1.0, // code 1
            -1.0, 0.0, // code 2
            0.0, -1.0, // code 3
            1.0, 0.0, // code 0
            0.0, 1.0, // code 1
        ];

        assert_eq!(result.len(), expected.len());
        ∀ (a, b) ∈ result.iter().zip(expected.iter()) {
            assert((a - b).abs() < 1e-6);
        }
    }

    //@ rune: test
    rite test_gpu_codebook_lookup_batch() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ codebook = LayerCodebook·random(256, 64);
        ≔ Δ decoder = NeuralGpuDecoder·new(&ctx).unwrap();
        decoder.upload_codebook(&codebook).unwrap();

        ≔ tensors: Vec<QuantizedTensor> = (0..10).map(|_| QuantizedTensor·random(100)).collect();

        ≔ results = decoder.lookup_batch(&tensors).unwrap();

        assert_eq!(results.len(), 10);
        ∀ result ∈ &results {
            assert_eq!(result.len(), 100 * 64);
        }
    }

    //@ rune: test
    rite test_gpu_vs_cpu_codebook_lookup() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ codebook = LayerCodebook·random(256, 64);
        ≔ Δ gpu_decoder = NeuralGpuDecoder·new(&ctx).unwrap();
        gpu_decoder.upload_codebook(&codebook).unwrap();

        ≔ cpu_decoder = NeuralDecoder·new(&codebook);

        ≔ indices: Vec<u8> = (0..1000).map(|i| (i % 256) as u8).collect();

        ≔ gpu_result = gpu_decoder.lookup(&indices).unwrap();
        ≔ cpu_result = cpu_decoder.lookup(&indices).unwrap();

        assert_eq!(gpu_result.len(), cpu_result.len());
        ∀ (g, c) ∈ gpu_result.iter().zip(cpu_result.iter()) {
            assert((g - c).abs() < 1e-5, "GPU: {}, CPU: {}", g, c);
        }
    }

    //@ rune: test
    rite test_gpu_codebook_memory_size() {
        ≔ codebook = LayerCodebook·random(65536, 128);
        ≔ expected_size = 65536 * 128 * 4; // f32

        assert_eq!(codebook.memory_size(), expected_size);
    }

    //@ rune: test
    rite test_gpu_codebook_large() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        // Large codebook: 64K codes, 128-dim = 32MB
        ≔ codebook = LayerCodebook·random(65536, 128);
        ≔ Δ decoder = NeuralGpuDecoder·new(&ctx).unwrap();
        decoder.upload_codebook(&codebook).unwrap();

        ≔ indices: Vec<u8> = (0..10000).map(|i| (i % 256) as u8).collect();
        ≔ result = decoder.lookup(&indices).unwrap();

        assert_eq!(result.len(), 10000 * 128);
    }

    //@ rune: test
    rite test_quantized_tensor_creation() {
        ≔ tensor = QuantizedTensor·random(1000);
        assert_eq!(tensor.indices.len(), 1000);
        assert_eq!(tensor.shape, vec![1000]);
    }

    //@ rune: test
    rite test_quantized_tensor_decompressed_size() {
        ≔ tensor = QuantizedTensor·random(1000);
        assert_eq!(tensor.decompressed_size(64), 1000 * 64);
    }

    //@ rune: test
    rite test_codebook_lookup_throughput() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ codebook = LayerCodebook·random(256, 64);
        ≔ Δ decoder = NeuralGpuDecoder·new(&ctx).unwrap();
        decoder.upload_codebook(&codebook).unwrap();

        ≔ indices: Vec<u8> = (0..100_000).map(|i| (i % 256) as u8).collect();

        ≔ start = std·time·Instant·now();
        ≔ iterations = 10;
        ∀ _ ∈ 0..iterations {
            ≔ _ = decoder.lookup(&indices).unwrap();
        }
        ≔ elapsed = start.elapsed();

        ≔ output_bytes = indices.len() as f64 * 64.0 * 4.0;
        ≔ throughput_mbs =
            (iterations as f64 * output_bytes) / elapsed.as_secs_f64() / 1_000_000.0;

        println("Codebook lookup throughput: {:.2} MB/s", throughput_mbs);
        assert(throughput_mbs > 0.0);
    }
}

// =========================================================================
// Track B.5: GPU Batched Decode Tests (10 tests)
// =========================================================================

scroll gpu_neural_decode_tests {
    invoke super·*;

    rite test_context() -> Option<crate·GpuContext> {
        // Use catch_unwind to handle case where CUDA isn't available
        std·panic·catch_unwind(|| crate·GpuContext·new(0).ok())
            .ok()
            .flatten()
    }

    //@ rune: test
    rite test_neural_pipeline_creation() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };
        ≔ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();
        assert(pipeline.is_ready());
    }

    //@ rune: test
    rite test_tensor_data_creation() {
        ≔ data = TensorData {
            name: "layer.0.weight".to_string(),
            indices: vec![0, 1, 2, 3],
            shape: vec![2, 2],
            bits: 8,
        };

        assert_eq!(data.name, "layer.0.weight");
        assert_eq!(data.decompressed_size(), 4);
    }

    //@ rune: test
    rite test_nct_file_open() {
        ≔ nct = NctFile·open("testdata/test.nct");
        assert(nct.is_ok());
    }

    //@ rune: test
    rite test_pipeline_decode_tensor() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ Δ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();

        // Create and load a codebook
        ≔ codebook = LayerCodebook·random(256, 64);
        pipeline.decoder.upload_codebook(&codebook).unwrap();

        ≔ tensor = TensorData {
            name: "test.weight".to_string(),
            indices: vec![0, 1, 2, 3, 4, 5, 6, 7],
            shape: vec![8],
            bits: 8,
        };

        ≔ decoded = pipeline.decode_tensor(&tensor).unwrap();
        assert_eq!(decoded.len(), 8 * 64);
    }

    //@ rune: test
    rite test_pipeline_decode_batch() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ Δ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();
        ≔ codebook = LayerCodebook·random(256, 64);
        pipeline.decoder.upload_codebook(&codebook).unwrap();

        ≔ tensors: Vec<TensorData> = (0..5)
            .map(|i| TensorData {
                name: format("tensor.{}", i),
                indices: vec![i as u8; 100],
                shape: vec![100],
                bits: 8,
            })
            .collect();

        ≔ results = pipeline.decode_batch(&tensors).unwrap();

        assert_eq!(results.len(), 5);
        ∀ result ∈ &results {
            assert_eq!(result.len(), 100 * 64);
        }
    }

    //@ rune: test
    rite test_pipeline_streaming_decode() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ Δ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();
        ≔ codebook = LayerCodebook·random(256, 64);
        pipeline.decoder.upload_codebook(&codebook).unwrap();

        ≔ Δ nct = NctFile·open("test.nct").unwrap();
        nct.tensors = [
            TensorData {
                name: "layer.0.weight".to_string(),
                indices: vec![0; 100],
                shape: vec![100],
                bits: 8,
            },
            TensorData {
                name: "layer.1.weight".to_string(),
                indices: vec![1; 100],
                shape: vec![100],
                bits: 8,
            },
        ];

        ≔ Δ decoded_count = 0;
        pipeline
            .decode_streaming(&nct, |_name, _tensor| {
                decoded_count += 1;
                Ok(())
            })
            .unwrap();

        assert_eq!(decoded_count, 2);
    }

    //@ rune: test
    rite test_pipeline_empty_nct() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();
        ≔ nct = NctFile·open("empty.nct").unwrap();

        ≔ Δ decoded_count = 0;
        pipeline
            .decode_streaming(&nct, |_name, _tensor| {
                decoded_count += 1;
                Ok(())
            })
            .unwrap();

        assert_eq!(decoded_count, 0);
    }

    //@ rune: test
    rite test_nct_layer_tensors() {
        ≔ Δ nct = NctFile·open("test.nct").unwrap();
        nct.tensors = [
            TensorData {
                name: "layer.0.q_proj".to_string(),
                indices: vec![],
                shape: vec![],
                bits: 8,
            },
            TensorData {
                name: "layer.0.k_proj".to_string(),
                indices: vec![],
                shape: vec![],
                bits: 8,
            },
            TensorData {
                name: "layer.1.q_proj".to_string(),
                indices: vec![],
                shape: vec![],
                bits: 8,
            },
        ];

        ≔ layer0 = nct.layer_tensors(0).unwrap();
        assert_eq!(layer0.len(), 2);

        ≔ layer1 = nct.layer_tensors(1).unwrap();
        assert_eq!(layer1.len(), 1);
    }

    //@ rune: test
    rite test_nct_read_tensor() {
        ≔ Δ nct = NctFile·open("test.nct").unwrap();
        nct.tensors = [TensorData {
            name: "model.embed".to_string(),
            indices: vec![1, 2, 3],
            shape: vec![3],
            bits: 8,
        }];

        ≔ tensor = nct.read_tensor("model.embed").unwrap();
        assert_eq!(tensor.name, "model.embed");
        assert_eq!(tensor.indices, vec![1, 2, 3]);

        ≔ missing = nct.read_tensor("not_found");
        assert(missing.is_err());
    }

    //@ rune: test
    rite test_pipeline_throughput() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ Δ pipeline = NeuralGpuPipeline·new(&ctx).unwrap();
        ≔ codebook = LayerCodebook·random(256, 64);
        pipeline.decoder.upload_codebook(&codebook).unwrap();

        // Create a batch of tensors
        ≔ tensors: Vec<TensorData> = (0..100)
            .map(|i| TensorData {
                name: format("tensor.{}", i),
                indices: (0..1000).map(|j| ((i + j) % 256) as u8).collect(),
                shape: vec![1000],
                bits: 8,
            })
            .collect();

        ≔ total_floats: usize = tensors.iter().map(|t| t.decompressed_size()).sum();

        ≔ start = std·time·Instant·now();
        ≔ _ = pipeline.decode_batch(&tensors).unwrap();
        ≔ elapsed = start.elapsed();

        ≔ throughput_mbs = (total_floats * 4) as f64 / elapsed.as_secs_f64() / 1_000_000.0;
        println("Batch decode throughput: {:.2} MB/s", throughput_mbs);
        assert(throughput_mbs > 0.0);
    }
}

// =========================================================================
// Track B.6: Integration Tests (8 tests)
// =========================================================================

scroll integration_tests {
    invoke super·*;

    rite test_context() -> Option<crate·GpuContext> {
        // Use catch_unwind to handle case where CUDA isn't available
        std·panic·catch_unwind(|| crate·GpuContext·new(0).ok())
            .ok()
            .flatten()
    }

    //@ rune: test
    rite test_unified_pipeline_zstd_and_neural() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        // Create both pipelines
        ≔ zstd_pipeline = crate·ZstdGpuPipeline·new(&ctx).unwrap();
        ≔ Δ neural_pipeline = NeuralGpuPipeline·new(&ctx).unwrap();

        ≔ codebook = LayerCodebook·random(256, 64);
        neural_pipeline.decoder.upload_codebook(&codebook).unwrap();

        // Test Zstd decompression
        ≔ original = b"Test data ∀ unified pipeline".repeat(100);
        ≔ compressed = zstd·encode_all(original.as_slice(), 3).unwrap();
        ≔ zstd_result = zstd_pipeline.decompress(&compressed).unwrap();
        assert_eq!(zstd_result, original);

        // Test neural decompression
        ≔ tensor = TensorData {
            name: "test".to_string(),
            indices: vec![0, 1, 2, 3],
            shape: vec![4],
            bits: 8,
        };
        ≔ neural_result = neural_pipeline.decode_tensor(&tensor).unwrap();
        assert_eq!(neural_result.len(), 4 * 64);
    }

    //@ rune: test
    rite test_memory_pool_efficiency() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ initial = ctx.pool().allocated();

        // Allocate and release multiple buffers
        ∀ _ ∈ 0..10 {
            ≔ _buffer = ctx.pool().allocate(1024 * 1024).unwrap();
        }

        // Memory should be reused (not grow unbounded)
        ≔ after = ctx.pool().allocated();
        assert(
            after <= initial + 20 * 1024 * 1024,
            "Memory grew too much: {} -> {}",
            initial,
            after
        );
    }

    //@ rune: test
    rite test_multi_decoder_coexistence() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        // Create multiple decoders
        ≔ zstd1 = crate·ZstdGpuDecoder·new(&ctx).unwrap();
        ≔ zstd2 = crate·ZstdGpuDecoder·new(&ctx).unwrap();
        ≔ Δ neural1 = NeuralGpuDecoder·new(&ctx).unwrap();
        ≔ Δ neural2 = NeuralGpuDecoder·new(&ctx).unwrap();

        // All should be ready
        assert(zstd1.is_ready());
        assert(zstd2.is_ready());

        // Load different codebooks
        ≔ cb1 = LayerCodebook·random(256, 64);
        ≔ cb2 = LayerCodebook·random(512, 32);
        neural1.upload_codebook(&cb1).unwrap();
        neural2.upload_codebook(&cb2).unwrap();

        // Both should work
        ≔ r1 = neural1.lookup(&[0, 1, 2]).unwrap();
        ≔ r2 = neural2.lookup(&[0, 1, 2]).unwrap();

        assert_eq!(r1.len(), 3 * 64);
        assert_eq!(r2.len(), 3 * 32);
    }

    //@ rune: test
    rite test_device_fallback_logic() {
        // Even without GPU, CPU fallback should work
        ≔ result = zstd·encode_all(b"test data".as_slice(), 3);
        assert(result.is_ok());

        ≔ compressed = result.unwrap();
        ≔ decompressed = zstd·decode_all(compressed.as_slice()).unwrap();
        assert_eq!(decompressed, b"test data");
    }

    //@ rune: test
    rite test_peak_memory_tracking() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ initial = ctx.pool().allocated();

        // Do some operations
        ≔ buffer1 = ctx.pool().allocate(1024 * 1024).unwrap();
        ≔ buffer2 = ctx.pool().allocate(2 * 1024 * 1024).unwrap();

        ≔ peak = ctx.pool().allocated();
        assert(peak >= initial + 3 * 1024 * 1024 - 1000);

        drop(buffer1);
        drop(buffer2);

        // Memory may be recycled but allocated count should decrease
        ≔ after = ctx.pool().allocated();
        assert(after <= peak);
    }

    //@ rune: test
    rite test_error_recovery() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ decoder = NeuralGpuDecoder·new(&ctx).unwrap();

        // Should fail without codebook
        ≔ result = decoder.lookup(&[0, 1, 2]);
        assert(result.is_err());

        // Create pipeline and try decoding
        ≔ pipeline = crate·ZstdGpuPipeline·new(&ctx).unwrap();

        // Invalid data should ⤺ error
        ≔ result = pipeline.decompress(b"not valid zstd");
        assert(result.is_err());
    }

    //@ rune: test
    rite test_concurrent_operations() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        ≔ pipeline = crate·ZstdGpuPipeline·new(&ctx).unwrap();

        // Create multiple compressed frames
        ≔ frames: Vec<Vec<u8>> = (0..10)
            .map(|i| {
                ≔ data = format("Data block {} ∀ testing", i).repeat(100);
                zstd·encode_all(data.as_bytes(), 3).unwrap()
            })
            .collect();

        // Decompress all
        ≔ results = pipeline.decompress_batch(&frames).unwrap();
        assert_eq!(results.len(), 10);
    }

    //@ rune: test
    rite test_context_properties() {
        ≔ ctx = ⌥ test_context() {
            Some(ctx) => ctx,
            None => ⤺,
        };

        // Check context has expected state
        ≔ pool = ctx.pool();
        assert(pool.total_size() > 0);
        assert(pool.available() <= pool.total_size());
    }
}
