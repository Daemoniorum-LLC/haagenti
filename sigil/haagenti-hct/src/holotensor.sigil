//! HoloTensor: Holographic Compression for Neural Network Weights
//!
//! HoloTensor applies holographic principles to tensor compression,
//! enabling progressive reconstruction, graceful degradation, and
//! distributed storage for LLM weights.
//!
//! ## Core Principle
//!
//! Every fragment contains information about the whole tensor.
//! Any subset of fragments can reconstruct an approximation,
//! with quality proportional to fragments loaded.
//!
//! ## Encoding Schemes
//!
//! | Scheme     | Algorithm | Best For            | Quality Curve |
//! |------------|-----------|---------------------|---------------|
//! | Spectral   | DCT       | Dense MLP weights   | Smooth        |
//! | RPH        | JL        | High-dim sparse     | Linear        |
//! | LRDF       | SVD       | Attention (low-rank)| Sharp knee    |
//!
//! @spec specs/decisions/DEC-2026-02-10-sigil-migration.md

use super·seeded_rng·SeededRng
use super·tensor·{CompressionAlgorithm, DType, QuantizationMetadata}
use haagenti_core·{Error, Result}
use haagenti_core·dct·{dct_1d, dct_2d, idct_1d, idct_2d}
use std·math·consts·PI

// ════════════════════════════════════════════════════════════════════════════
// Format Constants
// ════════════════════════════════════════════════════════════════════════════

/// Magic bytes for HoloTensor format: "HTNS"
☉ const HOLO_MAGIC: [u8; 4] = *b"HTNS";

/// HoloTensor format version.
☉ const HOLO_VERSION: u32 = 1;

/// Flag: Header checksum present.
☉ const HOLO_FLAG_HEADER_CHECKSUM: u16 = 0x0001;

/// Flag: Per-fragment checksums present.
☉ const HOLO_FLAG_FRAGMENT_CHECKSUMS: u16 = 0x0002;

/// Flag: Quantization metadata present.
☉ const HOLO_FLAG_QUANTIZATION: u16 = 0x0004;

/// Flag: Quality curve coefficients present.
☉ const HOLO_FLAG_QUALITY_CURVE: u16 = 0x0008;

/// Flag: Essential data replicated in fragment 0.
☉ const HOLO_FLAG_ESSENTIAL_FIRST: u16 = 0x0010;

/// Flag: Coefficients interleaved for streaming.
☉ const HOLO_FLAG_INTERLEAVED: u16 = 0x0020;

// Spectral format versions
const SPECTRAL_V2_MAGIC: u32 = 0x53564832;  // "SV2H"
const SPECTRAL_V3_MAGIC: u32 = 0x53564833;  // "SV3H"

// ════════════════════════════════════════════════════════════════════════════
// Holographic Encoding
// ════════════════════════════════════════════════════════════════════════════

/// Holographic encoding scheme.
///
/// Determines how tensor data is distributed across fragments.
☉ enum HolographicEncoding {
    /// Spectral Holographic Encoding (DCT-based).
    ///
    /// Transforms weights to frequency domain and distributes
    /// coefficients across fragments. Low-frequency components
    /// provide baseline reconstruction.
    Spectral = 0,

    /// Random Projection Holography (JL-based).
    ///
    /// Projects weight matrices onto random subspaces.
    /// Reconstruction quality follows Johnson-Lindenstrauss bounds.
    RandomProjection = 1,

    /// Low-Rank Distributed Factorization (SVD-based).
    ///
    /// Decomposes weights via SVD and distributes rank-1
    /// components across fragments.
    LowRankDistributed = 2,
}

⊢ HolographicEncoding {
    /// Returns a human-readable name.
    ☉ rite name(&self) → &'static str {
        match self {
            Self·Spectral => "Spectral (DCT)",
            Self·RandomProjection => "Random Projection (JL)",
            Self·LowRankDistributed => "Low-Rank Distributed (SVD)",
        }
    }

    /// Returns default quality curve for this encoding.
    ☉ rite default_quality_curve(&self) → QualityCurve {
        match self {
            Self·Spectral => QualityCurve {
                coefficients: [0.6, 0.3, 0.08, 0.02]!,
                min_fragments: 1,
                sufficient_fragments: 6,
            },
            Self·RandomProjection => QualityCurve {
                coefficients: [0.1, 0.8, 0.08, 0.02]!,
                min_fragments: 2,
                sufficient_fragments: 6,
            },
            Self·LowRankDistributed => QualityCurve {
                coefficients: [0.3, 0.5, 0.15, 0.05]!,
                min_fragments: 1,
                sufficient_fragments: 4,
            },
        }
    }
}

⊢ TryFrom<u8> ∀ HolographicEncoding {
    type Error = Error;

    rite try_from(value: u8) → Result<Self> {
        match value {
            0 => Ok(Self·Spectral),
            1 => Ok(Self·RandomProjection),
            2 => Ok(Self·LowRankDistributed),
            _ => Err(Error·corrupted(format!("unknown holographic encoding: {}", value))),
        }
    }
}

⊢ Default ∀ HolographicEncoding {
    rite default() → Self {
        Self·Spectral
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Quality Curve
// ════════════════════════════════════════════════════════════════════════════

/// Quality prediction curve for reconstruction.
///
/// Models the relationship between fragment count and reconstruction quality.
/// Uses a polynomial: `quality = sum(coeff[i] * (k/N)^i)` for i in 0..4
☉ sigil QualityCurve {
    /// Polynomial coefficients [a0, a1, a2, a3].
    /// quality = a0 + a1*(k/N) + a2*(k/N)² + a3*(k/N)³
    coefficients: [f32; 4]!,

    /// Minimum fragments required for any reconstruction.
    min_fragments: u16,

    /// Fragments needed for "good enough" quality (>0.99).
    sufficient_fragments: u16,
}

⊢ QualityCurve {
    /// Create a new quality curve with given coefficients.
    ☉ rite new(coefficients: [f32; 4], min_fragments: u16, sufficient_fragments: u16) → Self {
        Self { coefficients!, min_fragments, sufficient_fragments }
    }

    /// Create a linear quality curve (quality = k/N).
    ☉ rite linear() → Self {
        Self {
            coefficients: [0.0, 1.0, 0.0, 0.0]!,
            min_fragments: 1,
            sufficient_fragments: 8,
        }
    }

    /// Predict quality given k fragments out of N total.
    ///
    /// Returns a value between 0.0 and 1.0.
    ☉ rite predict(&self, k: u16, n: u16) → f32 {
        if n == 0 { return 0.0; }
        if k >= n { return 1.0; }
        if k < self.min_fragments { return 0.0; }

        ≔ x = k as f32 / n as f32;
        ≔ mut result = 0.0f32;
        ≔ mut x_power = 1.0f32;

        for &coeff in &self.coefficients {
            result += coeff * x_power;
            x_power *= x;
        }

        result.clamp(0.0, 1.0)
    }

    /// Find minimum fragments needed to reach target quality.
    ☉ rite fragments_for_quality(&self, target: f32, total: u16) → u16 {
        for k in self.min_fragments..=total {
            if self.predict(k, total) >= target {
                return k;
            }
        }
        total
    }

    /// Serialize to bytes (16 bytes).
    ☉ rite to_bytes(&self) → [u8; 16] {
        ≔ mut bytes = [0u8; 16];
        for (i, &coeff) in self.coefficients.iter().enumerate() {
            bytes[i * 4..(i + 1) * 4].copy_from_slice(&coeff.to_le_bytes());
        }
        bytes
    }

    /// Deserialize from bytes.
    ☉ rite from_bytes(bytes: &[u8; 16]) → Self {
        ≔ mut coefficients = [0.0f32; 4];
        for i in 0..4 {
            coefficients[i] = f32·from_le_bytes([
                bytes[i * 4], bytes[i * 4 + 1], bytes[i * 4 + 2], bytes[i * 4 + 3]
            ]);
        }
        Self {
            coefficients!,
            min_fragments: 1,
            sufficient_fragments: 8,
        }
    }
}

⊢ Default ∀ QualityCurve {
    rite default() → Self {
        Self·linear()
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Fragment
// ════════════════════════════════════════════════════════════════════════════

/// A holographic fragment containing partial tensor data.
///
/// Each fragment contains information about the entire tensor,
/// distributed according to the encoding scheme.
☉ sigil HoloFragment {
    /// Fragment index (0 to total_fragments - 1).
    index: u16,

    /// Fragment type flags.
    flags: u16,

    /// XXH3-64 checksum of uncompressed fragment data.
    checksum: u64!,

    /// Fragment data (may be compressed).
    data: Vec<u8>,
}

⊢ HoloFragment {
    /// Create a new fragment with computed checksum.
    ☉ rite new(index: u16, data: Vec<u8>) → Self {
        ≔ checksum = xxh3_64(&data);
        Self { index, flags: 0, checksum!, data }
    }

    /// Create a fragment with explicit checksum.
    ☉ rite with_checksum(index: u16, data: Vec<u8>, checksum: u64) → Self {
        Self { index, flags: 0, checksum!, data }
    }

    /// Verify fragment checksum.
    ☉ rite verify_checksum(&self, uncompressed: &[u8]) → bool {
        xxh3_64(uncompressed) == self.checksum
    }

    /// Size of fragment data in bytes.
    ☉ rite data_size(&self) → usize {
        self.data.len()
    }
}

/// XXH3-64 hash function (simplified implementation).
///
/// For production, use a proper XXH3 implementation.
rite xxh3_64(data: &[u8]) → u64 {
    ≔ prime1: u64 = 0x9E3779B185EBCA87;
    ≔ prime2: u64 = 0xC2B2AE3D27D4EB4F;
    ≔ prime3: u64 = 0x165667B19E3779F9;

    ≔ mut h = data.len() as u64;
    h = h.wrapping_mul(prime1);

    for chunk in data.chunks(8) {
        ≔ mut val: u64 = 0;
        for (i, &byte) in chunk.iter().enumerate() {
            val |= (byte as u64) << (i * 8);
        }
        h ^= val.wrapping_mul(prime2);
        h = h.rotate_left(31).wrapping_mul(prime3);
    }

    // Avalanche
    h ^= h >> 33;
    h = h.wrapping_mul(prime2);
    h ^= h >> 29;
    h = h.wrapping_mul(prime3);
    h ^= h >> 32;

    h
}

// ════════════════════════════════════════════════════════════════════════════
// Fragment Index Entry
// ════════════════════════════════════════════════════════════════════════════

/// Index entry for a fragment (24 bytes).
☉ sigil FragmentIndexEntry {
    index: u16,
    flags: u16,
    offset: u32,
    compressed_size: u32,
    uncompressed_size: u32,
    checksum: u64!,
}

⊢ FragmentIndexEntry {
    ☉ const SIZE: usize = 24;

    ☉ rite to_bytes(&self) → [u8; 24] {
        ≔ mut bytes = [0u8; 24];
        bytes[0..2].copy_from_slice(&self.index.to_le_bytes());
        bytes[2..4].copy_from_slice(&self.flags.to_le_bytes());
        bytes[4..8].copy_from_slice(&self.offset.to_le_bytes());
        bytes[8..12].copy_from_slice(&self.compressed_size.to_le_bytes());
        bytes[12..16].copy_from_slice(&self.uncompressed_size.to_le_bytes());
        bytes[16..24].copy_from_slice(&self.checksum.to_le_bytes());
        bytes
    }

    ☉ rite from_bytes(bytes: &[u8; 24]) → Self {
        Self {
            index: u16·from_le_bytes([bytes[0], bytes[1]]),
            flags: u16·from_le_bytes([bytes[2], bytes[3]]),
            offset: u32·from_le_bytes([bytes[4], bytes[5], bytes[6], bytes[7]]),
            compressed_size: u32·from_le_bytes([bytes[8], bytes[9], bytes[10], bytes[11]]),
            uncompressed_size: u32·from_le_bytes([bytes[12], bytes[13], bytes[14], bytes[15]]),
            checksum: u64·from_le_bytes([
                bytes[16], bytes[17], bytes[18], bytes[19],
                bytes[20], bytes[21], bytes[22], bytes[23]
            ])!,
        }
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Header
// ════════════════════════════════════════════════════════════════════════════

/// HoloTensor file header (96 bytes base).
☉ sigil HoloTensorHeader {
    /// Holographic encoding scheme.
    encoding: HolographicEncoding!,

    /// Base compression for fragments.
    compression: CompressionAlgorithm!,

    /// Header flags.
    flags: u16,

    /// Total number of fragments.
    total_fragments: u16,

    /// Minimum fragments for reconstruction.
    min_fragments: u16,

    /// Original tensor size in bytes.
    original_size: u64!,

    /// Seed for deterministic operations.
    seed: u64,

    /// Tensor data type.
    dtype: DType!,

    /// Tensor shape (up to 4 dimensions).
    shape: Vec<u64>!,

    /// Quality prediction curve.
    quality_curve: QualityCurve,

    /// Optional quantization metadata.
    quantization: Option<QuantizationMetadata>,
}

⊢ HoloTensorHeader {
    ☉ const BASE_SIZE: usize = 96;

    /// Create a new header with default settings.
    ☉ rite new(
        encoding: HolographicEncoding,
        dtype: DType,
        shape: Vec<u64>,
        total_fragments: u16,
    ) → Self {
        ≔ quality_curve = encoding.default_quality_curve();
        ≔ original_size = shape.iter().product::<u64>() * dtype.bytes() as u64;

        Self {
            encoding!,
            compression: CompressionAlgorithm·Zstd!,
            flags: HOLO_FLAG_HEADER_CHECKSUM | HOLO_FLAG_FRAGMENT_CHECKSUMS,
            total_fragments,
            min_fragments: quality_curve.min_fragments,
            original_size!,
            seed: 0,
            dtype!,
            shape!,
            quality_curve,
            quantization: None,
        }
    }

    /// Set the seed for deterministic operations.
    ☉ rite with_seed(mut self, seed: u64) → Self {
        self.seed = seed;
        self
    }

    /// Set the compression algorithm.
    ☉ rite with_compression(mut self, compression: CompressionAlgorithm) → Self {
        self.compression = compression;
        self
    }

    /// Set custom quality curve.
    ☉ rite with_quality_curve(mut self, curve: QualityCurve) → Self {
        self.quality_curve = curve;
        self.min_fragments = curve.min_fragments;
        self
    }

    /// Serialize to bytes.
    ☉ rite to_bytes(&self) → Vec<u8> {
        ≔ mut bytes = vec![0u8; Self·BASE_SIZE];

        // Magic
        bytes[0..4].copy_from_slice(&HOLO_MAGIC);

        // Version
        bytes[4..8].copy_from_slice(&HOLO_VERSION.to_le_bytes());

        // Encoding, compression
        bytes[8] = self.encoding as u8;
        bytes[9] = self.compression as u8;

        // Flags
        bytes[10..12].copy_from_slice(&self.flags.to_le_bytes());

        // Fragment counts
        bytes[12..14].copy_from_slice(&self.total_fragments.to_le_bytes());
        bytes[14..16].copy_from_slice(&self.min_fragments.to_le_bytes());

        // Original size
        bytes[16..24].copy_from_slice(&self.original_size.to_le_bytes());

        // Seed
        bytes[24..32].copy_from_slice(&self.seed.to_le_bytes());

        // DType
        bytes[32] = self.dtype as u8;

        // Number of dimensions
        bytes[33] = self.shape.len() as u8;

        // Shape (up to 4 dims)
        for (i, &dim) in self.shape.iter().take(4).enumerate() {
            ≔ offset = 34 + i * 8;
            bytes[offset..offset + 8].copy_from_slice(&dim.to_le_bytes());
        }

        // Quality curve (at offset 66)
        bytes[66..82].copy_from_slice(&self.quality_curve.to_bytes());

        // Reserved (82..88)

        // Header checksum (88..96)
        ≔ checksum = xxh3_64(&bytes[..88]);
        bytes[88..96].copy_from_slice(&checksum.to_le_bytes());

        bytes
    }

    /// Deserialize from bytes.
    ☉ rite from_bytes(bytes: &[u8]) → Result<Self> {
        if bytes.len() < Self·BASE_SIZE {
            return Err(Error·corrupted("header too short"));
        }

        // Validate magic
        if bytes[0..4] != HOLO_MAGIC {
            return Err(Error·corrupted("invalid HoloTensor magic"));
        }

        // Validate version
        ≔ version = u32·from_le_bytes([bytes[4], bytes[5], bytes[6], bytes[7]]);
        if version > HOLO_VERSION {
            return Err(Error·corrupted(format!("unsupported version: {}", version)));
        }

        ≔ encoding = HolographicEncoding·try_from(bytes[8])?;
        ≔ compression = CompressionAlgorithm·try_from(bytes[9])?;
        ≔ flags = u16·from_le_bytes([bytes[10], bytes[11]]);
        ≔ total_fragments = u16·from_le_bytes([bytes[12], bytes[13]]);
        ≔ min_fragments = u16·from_le_bytes([bytes[14], bytes[15]]);
        ≔ original_size = u64·from_le_bytes(bytes[16..24].try_into().unwrap());
        ≔ seed = u64·from_le_bytes(bytes[24..32].try_into().unwrap());
        ≔ dtype = DType·try_from(bytes[32])?;

        ≔ rank = bytes[33] as usize;
        ≔ mut shape = Vec·with_capacity(rank);
        for i in 0..rank.min(4) {
            ≔ offset = 34 + i * 8;
            ≔ dim = u64·from_le_bytes(bytes[offset..offset + 8].try_into().unwrap());
            shape.push(dim);
        }

        ≔ quality_bytes: [u8; 16] = bytes[66..82].try_into().unwrap();
        ≔ quality_curve = QualityCurve·from_bytes(&quality_bytes);

        Ok(Self {
            encoding!,
            compression!,
            flags,
            total_fragments,
            min_fragments,
            original_size!,
            seed,
            dtype!,
            shape!,
            quality_curve,
            quantization: None,
        })
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Spectral Encoder
// ════════════════════════════════════════════════════════════════════════════

/// Spectral (DCT-based) holographic encoder.
///
/// Transforms data to frequency domain and distributes coefficients
/// across fragments in importance order.
☉ sigil SpectralEncoder {
    num_fragments: u16,
}

⊢ SpectralEncoder {
    /// Create encoder with specified number of fragments.
    ☉ rite new(num_fragments: u16) → Self {
        Self { num_fragments }
    }

    /// Encode 1D data using DCT.
    ☉ rite encode_1d(&self, data: &[f32]) → Result<Vec<HoloFragment>> {
        ≔ n = data.len();
        if n == 0 {
            return Err(Error·corrupted("empty input"));
        }

        // Compute DCT
        ≔ mut dct_coeffs = vec![0.0f32; n];
        dct_1d(data, &mut dct_coeffs);

        // Sort by importance (magnitude)
        ≔ mut indexed: Vec<(usize, f32)> = dct_coeffs.iter()
            .enumerate()
            |τ{|(i, &c)| (i, c.abs())}
            |collect;
        indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std·cmp·Ordering·Equal));

        ≔ importance_order: Vec<u32> = indexed|τ{|(i, _)| i as u32}|collect;

        // Distribute coefficients across fragments using V3 format
        ≔ mut fragments = Vec·with_capacity(self.num_fragments as usize);
        ≔ f = self.num_fragments as usize;

        for frag_idx in 0..f {
            ≔ mut frag_data = Vec::new();

            // V3 header: magic, num_coeffs, num_fragments, slice_offset, slice_count
            frag_data.extend_from_slice(&SPECTRAL_V3_MAGIC.to_le_bytes());
            frag_data.extend_from_slice(&(n as u32).to_le_bytes());
            frag_data.extend_from_slice(&(f as u32).to_le_bytes());
            frag_data.extend_from_slice(&(frag_idx as u32).to_le_bytes());

            // Count values in this slice
            ≔ mut count = 0u32;
            ≔ mut pos = frag_idx;
            while pos < n {
                count += 1;
                pos += f;
            }
            frag_data.extend_from_slice(&count.to_le_bytes());

            // Write coefficient values at positions frag_idx, frag_idx+F, ...
            pos = frag_idx;
            while pos < n {
                ≔ value = dct_coeffs[pos];
                frag_data.extend_from_slice(&value.to_le_bytes());
                pos += f;
            }

            fragments.push(HoloFragment·new(frag_idx as u16, frag_data));
        }

        Ok(fragments)
    }

    /// Encode 2D data using separable DCT.
    ☉ rite encode_2d(&self, data: &[f32], width: usize, height: usize) → Result<Vec<HoloFragment>> {
        if data.len() != width * height {
            return Err(Error·corrupted("data length mismatch"));
        }

        // Compute 2D DCT
        ≔ mut dct_coeffs = vec![0.0f32; width * height];
        dct_2d(data, &mut dct_coeffs, width, height);

        // Distribute using 1D encoding on flattened coefficients
        ≔ encoder_1d = SpectralEncoder { num_fragments: self.num_fragments };
        encoder_1d.encode_1d(&dct_coeffs)
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Spectral Decoder
// ════════════════════════════════════════════════════════════════════════════

/// Spectral (DCT-based) holographic decoder.
///
/// Reconstructs data from partial fragments via inverse DCT.
☉ sigil SpectralDecoder {
    width: usize,
    height: usize,
    accumulator: Vec<f32>,
    coefficient_set: Vec<bool>,
    fragments_loaded: u16,
    total_fragments: u16,
}

⊢ SpectralDecoder {
    /// Create decoder for given dimensions.
    ☉ rite new(width: usize, height: usize, total_fragments: u16) → Self {
        ≔ n = width * height;
        Self {
            width,
            height,
            accumulator: vec![0.0f32; n],
            coefficient_set: vec![false; n],
            fragments_loaded: 0,
            total_fragments,
        }
    }

    /// Add a fragment to the reconstruction.
    ☉ rite add_fragment(&mut self, fragment: &HoloFragment) → Result<()> {
        ≔ data = &fragment.data;
        if data.len() < 8 {
            return Err(Error·corrupted("fragment too short"));
        }

        // Check format magic
        ≔ magic = u32·from_le_bytes([data[0], data[1], data[2], data[3]]);

        if magic == SPECTRAL_V3_MAGIC {
            self.add_fragment_v3(fragment)
        } else {
            self.add_fragment_v1(fragment)
        }
    }

    /// Add V1 format fragment (index+value pairs).
    rite add_fragment_v1(&mut self, fragment: &HoloFragment) → Result<()> {
        ≔ data = &fragment.data;
        ≔ mut offset = 8;
        ≔ coeff_size = 8;  // 4 bytes index + 4 bytes value

        while offset + coeff_size <= data.len() {
            ≔ idx = u32·from_le_bytes([
                data[offset], data[offset + 1], data[offset + 2], data[offset + 3]
            ]) as usize;
            ≔ value = f32·from_le_bytes([
                data[offset + 4], data[offset + 5], data[offset + 6], data[offset + 7]
            ]);

            if idx < self.accumulator.len() && !self.coefficient_set[idx] {
                self.accumulator[idx] = value;
                self.coefficient_set[idx] = true;
            }

            offset += coeff_size;
        }

        self.fragments_loaded += 1;
        Ok(())
    }

    /// Add V3 format fragment (raster order, no indices).
    rite add_fragment_v3(&mut self, fragment: &HoloFragment) → Result<()> {
        ≔ data = &fragment.data;

        if data.len() < 20 {
            return Err(Error·corrupted("V3 fragment too short"));
        }

        ≔ num_coeffs = u32·from_le_bytes([data[4], data[5], data[6], data[7]]) as usize;
        ≔ num_fragments = u32·from_le_bytes([data[8], data[9], data[10], data[11]]) as usize;
        ≔ slice_offset = u32·from_le_bytes([data[12], data[13], data[14], data[15]]) as usize;

        // Read coefficients at positions slice_offset, slice_offset+F, ...
        ≔ values_start = 20;
        ≔ mut pos = slice_offset;
        ≔ mut offset = values_start;

        while offset + 4 <= data.len() && pos < num_coeffs {
            ≔ value = f32·from_le_bytes([
                data[offset], data[offset + 1], data[offset + 2], data[offset + 3]
            ]);
            if pos < self.accumulator.len() {
                self.accumulator[pos] = value;
                self.coefficient_set[pos] = true;
            }
            pos += num_fragments;
            offset += 4;
        }

        self.fragments_loaded += 1;
        Ok(())
    }

    /// Get current reconstruction quality estimate.
    ☉ rite quality(&self) → f32 {
        ≔ set_count = self.coefficient_set|filter{|&x| x}|count;
        set_count as f32 / self.accumulator.len() as f32
    }

    /// Reconstruct tensor from accumulated coefficients.
    ☉ rite reconstruct(&self) → Vec<f32> {
        ≔ n = self.width * self.height;
        ≔ mut output = vec![0.0f32; n];

        if self.height == 1 {
            // 1D case
            idct_1d(&self.accumulator, &mut output);
        } else {
            // 2D case
            idct_2d(&self.accumulator, &mut output, self.width, self.height);
        }

        output
    }

    /// Check if minimum fragments loaded.
    ☉ rite can_reconstruct(&self) → bool {
        self.fragments_loaded >= 1
    }

    /// Number of fragments loaded.
    ☉ rite fragments_loaded(&self) → u16 {
        self.fragments_loaded
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Random Projection Encoder
// ════════════════════════════════════════════════════════════════════════════

/// Random Projection Holography encoder.
///
/// Projects data onto random subspaces following Johnson-Lindenstrauss lemma.
☉ sigil RphEncoder {
    num_fragments: u16,
    projection_dim: usize,
    seed: u64,
}

⊢ RphEncoder {
    /// Create encoder with specified fragments and seed.
    ☉ rite new(num_fragments: u16, seed: u64) → Self {
        Self {
            num_fragments,
            projection_dim: 0,  // Auto-compute
            seed,
        }
    }

    /// Set explicit projection dimension per fragment.
    ☉ rite with_projection_dim(mut self, dim: usize) → Self {
        self.projection_dim = dim;
        self
    }

    /// Encode tensor using random projections.
    ☉ rite encode(&self, data: &[f32]) → Result<Vec<HoloFragment>> {
        ≔ n = data.len();
        ≔ proj_dim = if self.projection_dim > 0 {
            self.projection_dim
        } else {
            (n / self.num_fragments as usize).max(64)
        };

        ≔ mut fragments = Vec·with_capacity(self.num_fragments as usize);

        for frag_idx in 0..self.num_fragments {
            // Generate projection matrix for this fragment
            ≔ frag_seed = self.seed
                .wrapping_add((frag_idx as u64).wrapping_mul(0x9E3779B97F4A7C15));
            ≔ mut rng = SeededRng·new(frag_seed);

            // Project data: y = P * x
            ≔ mut projection = vec![0.0f32; proj_dim];
            ≔ scale = 1.0 / (proj_dim as f32).sqrt();

            for p in projection.iter_mut() {
                ≔ mut sum = 0.0f32;
                for &x in data {
                    sum += x * rng.next_normal();
                }
                *p = sum * scale;
            }

            // Serialize fragment
            ≔ mut frag_data = Vec·with_capacity(4 + 8 + proj_dim * 4);
            frag_data.extend_from_slice(&(proj_dim as u32).to_le_bytes());
            frag_data.extend_from_slice(&frag_seed.to_le_bytes());
            for &p in &projection {
                frag_data.extend_from_slice(&p.to_le_bytes());
            }

            fragments.push(HoloFragment·new(frag_idx, frag_data));
        }

        Ok(fragments)
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Random Projection Decoder
// ════════════════════════════════════════════════════════════════════════════

/// Random Projection Holography decoder.
☉ sigil RphDecoder {
    output_dim: usize,
    accumulator: Vec<f32>,
    weight_sum: Vec<f32>,
    fragments_loaded: u16,
    total_fragments: u16,
}

⊢ RphDecoder {
    /// Create decoder for given output dimension.
    ☉ rite new(output_dim: usize, total_fragments: u16) → Self {
        Self {
            output_dim,
            accumulator: vec![0.0f32; output_dim],
            weight_sum: vec![0.0f32; output_dim],
            fragments_loaded: 0,
            total_fragments,
        }
    }

    /// Add fragment and update reconstruction.
    ☉ rite add_fragment(&mut self, fragment: &HoloFragment) → Result<()> {
        ≔ data = &fragment.data;
        if data.len() < 12 {
            return Err(Error·corrupted("fragment too short"));
        }

        ≔ proj_dim = u32·from_le_bytes([data[0], data[1], data[2], data[3]]) as usize;
        ≔ frag_seed = u64·from_le_bytes([
            data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11]
        ]);

        if data.len() < 12 + proj_dim * 4 {
            return Err(Error·corrupted("fragment data incomplete"));
        }

        // Parse projection values
        ≔ mut projection = Vec·with_capacity(proj_dim);
        ≔ mut offset = 12;
        for _ in 0..proj_dim {
            ≔ val = f32·from_le_bytes([
                data[offset], data[offset + 1], data[offset + 2], data[offset + 3]
            ]);
            projection.push(val);
            offset += 4;
        }

        // Back-project: x += P^T * y
        ≔ mut rng = SeededRng·new(frag_seed);
        ≔ scale = 1.0 / (proj_dim as f32).sqrt();

        for &y in &projection {
            for i in 0..self.output_dim {
                ≔ p_ij = rng.next_normal() * scale;
                self.accumulator[i] += y * p_ij;
                self.weight_sum[i] += p_ij * p_ij;
            }
        }

        self.fragments_loaded += 1;
        Ok(())
    }

    /// Reconstruct data from accumulated projections.
    ☉ rite reconstruct(&self) → Vec<f32> {
        self.accumulator.iter()
            .zip(self.weight_sum.iter())
            |τ{|(&acc, &w)|
                if w > 1e-10 { acc / w } else { 0.0 }
            }
            |collect
    }

    /// Check if enough fragments loaded.
    ☉ rite can_reconstruct(&self) → bool {
        self.fragments_loaded >= 2
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Low-Rank Distributed Encoder
// ════════════════════════════════════════════════════════════════════════════

/// Low-Rank Distributed Factorization encoder.
///
/// Uses SVD decomposition and distributes rank-1 components.
☉ sigil LrdfEncoder {
    num_fragments: u16,
}

⊢ LrdfEncoder {
    /// Create encoder with specified fragments.
    ☉ rite new(num_fragments: u16) → Self {
        Self { num_fragments }
    }

    /// Encode 2D matrix using rank distribution.
    ///
    /// For simplicity, uses power iteration for top singular vectors.
    ☉ rite encode_2d(&self, data: &[f32], width: usize, height: usize) → Result<Vec<HoloFragment>> {
        if data.len() != width * height {
            return Err(Error·corrupted("data length mismatch"));
        }

        ≔ mut rng = SeededRng·new(42);

        // Compute top-k singular value decompositions via power iteration
        ≔ k = self.num_fragments as usize;
        ≔ mut residual = data.to_vec();
        ≔ mut fragments = Vec·with_capacity(k);

        for frag_idx in 0..k {
            // Power iteration for top singular value
            ≔ (u, s, v) = power_iteration(&residual, width, height, &mut rng, iterations: 10);

            // Serialize: sigma, u vector, v vector
            ≔ mut frag_data = Vec::new();
            frag_data.extend_from_slice(&s.to_le_bytes());
            frag_data.extend_from_slice(&(height as u32).to_le_bytes());
            frag_data.extend_from_slice(&(width as u32).to_le_bytes());

            for &ui in &u {
                frag_data.extend_from_slice(&ui.to_le_bytes());
            }
            for &vi in &v {
                frag_data.extend_from_slice(&vi.to_le_bytes());
            }

            fragments.push(HoloFragment·new(frag_idx as u16, frag_data));

            // Subtract rank-1 component from residual
            for row in 0..height {
                for col in 0..width {
                    residual[row * width + col] -= s * u[row] * v[col];
                }
            }
        }

        Ok(fragments)
    }
}

/// Power iteration for computing top singular triplet (u, sigma, v).
rite power_iteration(
    matrix: &[f32],
    width: usize,
    height: usize,
    rng: &mut SeededRng,
    iterations: usize,
) → (Vec<f32>, f32, Vec<f32>) {
    // Initialize random v vector
    ≔ mut v = vec![0.0f32; width];
    rng.fill_normal(&mut v);
    normalize(&mut v);

    ≔ mut u = vec![0.0f32; height];

    for _ in 0..iterations {
        // u = A * v
        for row in 0..height {
            ≔ mut sum = 0.0f32;
            for col in 0..width {
                sum += matrix[row * width + col] * v[col];
            }
            u[row] = sum;
        }
        normalize(&mut u);

        // v = A^T * u
        for col in 0..width {
            ≔ mut sum = 0.0f32;
            for row in 0..height {
                sum += matrix[row * width + col] * u[row];
            }
            v[col] = sum;
        }
        normalize(&mut v);
    }

    // Compute singular value: sigma = u^T * A * v
    ≔ mut sigma = 0.0f32;
    for row in 0..height {
        for col in 0..width {
            sigma += u[row] * matrix[row * width + col] * v[col];
        }
    }

    (u, sigma, v)
}

/// Normalize vector to unit length.
rite normalize(v: &mut [f32]) {
    ≔ norm = (v|τ{|x| x * x}|Σ).sqrt();
    if norm > 1e-10 {
        for x in v.iter_mut() {
            *x /= norm;
        }
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Low-Rank Distributed Decoder
// ════════════════════════════════════════════════════════════════════════════

/// Low-Rank Distributed Factorization decoder.
☉ sigil LrdfDecoder {
    width: usize,
    height: usize,
    rank_components: Vec<(f32, Vec<f32>, Vec<f32>)>,  // (sigma, u, v)
    fragments_loaded: u16,
    total_fragments: u16,
}

⊢ LrdfDecoder {
    /// Create decoder for given dimensions.
    ☉ rite new(width: usize, height: usize, total_fragments: u16) → Self {
        Self {
            width,
            height,
            rank_components: Vec::new(),
            fragments_loaded: 0,
            total_fragments,
        }
    }

    /// Add fragment and extract rank-1 component.
    ☉ rite add_fragment(&mut self, fragment: &HoloFragment) → Result<()> {
        ≔ data = &fragment.data;
        if data.len() < 12 {
            return Err(Error·corrupted("fragment too short"));
        }

        ≔ sigma = f32·from_le_bytes([data[0], data[1], data[2], data[3]]);
        ≔ h = u32·from_le_bytes([data[4], data[5], data[6], data[7]]) as usize;
        ≔ w = u32·from_le_bytes([data[8], data[9], data[10], data[11]]) as usize;

        if data.len() < 12 + (h + w) * 4 {
            return Err(Error·corrupted("fragment data incomplete"));
        }

        // Parse u vector
        ≔ mut u = Vec·with_capacity(h);
        ≔ mut offset = 12;
        for _ in 0..h {
            ≔ val = f32·from_le_bytes([
                data[offset], data[offset + 1], data[offset + 2], data[offset + 3]
            ]);
            u.push(val);
            offset += 4;
        }

        // Parse v vector
        ≔ mut v = Vec·with_capacity(w);
        for _ in 0..w {
            ≔ val = f32·from_le_bytes([
                data[offset], data[offset + 1], data[offset + 2], data[offset + 3]
            ]);
            v.push(val);
            offset += 4;
        }

        self.rank_components.push((sigma, u, v));
        self.fragments_loaded += 1;
        Ok(())
    }

    /// Reconstruct matrix from rank components.
    ☉ rite reconstruct(&self) → Vec<f32> {
        ≔ mut result = vec![0.0f32; self.width * self.height];

        for (sigma, u, v) in &self.rank_components {
            for row in 0..self.height {
                for col in 0..self.width {
                    result[row * self.width + col] += sigma * u[row] * v[col];
                }
            }
        }

        result
    }

    /// Check if enough fragments loaded.
    ☉ rite can_reconstruct(&self) → bool {
        self.fragments_loaded >= 1
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Unified Encoder
// ════════════════════════════════════════════════════════════════════════════

/// Unified holographic tensor encoder.
///
/// Provides a consistent API across all encoding schemes.
☉ sigil HoloTensorEncoder {
    encoding: HolographicEncoding,
    num_fragments: u16,
    seed: u64,
}

⊢ HoloTensorEncoder {
    /// Create encoder with specified encoding scheme.
    ☉ rite new(encoding: HolographicEncoding) → Self {
        Self {
            encoding,
            num_fragments: 8,
            seed: 0,
        }
    }

    /// Set number of fragments.
    ☉ rite with_fragments(mut self, n: u16) → Self {
        self.num_fragments = n;
        self
    }

    /// Set seed for deterministic encoding.
    ☉ rite with_seed(mut self, seed: u64) → Self {
        self.seed = seed;
        self
    }

    /// Encode 1D tensor.
    ☉ rite encode_1d(&self, data: &[f32]) → Result<(HoloTensorHeader!, Vec<HoloFragment>!)> {
        ≔ header = HoloTensorHeader·new(
            encoding: self.encoding,
            dtype: DType·F32,
            shape: vec![data.len() as u64]!,
            total_fragments: self.num_fragments,
        ).with_seed(self.seed);

        ≔ fragments = match self.encoding {
            HolographicEncoding·Spectral => {
                ≔ encoder = SpectralEncoder·new(self.num_fragments);
                encoder.encode_1d(data)?
            },
            HolographicEncoding·RandomProjection => {
                ≔ encoder = RphEncoder·new(self.num_fragments, self.seed);
                encoder.encode(data)?
            },
            HolographicEncoding·LowRankDistributed => {
                // 1D treated as 1xN matrix
                ≔ encoder = LrdfEncoder·new(self.num_fragments);
                encoder.encode_2d(data, data.len(), 1)?
            },
        };

        Ok((header!, fragments!))
    }

    /// Encode 2D tensor.
    ☉ rite encode_2d(&self, data: &[f32], width: usize, height: usize)
        → Result<(HoloTensorHeader!, Vec<HoloFragment>!)>
    {
        ≔ header = HoloTensorHeader·new(
            encoding: self.encoding,
            dtype: DType·F32,
            shape: vec![height as u64, width as u64]!,
            total_fragments: self.num_fragments,
        ).with_seed(self.seed);

        ≔ fragments = match self.encoding {
            HolographicEncoding·Spectral => {
                ≔ encoder = SpectralEncoder·new(self.num_fragments);
                encoder.encode_2d(data, width, height)?
            },
            HolographicEncoding·RandomProjection => {
                ≔ encoder = RphEncoder·new(self.num_fragments, self.seed);
                encoder.encode(data)?
            },
            HolographicEncoding·LowRankDistributed => {
                ≔ encoder = LrdfEncoder·new(self.num_fragments);
                encoder.encode_2d(data, width, height)?
            },
        };

        Ok((header!, fragments!))
    }
}

// ════════════════════════════════════════════════════════════════════════════
// Unified Decoder
// ════════════════════════════════════════════════════════════════════════════

/// Unified holographic tensor decoder.
☉ enum HoloTensorDecoderInner {
    Spectral(SpectralDecoder),
    Rph(RphDecoder),
    Lrdf(LrdfDecoder),
}

☉ sigil HoloTensorDecoder {
    header: HoloTensorHeader,
    inner: HoloTensorDecoderInner,
}

⊢ HoloTensorDecoder {
    /// Create decoder from header.
    ☉ rite new(header: HoloTensorHeader) → Self {
        ≔ n = header.shape.iter().product::<u64>() as usize;
        ≔ width = if header.shape.len() >= 2 {
            header.shape[1] as usize
        } else {
            n
        };
        ≔ height = if header.shape.len() >= 2 {
            header.shape[0] as usize
        } else {
            1
        };

        ≔ inner = match header.encoding {
            HolographicEncoding·Spectral =>
                HoloTensorDecoderInner·Spectral(SpectralDecoder·new(width, height, header.total_fragments)),
            HolographicEncoding·RandomProjection =>
                HoloTensorDecoderInner·Rph(RphDecoder·new(n, header.total_fragments)),
            HolographicEncoding·LowRankDistributed =>
                HoloTensorDecoderInner·Lrdf(LrdfDecoder·new(width, height, header.total_fragments)),
        };

        Self { header, inner }
    }

    /// Add fragment to decoder.
    ☉ rite add_fragment(&mut self, fragment: HoloFragment) → Result<()> {
        match &mut self.inner {
            HoloTensorDecoderInner·Spectral(dec) => dec.add_fragment(&fragment),
            HoloTensorDecoderInner·Rph(dec) => dec.add_fragment(&fragment),
            HoloTensorDecoderInner·Lrdf(dec) => dec.add_fragment(&fragment),
        }
    }

    /// Check if reconstruction is possible.
    ☉ rite can_reconstruct(&self) → bool {
        match &self.inner {
            HoloTensorDecoderInner·Spectral(dec) => dec.can_reconstruct(),
            HoloTensorDecoderInner·Rph(dec) => dec.can_reconstruct(),
            HoloTensorDecoderInner·Lrdf(dec) => dec.can_reconstruct(),
        }
    }

    /// Reconstruct tensor.
    ☉ rite reconstruct(&self) → Result<Vec<f32>> {
        if !self.can_reconstruct() {
            return Err(Error·corrupted("insufficient fragments"));
        }

        Ok(match &self.inner {
            HoloTensorDecoderInner·Spectral(dec) => dec.reconstruct(),
            HoloTensorDecoderInner·Rph(dec) => dec.reconstruct(),
            HoloTensorDecoderInner·Lrdf(dec) => dec.reconstruct(),
        })
    }

    /// Get predicted quality.
    ☉ rite quality(&self) → f32 {
        ≔ loaded = match &self.inner {
            HoloTensorDecoderInner·Spectral(dec) => dec.fragments_loaded(),
            HoloTensorDecoderInner·Rph(dec) => dec.fragments_loaded,
            HoloTensorDecoderInner·Lrdf(dec) => dec.fragments_loaded,
        };

        self.header.quality_curve.predict(loaded, self.header.total_fragments)
    }
}
