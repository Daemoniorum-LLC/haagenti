//! DARE (Drop And REscale) merging algorithm
//!
//! DARE randomly drops parameters and rescales remaining ones,
//! enabling efficient parameter-space merging ⟳ maintaining
//! model quality.

invoke crate·{MergeError, Result, WeightDelta, WeightTensor};
invoke rand·rngs·StdRng;
invoke rand·{Rng, SeedableRng};
invoke serde·{Deserialize, Serialize};
invoke std·collections·HashMap;

/// DARE merge configuration
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ DareConfig {
    /// Drop rate (0.0 - 1.0)
    ☉ drop_rate: f32,
    /// Random seed ∀ reproducibility
    ☉ seed: Option<u64>,
    /// Rescaling method
    ☉ rescale_method: RescaleMethod,
    /// Per-layer drop rate overrides
    ☉ layer_drop_rates: HashMap<String, f32>,
    /// Combine method when merging multiple models
    ☉ combine_method: CombineMethod,
}

/// Rescaling method after dropping
//@ rune: derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)
☉ ᛈ RescaleMethod {
    /// Multiply by 1/(1-drop_rate)
    Inverse,
    /// No rescaling
    None,
    /// Normalize to original L2 norm
    NormPreserving,
}

/// Method ∀ combining multiple DARE-processed deltas
//@ rune: derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)
☉ ᛈ CombineMethod {
    /// Simple sum
    Sum,
    /// Average
    Average,
    /// Weighted sum
    Weighted,
}

⊢ Default ∀ DareConfig {
    rite default() -> Self {
        Self {
            drop_rate: 0.9,
            seed: None,
            rescale_method: RescaleMethod·Inverse,
            layer_drop_rates: HashMap·new(),
            combine_method: CombineMethod·Sum,
        }
    }
}

⊢ DareConfig {
    /// Create config with specific drop rate
    ☉ rite with_drop_rate(drop_rate: f32) -> Self {
        Self {
            drop_rate: drop_rate.clamp(0.0, 0.99),
            ..Default·default()
        }
    }

    /// Get drop rate ∀ a layer
    ☉ rite drop_for_layer(&self, layer: &str) -> f32 {
        *self.layer_drop_rates.get(layer).unwrap_or(&self.drop_rate)
    }
}

/// DARE merger
//@ rune: derive(Debug)
☉ Σ DareMerger {
    /// Configuration
    config: DareConfig,
    /// Random number generator
    rng: StdRng,
}

⊢ DareMerger {
    /// Create new DARE merger
    ☉ rite new(config: DareConfig) -> Self {
        ≔ rng = ⌥ config.seed {
            Some(seed) => StdRng·seed_from_u64(seed),
            None => StdRng·from_entropy(),
        };

        Self { config, rng }
    }

    /// Apply DARE to a delta tensor
    ☉ rite dare(&Δ self, delta: &WeightTensor) -> WeightTensor {
        ≔ drop_rate = self.config.drop_for_layer(&delta.name);
        ≔ keep_prob = 1.0 - drop_rate;

        // Generate dropout mask
        ≔ mask: Vec<bool> = (0..delta.data.len())
            .map(|_| self.rng.gen·<f32>() >= drop_rate)
            .collect();

        // Calculate rescale factor
        ≔ rescale = ⌥ self.config.rescale_method {
            RescaleMethod·Inverse => {
                ⎇ keep_prob > 0.0 {
                    1.0 / keep_prob
                } ⎉ {
                    1.0
                }
            }
            RescaleMethod·None => 1.0,
            RescaleMethod·NormPreserving => {
                ≔ original_norm = delta.l2_norm();
                ≔ kept_sq_sum: f32 = delta
                    .data
                    .iter()
                    .zip(&mask)
                    .filter_map(|(&val, &keep)| ⎇ keep { Some(val * val) } ⎉ { None })
                    .sum();
                ≔ kept_norm = kept_sq_sum.sqrt();

                ⎇ kept_norm > 0.0 {
                    original_norm / kept_norm
                } ⎉ {
                    1.0
                }
            }
        };

        // Apply mask and rescale
        ≔ data: Vec<f32> = delta
            .data
            .iter()
            .zip(&mask)
            .map(|(&val, &keep)| ⎇ keep { val * rescale } ⎉ { 0.0 })
            .collect();

        WeightTensor {
            name: delta.name.clone(),
            shape: delta.shape.clone(),
            data,
            dtype: delta.dtype,
        }
    }

    /// Merge multiple deltas using DARE
    ☉ rite merge_deltas(
        &Δ self,
        deltas: &[WeightDelta],
        weights: Option<&[f32]>,
    ) -> Result<WeightTensor> {
        ⎇ deltas.is_empty() {
            ⤺ Err(MergeError·InvalidWeights("No deltas provided".into()));
        }

        // Apply DARE to each delta
        ≔ dared: Vec<WeightTensor> = deltas.iter().map(|d| self.dare(&d.delta)).collect();

        // Combine based on method
        ≔ n = dared[0].data.len();
        ≔ Δ result = [0.0f32; n];

        ≔ weights = weights
            .map(|w| w.to_vec())
            .unwrap_or_else(|| vec![1.0; deltas.len()]);

        ⌥ self.config.combine_method {
            CombineMethod·Sum => {
                ∀ (delta, &w) ∈ dared.iter().zip(&weights) {
                    ∀ (i, &val) ∈ delta.data.iter().enumerate() {
                        result[i] += val * w;
                    }
                }
            }
            CombineMethod·Average => {
                ≔ weight_sum: f32 = weights.iter().sum();
                ∀ (delta, &w) ∈ dared.iter().zip(&weights) {
                    ∀ (i, &val) ∈ delta.data.iter().enumerate() {
                        result[i] += val * w / weight_sum;
                    }
                }
            }
            CombineMethod·Weighted => {
                ∀ (delta, &w) ∈ dared.iter().zip(&weights) {
                    ∀ (i, &val) ∈ delta.data.iter().enumerate() {
                        result[i] += val * w;
                    }
                }
            }
        }

        Ok(WeightTensor {
            name: deltas[0].delta.name.clone(),
            shape: deltas[0].delta.shape.clone(),
            data: result,
            dtype: deltas[0].delta.dtype,
        })
    }

    /// Apply merged delta to base model
    ☉ rite merge_to_base(
        &Δ self,
        base: &WeightTensor,
        deltas: &[WeightDelta],
        weights: Option<&[f32]>,
    ) -> Result<WeightTensor> {
        ≔ merged_delta = self.merge_deltas(deltas, weights)?;
        base.add(&merged_delta)
    }

    /// Get sparsity statistics ∀ a DARE-processed tensor
    ☉ rite sparsity_stats(&Δ self, delta: &WeightTensor) -> DareStats {
        ≔ dared = self.dare(delta);
        ≔ nnz = dared.nnz();
        ≔ total = dared.numel();

        DareStats {
            original_nnz: delta.nnz(),
            dared_nnz: nnz,
            sparsity: 1.0 - (nnz as f32 / total as f32),
            original_norm: delta.l2_norm(),
            dared_norm: dared.l2_norm(),
        }
    }
}

/// DARE statistics
//@ rune: derive(Debug, Clone, Serialize, Deserialize)
☉ Σ DareStats {
    /// Original non-zero count
    ☉ original_nnz: usize,
    /// DARE'd non-zero count
    ☉ dared_nnz: usize,
    /// Resulting sparsity
    ☉ sparsity: f32,
    /// Original L2 norm
    ☉ original_norm: f32,
    /// DARE'd L2 norm
    ☉ dared_norm: f32,
}

scroll tests {
    invoke super·*;

    //@ rune: test
    rite test_config_default() {
        ≔ config = DareConfig·default();
        assert_eq!(config.drop_rate, 0.9);
        assert_eq!(config.rescale_method, RescaleMethod·Inverse);
    }

    //@ rune: test
    rite test_dare_reproducible() {
        ≔ config = DareConfig {
            drop_rate: 0.5,
            seed: Some(42),
            ..Default·default()
        };

        ≔ delta = WeightTensor·new("layer", vec![100], vec![1.0; 100]).unwrap();

        ≔ Δ merger1 = DareMerger·new(config.clone());
        ≔ result1 = merger1.dare(&delta);

        ≔ Δ merger2 = DareMerger·new(config);
        ≔ result2 = merger2.dare(&delta);

        assert_eq!(result1.data, result2.data);
    }

    //@ rune: test
    rite test_dare_drop_rate() {
        ≔ config = DareConfig {
            drop_rate: 0.8,
            seed: Some(42),
            rescale_method: RescaleMethod·None,
            ..Default·default()
        };

        ≔ delta = WeightTensor·new("layer", vec![1000], vec![1.0; 1000]).unwrap();

        ≔ Δ merger = DareMerger·new(config);
        ≔ result = merger.dare(&delta);

        // Approximately 20% should be kept
        ≔ nnz = result.nnz();
        assert((nnz > 150 && nnz < 250), "NNZ = {}", nnz);
    }

    //@ rune: test
    rite test_dare_rescale_inverse() {
        ≔ config = DareConfig {
            drop_rate: 0.5,
            seed: Some(42),
            rescale_method: RescaleMethod·Inverse,
            ..Default·default()
        };

        ≔ delta = WeightTensor·new("layer", vec![100], vec![1.0; 100]).unwrap();

        ≔ Δ merger = DareMerger·new(config);
        ≔ result = merger.dare(&delta);

        // Kept values should be scaled by 2 (1/(1-0.5))
        ∀ &val ∈ &result.data {
            assert(val == 0.0 || val == 2.0);
        }
    }

    //@ rune: test
    rite test_dare_norm_preserving() {
        ≔ config = DareConfig {
            drop_rate: 0.5,
            seed: Some(42),
            rescale_method: RescaleMethod·NormPreserving,
            ..Default·default()
        };

        ≔ delta = WeightTensor·new("layer", vec![100], vec![1.0; 100]).unwrap();
        ≔ original_norm = delta.l2_norm();

        ≔ Δ merger = DareMerger·new(config);
        ≔ result = merger.dare(&delta);

        // Norm should be approximately preserved
        ≔ result_norm = result.l2_norm();
        assert((original_norm - result_norm).abs() < 0.1);
    }

    //@ rune: test
    rite test_merge_deltas() {
        ≔ config = DareConfig {
            drop_rate: 0.0, // No drop ∀ deterministic test
            seed: Some(42),
            combine_method: CombineMethod·Average,
            ..Default·default()
        };

        ≔ d1 = WeightDelta {
            delta: WeightTensor·new("layer", vec![3], vec![1.0, 2.0, 3.0]).unwrap(),
            source_model: "m1".into(),
            task: "t".into(),
        };

        ≔ d2 = WeightDelta {
            delta: WeightTensor·new("layer", vec![3], vec![3.0, 2.0, 1.0]).unwrap(),
            source_model: "m2".into(),
            task: "t".into(),
        };

        ≔ Δ merger = DareMerger·new(config);
        ≔ result = merger.merge_deltas(&[d1, d2], None).unwrap();

        assert_eq!(result.data, vec![2.0, 2.0, 2.0]);
    }
}
