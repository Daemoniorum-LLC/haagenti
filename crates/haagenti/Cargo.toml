[package]
name = "haagenti"
version.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true
authors.workspace = true
rust-version.workspace = true
description = "Next-generation compression library with LZ4, Zstd, Brotli, and Deflate support"
keywords = ["compression", "lz4", "zstd", "brotli", "deflate"]
categories = ["compression", "encoding"]

[features]
default = ["lz4", "zstd", "brotli", "deflate", "simd", "stream"]

# Algorithm features
lz4 = ["dep:haagenti-lz4"]
zstd = ["dep:haagenti-zstd"]
brotli = ["dep:haagenti-brotli"]
deflate = ["dep:haagenti-deflate"]

# Performance features
simd = ["dep:haagenti-simd"]
stream = ["dep:haagenti-stream"]
parallel = ["dep:rayon"]

# Testing utilities (safetensors parsing, metrics, INT4 quantization)
# serde_json is now a regular dependency for importance.rs
testing = []

# All algorithms
full = ["lz4", "zstd", "brotli", "deflate", "simd", "stream"]

# High-performance pipeline with parallel processing
turbo = ["parallel", "zstd"]

# ═══════════════════════════════════════════════════════════════════════════════
# INFERENCE STACK - GPU acceleration and platform support for Infernum
# ═══════════════════════════════════════════════════════════════════════════════

# GPU-accelerated compression/inference pipeline
cuda = ["dep:haagenti-cuda", "turbo"]

# WebGPU compute shaders (browser/cross-platform inference)
webgpu = ["dep:haagenti-webgpu"]

# Mobile inference backends (CoreML, NNAPI)
mobile = ["dep:haagenti-mobile"]

# Distributed inference topologies (Ring, Mesh, Tree, etc.)
distributed = ["dep:haagenti-distributed"]

# Serverless cold-start optimization
serverless = ["dep:haagenti-serverless"]

# Progressive streaming decompression
inference-streaming = ["dep:haagenti-streaming"]

# Cross-model fragment sharing
fragments = ["dep:haagenti-fragments"]

# ML-guided importance scoring
importance = ["dep:haagenti-importance", "fragments"]

# Runtime auto-optimization
autoopt = ["dep:haagenti-autoopt"]

# Online learning (LoRA, reservoir computing)
learning = ["dep:haagenti-learning"]

# Speculative execution
speculative = ["dep:haagenti-speculative", "fragments"]

# ═══════════════════════════════════════════════════════════════════════════════
# INFERENCE BUNDLES - convenient feature sets for Infernum integration
# ═══════════════════════════════════════════════════════════════════════════════

# Full inference stack for local frontier model inference
inference = [
    "cuda",
    "webgpu",
    "inference-streaming",
    "autoopt",
    "fragments",
    "importance",
    "speculative",
]

# Mobile inference (iOS Neural Engine, Android NPU)
inference-mobile = [
    "mobile",
    "inference-streaming",
]

# Distributed/federated inference
inference-distributed = [
    "distributed",
    "inference-streaming",
    "serverless",
]

# Everything (full sovereign inference stack)
sovereign = [
    "full",
    "inference",
    "inference-mobile",
    "inference-distributed",
    "learning",
]

[dependencies]
# Core
haagenti-core = { workspace = true, features = ["dct"] }

# HCT format (tensor and holotensor modules)
haagenti-hct = { workspace = true }

# Compression algorithms (optional)
haagenti-lz4 = { workspace = true, optional = true }
haagenti-zstd = { workspace = true, optional = true }
haagenti-brotli = { workspace = true, optional = true }
haagenti-deflate = { workspace = true, optional = true }

# Performance (optional)
haagenti-simd = { workspace = true, optional = true }
haagenti-stream = { workspace = true, optional = true }

# ═══════════════════════════════════════════════════════════════════════════════
# INFERENCE STACK (optional) - GPU, Mobile, Distributed inference support
# ═══════════════════════════════════════════════════════════════════════════════
haagenti-cuda = { workspace = true, optional = true }
haagenti-webgpu = { workspace = true, optional = true }
haagenti-mobile = { workspace = true, optional = true }
haagenti-distributed = { workspace = true, optional = true }
haagenti-serverless = { workspace = true, optional = true }
haagenti-streaming = { workspace = true, optional = true }
haagenti-autoopt = { workspace = true, optional = true }
haagenti-learning = { workspace = true, optional = true }
haagenti-fragments = { workspace = true, optional = true }
haagenti-importance = { workspace = true, optional = true }
haagenti-speculative = { workspace = true, optional = true }

# External dependencies
xxhash-rust = { workspace = true }
rustfft = { workspace = true }
half = "2.4"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
memmap2 = "0.9"
indicatif = "0.17"
rayon = { version = "1.10", optional = true }
zstd = "0.13"  # Reference zstd for turbo pipeline (haagenti-zstd has compatibility issues)

[dev-dependencies]
criterion = { workspace = true }
half = "2.4"
proptest = "1.4"
rand = { workspace = true }
rand_distr = "0.4"
tempfile = "3.10"

[[example]]
name = "weight_compression_analysis"
required-features = ["lz4", "zstd"]

[[example]]
name = "safetensors_to_hct"
required-features = ["lz4", "zstd"]

[[example]]
name = "safetensors_to_int4_hct"
required-features = ["lz4"]

[[bench]]
name = "compression"
harness = false

[[example]]
name = "roundtrip_validation"
required-features = ["lz4", "zstd"]

[[test]]
name = "integration_tests"
required-features = ["lz4", "zstd", "testing"]

[[test]]
name = "proptest_compression"
required-features = ["lz4", "zstd", "testing"]

[[example]]
name = "compress_405b"
required-features = ["lz4", "zstd"]

[[example]]
name = "compress_turbo"
required-features = ["parallel", "zstd"]

[[example]]
name = "compress_selective"
required-features = ["parallel", "zstd"]

[[example]]
name = "compress_adaptive"
required-features = ["parallel", "zstd"]
